{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e37f3c7",
   "metadata": {},
   "source": [
    "# ğŸ§© Ø§Ù„Ù‚Ø³Ù… 1: Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ù€ NLP | NLP Libraries Overview\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ **Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**  \n",
    "> ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø³Ù†ØªØ¹Ø±Ù‘Ù Ø¹Ù„Ù‰ Ø£Ù‡Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ  \n",
    "> ÙˆØ³Ù†Ù‡ÙŠÙ‘Ø¦ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„ Ù„ØªØ¬Ø§Ø±Ø¨Ù†Ø§ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ÙƒÙˆØ±Ø³.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ **In English:**  \n",
    "> In this section, weâ€™ll explore the main Python libraries used in Natural Language Processing (NLP)  \n",
    "> and prepare our coding environment for the upcoming lessons.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§° Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© | Core Libraries\n",
    "\n",
    "| Ø§Ù„Ù…ÙƒØªØ¨Ø© | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… / Usage | Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª / Notes |\n",
    "|----------|------------------|--------------------|\n",
    "| **NLTK** | Ù…ÙƒØªØ¨Ø© ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ (Tokenization, Stemming, Stopwords)  <br> *A classic NLP library for tokenization, stemming, and stopwords handling.* | Ù…Ù…ØªØ§Ø²Ø© Ù„Ù„ØªØ¹Ù„Ù‘Ù… ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©  <br> *Ideal for learning and academic use.* |\n",
    "| **spaCy** | Ù…ÙƒØªØ¨Ø© Ø­Ø¯ÙŠØ«Ø© ÙˆØ³Ø±ÙŠØ¹Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù†ÙˆÙŠØ©  <br> *A modern, fast library for sentence parsing and entity extraction.* | ØªÙØ³ØªØ®Ø¯Ù… Ø¨ÙƒØ«Ø±Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©  <br> *Widely used in production-grade NLP projects.* |\n",
    "| **re** | Ù…ÙƒØªØ¨Ø© Ø¨Ø§ÙŠØ«ÙˆÙ† Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© Ù„Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© (Regular Expressions)  <br> *Pythonâ€™s built-in library for regular expressions.* | Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù‚Ø¨Ù„ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§  <br> *Essential for text cleaning and preprocessing.* |\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ Ø³Ù†Ø¨Ø¯Ø£ Ø¨Ù€ **NLTK** Ùˆ **spaCy** Ùˆ **re** Ù„Ø£Ù†Ù‡Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ ÙÙŠ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ØŒ  \n",
    "> Ø«Ù… Ø³Ù†Ù†ØªÙ‚Ù„ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ù„Ù‰ Ù…ÙƒØªØ¨Ø§Øª Ø£ÙƒØ«Ø± ØªÙ‚Ø¯Ù…Ù‹Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„ÙƒÙˆØ±Ø³.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ Weâ€™ll start with **NLTK** , **spaCy** and **re** , as theyâ€™re the backbone of most NLP projects,  \n",
    "> then explore more advanced tools later in the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6a64ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Installing NLTK\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c199fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ§  Import and check version\n",
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¥ Download basic resources (tokenizers, stopwords, etc.)\n",
    "nltk.download()\n",
    "\n",
    "# ğŸ’¡ This will open a small window to download NLTK data files such as:\n",
    "# punkt, stopwords, wordnet, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0f03e",
   "metadata": {},
   "source": [
    "## ğŸ§¬ Ù…ÙƒØªØ¨Ø© spaCy | The spaCy Library\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ **Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**  \n",
    "> Ù…ÙƒØªØ¨Ø© Ù‚ÙˆÙŠØ© ÙˆØ³Ø±ÙŠØ¹Ø© ØªÙØ³ØªØ®Ø¯Ù… Ø¨ÙƒØ«Ø±Ø© ÙÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ù€ NLP Ø§Ù„Ø­Ø¯ÙŠØ«Ø©.  \n",
    "> ØªÙ…ØªØ§Ø² Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù†ÙˆÙŠØ©.\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ **In English:**  \n",
    "> A powerful and modern NLP library, known for speed and high accuracy in tasks  \n",
    "> like sentence parsing, part-of-speech tagging, and named entity recognition.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5285edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Installing spacy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Check version\n",
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad83e92",
   "metadata": {},
   "source": [
    "### ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ | Downloading the Language Model\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ Ø¨Ø¹Ø¯ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø©ØŒ Ù†Ø­ØªØ§Ø¬ Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ Ø¬Ø§Ù‡Ø² Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ.  \n",
    "> ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù€ **Anaconda Prompt** Ø£Ùˆ Ø§Ù„Ù€ **Terminal**:\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ After installing spaCy, we need to download a language model to analyze text.  \n",
    "> Use one of the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "# or\n",
    "python -m spacy download en_core_web_lg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae872ebd",
   "metadata": {},
   "source": [
    "## ğŸ”¤ Ù…ÙƒØªØ¨Ø© re | Regular Expressions\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ Ù…ÙƒØªØ¨Ø© Ø¨Ø§ÙŠØ«ÙˆÙ† Ø§Ù„Ù…Ø®ØµÙ‘ØµØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø®Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© (Regex).  \n",
    "> ØªÙØ³ØªØ®Ø¯Ù… Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø£Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù†Ù…Ø§Ø· Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¬Ù…Ù„.\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ Pythonâ€™s built-in library for pattern matching and text cleaning using regular expressions.  \n",
    "> Often used for preprocessing tasks like removing punctuation or extracting word patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Ø£Ù†Ø§ Ø£Ø­Ø¨ NLP! ğŸ˜ Ù‡Ù„ ØªÙÙ‡Ù… Ø§Ù„Ø±Ù…ÙˆØ²ØŸ 2025\"\n",
    "cleaned = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "print(\"Ù‚Ø¨Ù„:\", text)\n",
    "print(\"Ø¨Ø¹Ø¯:\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc394b",
   "metadata": {},
   "source": [
    "> âœ… **Ø§Ù„Ø¢Ù† Ø¨ÙŠØ¦ØªÙ†Ø§ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø¬Ø§Ù‡Ø²Ø©.**  \n",
    "> ÙÙŠ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù‚Ø§Ø¯Ù… Ø³Ù†ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© ÙˆÙ‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆØ§Ù‡Ø§ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§.\n",
    ">\n",
    "> âœ… **Our environment is now ready!**  \n",
    "> Next, weâ€™ll learn how to read and process text files effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d33e74",
   "metadata": {},
   "source": [
    "# ğŸ“‚ Ø§Ù„Ù‚Ø³Ù… 2: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© | Working with Text Files\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ **Ø§Ù„Ù‡Ø¯Ù:**  \n",
    "> ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø³Ù†ØªØ¹Ù„Ù‘Ù… ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ØŒ Ù‚Ø±Ø§Ø¡Ø©ØŒ ÙˆÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© ÙÙŠ Ø¨Ø§ÙŠØ«ÙˆÙ† â€”  \n",
    "> ÙˆÙ‡ÙŠ Ù…Ù‡Ø§Ø±Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù‚Ø¨Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ **Goal:**  \n",
    "> In this section, we'll learn how to create, read, and write text files in Python â€”  \n",
    "> an essential step before processing linguistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd4c9e",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¦Ø© **Jupyter Notebook**ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„Ø³Ø­Ø±ÙŠØ© `%%writefile`  \n",
    "> Ø§Ù„ØªÙŠ ØªÙƒØªØ¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ù„ÙŠØ© Ø¥Ù„Ù‰ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ Inside **Jupyter Notebook**, you can create a text file directly using the magic command `%%writefile`,  \n",
    "> which saves the cellâ€™s content into a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad38424",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test.txt\n",
    "bla bla bla bla bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b73e2",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© `open()` Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØŒ Ø«Ù… Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `.read()`.  \n",
    "> ğŸ‡¬ğŸ‡§ Use the `open()` function to create a file object, and call `.read()` to view its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open('test.txt')\n",
    "my_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba9f45",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ø¥Ø°Ø§ Ø§Ø³ØªØ¯Ø¹ÙŠØª `read()` Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ø©ØŒ Ù„Ù† ÙŠØ¸Ù‡Ø± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¬Ø¯Ø¯Ù‹Ø§ â€” Ù„Ø£Ù† Ù…Ø¤Ø´Ø± Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ù.  \n",
    "> ğŸ‡¬ğŸ‡§ If you call `read()` multiple times, it returns an empty string because the read cursor has reached the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file.read()  # returns ''\n",
    "my_file.seek(0) # return cursor to beginning\n",
    "my_file.read()  # now works again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file.seek(0)\n",
    "my_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always close the file when finished â€” it frees up system resources.\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad6be0",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ø§Ù„ÙˆØ¶Ø¹ `w+` ÙŠØ¹Ù†ÙŠ **Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙˆØ§Ù„Ù‚Ø±Ø§Ø¡Ø©**ØŒ Ù„ÙƒÙ† Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ Ø³ÙŠØªÙ… Ø­Ø°Ù Ù…Ø­ØªÙˆØ§Ù‡.  \n",
    "> ğŸ‡¬ğŸ‡§ The `w+` mode means **write and read**, but it **overwrites** any existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbce68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open('test.txt','w+')\n",
    "my_file.write('This is a new first line')\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab637754",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ø§Ù„ÙˆØ¶Ø¹ `a+` ÙŠØ¹Ù†ÙŠ **Ø§Ù„ÙƒØªØ§Ø¨Ø© Ù…Ø¹ Ø§Ù„Ø¥Ø¶Ø§ÙØ© (append)** â€” Ù„Ø§ ÙŠØ­Ø°Ù Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚Ø¯ÙŠÙ….  \n",
    "> ğŸ‡¬ğŸ‡§ The `a+` mode allows **writing while appending** without deleting previous content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc79e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open('test.txt','a+')\n",
    "my_file.write('\\nThis line is being appended to test.txt')\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a795022",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø³Ø­Ø±ÙŠ `%%writefile -a` Ù„Ø¥Ø¶Ø§ÙØ© Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø®Ù„ÙŠØ©.  \n",
    "> ğŸ‡¬ğŸ‡§ You can use `%%writefile -a` magic to **append** text to a file directly from a notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a test.txt\n",
    "This is more text being appended to test.txt\n",
    "And another line here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using with open() is recommended because it automatically closes the file afterward.\n",
    "with open('test.txt','r') as txt:\n",
    "    first_line = txt.readlines()[0]\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1066fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','r') as txt:\n",
    "    for line in txt:\n",
    "        print(line, end='')  # end='' removes extra line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab59def",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ù…Ù„ÙØ§Øª CSV (Comma-Separated Values) Ù‡ÙŠ Ù…Ù† Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ©.  \n",
    "> ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© **pandas**.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ CSV files (Comma-Separated Values) are among the most common formats for tabular data.  \n",
    "> They can be read easily using the **pandas** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a136de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('01.csv', skiprows=2)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('02.xlsx', skiprows=2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('03.csv', 'w')\n",
    "outfile.write('a')\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('04.xls', 'w')\n",
    "outfile.write('a')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73432d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('5.txt','w')  # write\n",
    "f.write('write this line in the file')\n",
    "f.close()\n",
    "\n",
    "f = open('5.txt','r')  # read\n",
    "for a in f:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('5.txt','a')  # append\n",
    "f.write('\\nmore lines')\n",
    "f.close()\n",
    "\n",
    "f = open('5.txt','r')  # read again\n",
    "for a in f:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = pd.DataFrame(pd.Series(np.random.rand(10000)))\n",
    "data.to_csv('6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bfa54",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ±Ù…ÙŠØ² `encoding=\"utf8\"` Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø£Ùˆ Ø§Ù„ÙƒØªØ§Ø¨Ø©ØŒ  \n",
    "> ÙˆØ¥Ù„Ø§ Ù‚Ø¯ ØªØ¸Ù‡Ø± Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ When working with Arabic (or any non-Latin script),  \n",
    "> always use `encoding=\"utf8\"` when reading or writing text files to avoid garbled text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('7.txt','w', encoding=\"utf8\") \n",
    "f.write('Ø³Ø·ÙˆØ± Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©')\n",
    "f.close()\n",
    "\n",
    "f = open('7.txt','a', encoding=\"utf8\") \n",
    "f.write('\\nØ³Ø·Ø± Ø«Ø§Ù†ÙŠ')\n",
    "f.write('\\nØ³Ø·Ø± Ø«Ø§Ù„Ø«')\n",
    "f.write('\\nØ³Ø·Ø± Ø±Ø§Ø¨Ø¹')\n",
    "f.write('\\nØ³Ø·Ø± Ø£Ø®ÙŠØ±')\n",
    "f.close()\n",
    "\n",
    "f = open('7.txt','r', encoding=\"utf8\") \n",
    "for a in f:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595ec67",
   "metadata": {},
   "source": [
    "> âœ… **Ø®Ù„Ø§ØµØ© Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…:**  \n",
    "> ØªØ¹Ù„Ù…Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ ÙˆÙ‚Ø±Ø§Ø¡Ø© ÙˆÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©ØŒ ÙˆÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ CSV ÙˆExcelØŒ  \n",
    "> Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø£Ù…Ø§Ù†.  \n",
    ">\n",
    "> âœ… **Summary:**  \n",
    "> We learned how to create, read, and write text files, handle CSV and Excel formats,  \n",
    "> and properly manage Arabic text using UTF-8 encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1068061",
   "metadata": {},
   "source": [
    "# ğŸ“‘ Ø§Ù„Ù‚Ø³Ù… 3: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ PDF | Working with PDF Files\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ **Ø§Ù„Ù‡Ø¯Ù:**  \n",
    "> ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø³Ù†ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ù‚Ø±Ø§Ø¡Ø©ØŒ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµØŒ ÙˆØ¯Ù…Ø¬ Ø§Ù„ØµÙØ­Ø§Øª Ø£Ùˆ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ù„ÙØ§Øª PDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© **PyPDF2**.\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ **Goal:**  \n",
    "> In this section, weâ€™ll learn how to read, extract text, and merge PDF files using the **PyPDF2** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Install PyPDF2\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter, PdfMerger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efab89",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ Ù†ÙØªØ­ Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ `rb` (read binary)ØŒ Ù„Ø£Ù† Ù…Ù„ÙØ§Øª PDF Ù„ÙŠØ³Øª Ù†ØµÙˆØµÙ‹Ø§ Ø¹Ø§Ø¯ÙŠØ©.  \n",
    "> Ø«Ù… Ù†Ø³ØªØ®Ø¯Ù… `PdfFileReader` Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù.\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ We open the file in binary mode `rb` (read binary), since PDF files are not plain text.  \n",
    "> Then we use `PdfFileReader` to access its pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('US_Declaration.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pages Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª \n",
    "pdf_reader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In modern versions of PyPDF2, we use ÙÙŠ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù…Ù† PyPDF2ØŒ Ù†Ø³ØªØ®Ø¯Ù…\n",
    "len(pdf_reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ğŸ‡¸ğŸ‡¦ Ù‡Ø°Ø§ ÙŠØ¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù„ÙØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø£ÙŠ ØµÙØ­Ø© Ø¹Ø¨Ø± Ø±Ù‚Ù…Ù‡Ø§ (Ø¨Ø¯Ø§ÙŠØ© Ù…Ù† 0).\n",
    "ğŸ‡¬ğŸ‡§ This shows a list of all pages; each can be accessed by index (starting from 0). \"\"\"\n",
    "\n",
    "pdf_reader.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a one-page text Ù‚Ø±Ø§Ø¡Ø© Ù†Øµ Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ù‡\n",
    "page_one = pdf_reader.getPage(0)\n",
    "page_one_text = page_one.extractText()\n",
    "print(page_one_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern Update (2025) Ù†Ø³Ø®Ø© Ø­Ø¯ÙŠØ«Ø©\n",
    "page_one = pdf_reader.pages[0]\n",
    "print(page_one.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the text and splitting it into words Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙˆØªÙ‚Ø³ÙŠÙ…Ø© Ø§Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª\n",
    "print(pdf_reader.getPage(0).extractText().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire file, page by page. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ù‹Ø§ ØµÙØ­Ø© Ø¨ØµÙØ­Ø©\n",
    "f = open('US_Declaration.pdf', 'rb')\n",
    "\n",
    "# List to store text for each page\n",
    "pdf_text = [0]  # placeholder to make page 1 = index 1\n",
    "\n",
    "pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "\n",
    "for p in range(pdf_reader.numPages):\n",
    "    page = pdf_reader.getPage(p)\n",
    "    pdf_text.append(page.extractText())\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cb9b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Modern Update (2025) Ø§Ù„Ù†Ø³Ø®Ù‡ Ø§Ù„Ø§Ø­Ø¯Ø«\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[0;32m      3\u001b[0m pdf_reader \u001b[38;5;241m=\u001b[39m PdfReader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS_Declaration.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m pdf_text \u001b[38;5;241m=\u001b[39m [page\u001b[38;5;241m.\u001b[39mextract_text() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_reader\u001b[38;5;241m.\u001b[39mpages]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "# Modern Update (2025) Ø§Ù„Ù†Ø³Ø®Ù‡ Ø§Ù„Ø§Ø­Ø¯Ø«\n",
    "from PyPDF2 import PdfReader\n",
    "pdf_reader = PdfReader(\"US_Declaration.pdf\")\n",
    "pdf_text = [page.extract_text() for page in pdf_reader.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e75a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new PDF file and copy a page into it. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PDF Ø¬Ø¯ÙŠØ¯ ÙˆÙ†Ø³Ø® ØµÙØ­Ø© Ø¥Ù„ÙŠÙ‡\n",
    "f = open('US_Declaration.pdf','rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "first_page = pdf_reader.getPage(0)\n",
    "\n",
    "pdf_writer = PyPDF2.PdfFileWriter()\n",
    "pdf_writer.addPage(first_page)\n",
    "\n",
    "pdf_output = open(\"new file.pdf\", \"wb\")\n",
    "pdf_writer.write(pdf_output)\n",
    "pdf_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640aefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern Update (2025) Ø§Ù„Ù†Ø³Ø®Ù‡ Ø§Ù„Ø§Ø­Ø¯Ø«\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "reader = PdfReader(\"US_Declaration.pdf\")\n",
    "writer = PdfWriter()\n",
    "writer.add_page(reader.pages[0])\n",
    "with open(\"new_file.pdf\", \"wb\") as out:\n",
    "    writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38121424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PDF files. Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª PDF\n",
    "from PyPDF2 import PdfFileMerger, PdfFileReader, PdfFileWriter\n",
    "\n",
    "filename1 = 'F1.pdf'\n",
    "filename2 = 'F2.pdf'\n",
    "\n",
    "merger = PdfFileMerger()\n",
    "merger.append(PdfFileReader(open(filename1, 'rb')))\n",
    "merger.append(PdfFileReader(open(filename2, 'rb')))\n",
    "\n",
    "merger.write(\"merged file 1.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26907de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern Update (2025) Ø§Ù„Ù†Ø³Ø®Ù‡ Ø§Ù„Ø§Ø­Ø¯Ø«\n",
    "from PyPDF2 import PdfMerger\n",
    "merger = PdfMerger()\n",
    "merger.append(\"F1.pdf\")\n",
    "merger.append(\"F2.pdf\")\n",
    "merger.write(\"merged_file_1.pdf\")\n",
    "merger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ed5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge specific pages from different files Ø¯Ù…Ø¬ ØµÙØ­Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ù…Ù„ÙØ§Øª Ù…Ø®ØªÙ„ÙØ©\n",
    "file1 = PdfFileReader(open(filename1, \"rb\"))\n",
    "file2 = PdfFileReader(open(filename2, \"rb\"))\n",
    "\n",
    "output = PdfFileWriter()\n",
    "output.addPage(file1.getPage(5))\n",
    "output.addPage(file2.getPage(3))\n",
    "\n",
    "outputStream = open(\"merged file 2.pdf\", \"wb\")\n",
    "output.write(outputStream)\n",
    "outputStream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern Update (2025) Ø§Ù„Ù†Ø³Ø®Ù‡ Ø§Ù„Ø§Ø­Ø¯Ø«\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "output = PdfWriter()\n",
    "output.add_page(PdfReader(\"F1.pdf\").pages[5])\n",
    "output.add_page(PdfReader(\"F2.pdf\").pages[3])\n",
    "with open(\"merged_file_2.pdf\", \"wb\") as out:\n",
    "    output.write(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b7ab3",
   "metadata": {},
   "source": [
    "> ğŸ‡¸ğŸ‡¦ **ØªØ¹Ù„Ù…Ù†Ø§ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…:**\n",
    "> - ÙƒÙŠÙÙŠØ© Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª PDF ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù†Ù‡Ø§.\n",
    "> - ÙƒÙŠÙÙŠØ© Ø¯Ù…Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PyPDF2.\n",
    "> - Ø§Ù„ØªÙØ±Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ø¬Ø¯ÙŠØ¯Ø© (2025 Update).\n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ **In this section, you learned:**\n",
    "> - How to read and extract text from PDFs.\n",
    "> - How to merge pages and files using PyPDF2.\n",
    "> - The difference between legacy methods and modern 2025 syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64148257",
   "metadata": {},
   "source": [
    "# ğŸ” Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© | Introduction to Regular Expressions\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦ Ù…ÙƒØªØ¨Ø© **re** ÙÙŠ Ø¨Ø§ÙŠØ«ÙˆÙ† ØªÙØ³ØªØ®Ø¯Ù… Ù„Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†ØµÙˆØµØŒ  \n",
    "> Ù„Ø§ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© ÙÙ‚Ø·ØŒ Ø¨Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ **Ø£Ù†Ù…Ø§Ø· (Patterns)** ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙ…Ø«Ù„ ÙƒÙ„Ù…Ø§Øª Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ ØªØ±Ø§ÙƒÙŠØ¨ Ù„ØºÙˆÙŠØ©.  \n",
    ">\n",
    "> ğŸ‡¬ğŸ‡§ The **re** module in Python allows text searching not just by exact words,  \n",
    "> but by defining **patterns** (regular expressions) that can match flexible text structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f862217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266b5331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© Ù…Ø¹ÙŠÙ†Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ | Searching for a Word in Text\n",
    "pattern = 'phone'\n",
    "text = \"The agent's phone number is 408-555-1234. Call soon!\"\n",
    "match = re.search(pattern, text)\n",
    "match.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd07c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ¶Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ù†Øµ | Getting the Match Position\n",
    "match.start(), match.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7fb748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 12)\n",
      "(92, 97)\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "A telephone is a telecommunications device that permits two or more users...\n",
    "Most are smartphones, integrating all mobile communication and many computing needs.\n",
    "'''\n",
    "matches = re.findall(pattern, text)\n",
    "len(matches)\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match.span())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9ed74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 53)\n",
      "(157, 162)\n"
     ]
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ù†Øµ Ø¹Ø±Ø¨ÙŠ | Searching in Arabic Text\n",
    "pattern = 'Ù†Ø³Ø¨ÙŠØ©'\n",
    "text = '''\n",
    "ÙˆØ±Ù‚Ø© Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ† Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø© ÙƒØ§Ù†Øª Ø¹Ù† \"Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ©\"ØŒ Ø§Ù‚ØªØ±Ø­Ù‡Ø§ ÙˆÙ†Ø´Ø±Ù‡Ø§ ÙÙŠ 26 Ø³Ø¨ØªÙ…Ø¨Ø± 1905 Ø¨Ø¹Ù†ÙˆØ§Ù† \"Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆØ¯ÙŠÙ†Ø§Ù…ÙŠÙƒ Ù„Ù„Ø£Ø¬Ø³Ø§Ù… Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©\".\n",
    "Ø§Ø¹ØªØ¨Ø§Ø±Ø§ Ù…Ù† Ø§Ù„ÙŠÙˆÙ…ØŒ Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ù‡ÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„Ø­Ø±ÙƒØ© Ø¨Ø£ÙŠ Ø³Ø±Ø¹Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ø¶Ø¦ÙŠÙ„Ø©.\n",
    "'''\n",
    "match = re.search(pattern, text)\n",
    "match.span()\n",
    "match.start(), match.end()\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "len(matches)\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match.span())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a431e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'408-555-1234'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆØ§ØªÙ | Extracting Phone Numbers\n",
    "text = \"My telephone number is 408-555-1234 and mohamed`s phone is 856-987-6632\"\n",
    "phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d', text)\n",
    "phone.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319cf835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(23, 35), match='408-555-1234'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A better method using a specific repetition Ø·Ø±ÙŠÙ‚Ø© Ø£ÙØ¶Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙƒØ±Ø§Ø± Ù…Ø­Ø¯Ø¯\n",
    "re.search(r'\\d{3}-\\d{3}-\\d{4}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48441ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'586'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø±Ù‚Ù… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª | Grouping Parts of a Phone Number\n",
    "phone_pattern = r'(\\d{3})-(\\d{3})-(\\d{4})'\n",
    "results = re.search(phone_pattern, text)\n",
    "results.group()\n",
    "results.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93397efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(5, 10), match='woman'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© | Search for Multiple Alternatives\n",
    "re.search(r\"man|woman\", \"This man was here.\")\n",
    "re.search(r\"man|woman\", \"This woman was here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9a5010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bat', 'splat', '*plat']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ù…ØªØ´Ø§Ø¨Ù‡Ø© | Searching for Similar Patterns\n",
    "re.findall(r\".at\", \"The cat in the hat sat here.\")\n",
    "re.findall(r\".at\", \"The bat went splat\")\n",
    "re.findall(r\"...at\", \"The bat went splat\")\n",
    "re.findall(r'\\S+at', \"The bat went splat *plat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e670c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ù‚Ù… ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù†Øµ | Find Number at End of Line\n",
    "re.findall(r'\\d$', 'This ends with a number 28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69842b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " 'l',\n",
       " 'i',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'n',\n",
       " 'u',\n",
       " 'm',\n",
       " 'b',\n",
       " 'e',\n",
       " 'r',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„ Ù…Ø§ Ø¹Ø¯Ø§ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… | Find Everything Except Numbers\n",
    "re.findall(r'[^\\d]', '1 is the loneliest number.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c61571b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ù‚Ù… ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Øµ | Find Number at Start of Line\n",
    "re.findall(r'^\\d', '1 is the loneliest number.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30a67cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there are ', ' numbers ', ' inside ', ' this sentence.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… | Extract Text Without Numbers\n",
    "phrase = \"there are 3 numbers 34 inside 5 this sentence.\"\n",
    "re.findall(r'[^\\d]+', phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a679e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'string',\n",
       " 'But',\n",
       " 'it',\n",
       " 'has',\n",
       " 'punctuation',\n",
       " 'How',\n",
       " 'can',\n",
       " 'we',\n",
       " 'remove',\n",
       " 'it']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ù†Øµ | Removing Punctuation from Text\n",
    "test_phrase = 'This is a string! But it has punctuation. How can we remove it?'\n",
    "re.findall('[^!.? ]+', test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2bbf593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string But it has punctuation How can we remove it\n"
     ]
    }
   ],
   "source": [
    "# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ | Cleaning Text\n",
    "clean = ' '.join(re.findall('[^!.? ]+', test_phrase))\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44022240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hypen-words', 'long-ish']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø´Ø±Ø·Ø© (hyphen) | Finding Hyphenated Words\n",
    "text = 'Only find the hypen-words in this sentence. But you do not know how long-ish they are'\n",
    "re.findall(r'[\\w]+-[\\w]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db2ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('586', '986', '9965')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø£Ù…Ø«Ù„Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© | Arabic Hyphenated Words and Patterns\n",
    "text = 'Ø±Ù‚Ù… Ù‡Ø§ØªÙÙŠ Ù‡Ùˆ 586-986-9965   Ùˆ  Ø±Ù‚Ù… Ù‡Ø§ØªÙ Ù…Ø­Ù…Ø¯  Ù‡Ùˆ 826-476-9785   Ùˆ Ø±Ù‚Ù… Ù‡Ø§ØªÙ Ù…Ø´ÙŠØ±Ø© Ù‡Ùˆ 186-006-6855'\n",
    "phone = re.search(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d', text)\n",
    "phone.group()\n",
    "\n",
    "re.search(r'\\d{3}-\\d{3}-\\d{4}', text)\n",
    "phone_pattern = r'(\\d{3})-(\\d{3})-(\\d{4})'\n",
    "results = re.search(phone_pattern, text)\n",
    "results.group()\n",
    "results.group(1), results.group(2), results.group(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5db45a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 11), match='Ù…Ù†ÙŠ'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© | Search for Arabic Words\n",
    "text1 = \"Ø°Ù‡Ø¨ Ù…Ø­Ù…Ø¯ Ø§Ù„ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\"\n",
    "text2 = \"Ø§Ø³ØªÙŠÙ‚Ø¸Øª Ù…Ù†ÙŠ Ù…Ù† Ø§Ù„Ù†ÙˆÙ…\"\n",
    "re.search(r\"Ù…Ø­Ù…Ø¯|Ù…Ù†ÙŠ\", text1)\n",
    "re.search(r\"Ù…Ø­Ù…Ø¯|Ù…Ù†ÙŠ\", text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c97e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø§Ù„Ø·Ø§Ù„Ø¨Ø§Øª', 'Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª', 'Ø¬Ø§Ù…Ø¹Ø§Øª']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªÙ†ØªÙ‡ÙŠ Ø¨Ù€ (Ø§Øª) | Find Arabic Words Ending with (Ø§Øª)\n",
    "text1 = 'Ø°Ù‡Ø¨Øª Ø§Ù„Ø·Ø§Ù„Ø¨Ø§Øª Ù„Ø­Ø¶ÙˆØ± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª ÙÙŠ Ø¬Ø§Ù…Ø¹Ø§ØªÙ‡Ù†'\n",
    "re.findall(r\".Ø§Øª\", text1)\n",
    "re.findall(r\"..Ø§Øª\", text1)\n",
    "re.findall(r\"\\S+Ø§Øª\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a359d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ù‚Ø§Ø¯ Ø¬ÙˆØ±Ø¬ Ù…Ø³Ø§ÙØ© ', ' ÙƒÙŠÙ„Ùˆ Ù…ØªØ±  Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© ', ' Ø§Ù„ÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ ØºÙŠØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ© | Extract Non-Numeric Parts\n",
    "phrase = 'Ù‚Ø§Ø¯ Ø¬ÙˆØ±Ø¬ Ù…Ø³Ø§ÙØ© 50 ÙƒÙŠÙ„Ùˆ Ù…ØªØ±  Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© 8 Ø§Ù„ÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 10'\n",
    "re.findall(r'[^\\d]+', phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "587a6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø§Ø°Ø§ ÙƒÙŠÙ ØªØ¬Ø±Ø¤ Ù‡Ù„ ØªØªØ®ÙŠÙ„ Ø¹Ø§Ù‚Ø¨Ø© Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± ÙˆÙŠÙ„ Ù„Ù‡\n"
     ]
    }
   ],
   "source": [
    "# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… | Clean Arabic Text from Punctuation\n",
    "test_phrase = ' Ù…Ø§Ø°Ø§ ØŸ ØŸ  ÙƒÙŠÙ ØªØ¬Ø±Ø¤ !! , Ù‡Ù„ ØªØªØ®ÙŠÙ„ Ø¹Ø§Ù‚Ø¨Ø© Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± ØŸ ØŸ  , ÙˆÙŠÙ„ Ù„Ù‡ . '\n",
    "re.findall('[^ØŸ,!. ]+', test_phrase)\n",
    "clean = ' '.join(re.findall('[^ØŸ,!. ]+', test_phrase))\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97f9f4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ù†ÙŠÙˆ-ÙŠÙˆØ±Ùƒ', 'Ø³Ø§Ù†-ÙØ±Ø§Ù†Ø³ÙŠØ³ÙƒÙˆ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø´Ø±Ø·Ø© | Find Arabic Hyphenated Words\n",
    "text = 'Ø°Ù‡Ø¨Øª Ø§Ù„ÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù†ÙŠÙˆ-ÙŠÙˆØ±Ùƒ ,  Ùˆ Ù…Ù†Ù‡Ø§ Ø§Ù„ÙŠ Ø³Ø§Ù†-ÙØ±Ø§Ù†Ø³ÙŠØ³ÙƒÙˆ'\n",
    "re.findall(r'[\\w]+-[\\w]+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3024696",
   "metadata": {},
   "source": [
    "# ğŸª„ Ø§Ù„Ø®Ø§ØªÙ…Ø© | Conclusion\n",
    "\n",
    "> ğŸ‡¸ğŸ‡¦  \n",
    "> ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒØŒ ØªØ¹Ø±Ù‘ÙÙ†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ (NLP) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø§ÙŠØ«ÙˆÙ†ØŒ  \n",
    "> Ø¨Ø¯Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„Ù€ PDFØŒ ÙˆØµÙˆÙ„Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© **(Regular Expressions)**.  \n",
    ">\n",
    "> ØªØ¹Ù„Ù…Ù†Ø§ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„Ø­Ø§Ø³Ø¨ Ø£Ù† ÙŠÙ‚Ø±Ø£ØŒ ÙˆÙŠØ¨Ø­Ø«ØŒ ÙˆÙŠØ­Ù„Ù„ Ø§Ù„Ù†ØµÙˆØµ â€” Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© â€” Ù…Ø«Ù„ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§.  \n",
    "> Ø±Ø£ÙŠÙ†Ø§ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ø¨Ø³ÙŠØ·Ø© Ù…Ø«Ù„ **re** Ùˆ **pandas** Ø£Ù† Ù†Ø¨Ù†ÙŠ Ø£Ø³Ø§Ø³Ù‹Ø§ Ù‚ÙˆÙŠÙ‹Ø§ Ù„Ø£ÙŠ Ù…Ø´Ø±ÙˆØ¹ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù€ NLP.  \n",
    ">\n",
    "> âœ¨ ØªØ°ÙƒÙ‘Ø±: Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„Ù€ NLP ÙŠØ¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… â€” ÙˆÙƒÙŠÙ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡ Ø¨Ø­Ø°Ø± ÙˆØ°ÙƒØ§Ø¡.\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ‡¬ğŸ‡§  \n",
    "> In this notebook, we explored the essential foundations of Natural Language Processing (NLP) using Python â€”  \n",
    "> starting from handling text and PDF files, up to searching within text using **Regular Expressions (re)**.  \n",
    ">\n",
    "> We learned how computers can read, process, and analyze language â€” step by step â€” almost like humans do.  \n",
    "> With simple yet powerful tools like **re** and **pandas**, weâ€™ve built a solid foundation for future NLP work.  \n",
    ">\n",
    "> âœ¨ Remember: True NLP begins with raw text â€” and how wisely you choose to handle it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
