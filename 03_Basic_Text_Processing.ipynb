{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29235b3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ŸÖŸÇÿØŸÖÿ© ÿßŸÑŸÇÿ≥ŸÖ ÿßŸÑÿ´ÿßŸÑÿ´ | ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ©\n",
    "\n",
    "## üß† üá¨üáß Section 3 Introduction | Basic Text Processing\n",
    "\n",
    "> üá∏üá¶\n",
    "> ÿ®ÿπÿØ ÿ£ŸÜ ÿ™ÿπÿ±ŸëŸÅŸÜÿß ŸÅŸä ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿØŸàÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÑÿ∫ÿ© (ŸÖÿ´ŸÑ Tokenizationÿå POSÿå NER...)ÿå\n",
    "> ŸÜŸÜÿ™ŸÇŸÑ ÿßŸÑÿ¢ŸÜ ÿ•ŸÑŸâ ŸÖÿ±ÿ≠ŸÑÿ© **ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© ŸÑŸÑŸÜÿµŸàÿµ (Basic Text Processing)**ÿå ŸàŸáŸä ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ≥ÿ®ŸÇ ÿ£Ÿä ÿ™ÿ≠ŸÑŸäŸÑ ÿ∞ŸÉŸä ÿ£Ÿà ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ÿπŸÑŸÖ ÿ¢ŸÑŸä.\n",
    "> ŸáŸÜÿß ŸÜÿ®ÿØÿ£ ÿ®ŸÅŸáŸÖ **ŸÖÿπÿßŸÜŸä ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸàÿπŸÑÿßŸÇÿßÿ™Ÿáÿß** ‚Äî ŸÑÿ£ŸÜ ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ŸÑÿß ŸäŸÅŸáŸÖ ÿßŸÑŸÑÿ∫ÿ© ŸÖÿ´ŸÑ ÿßŸÑÿ®ÿ¥ÿ±ÿå ÿ®ŸÑ Ÿäÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑ ŸÖŸÜÿ∑ŸÇŸä Ÿäÿ±ÿ®ÿ∑ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ÿå ŸÖÿπÿßŸÜŸäŸáÿßÿå ŸàŸÖÿ±ÿßÿØŸÅÿßÿ™Ÿáÿß.\n",
    "\n",
    "> üá¨üáß\n",
    "> After learning the core NLP tools (Tokenization, POS, NER...),\n",
    "> we now move to **Basic Text Processing** ‚Äî the foundation before training any intelligent model.\n",
    "> This step focuses on understanding **word meanings and relationships**, since computers need structured ways to connect words, meanings, and contexts.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ üá∏üá¶ ŸÖÿπŸÜŸâ ÿßŸÑŸÉŸÑŸÖÿ© (Word Meaning)\n",
    "\n",
    "---\n",
    "\n",
    "### üìò üá∏üá¶ Lemma Form | üá¨üáß The Base Form\n",
    "\n",
    "> üá∏üá¶\n",
    "> \"ÿßŸÑŸÄ Lemma\" ŸáŸä ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ÿ£Ÿà ÿßŸÑÿ¨ÿ∞ÿ± ÿßŸÑŸÖÿπÿ¨ŸÖŸä ŸÑŸÑŸÉŸÑŸÖÿ© ÿ®ÿπÿØ ÿ•ÿ≤ÿßŸÑÿ© ÿ£Ÿä ÿ™ÿ∫ŸäŸäÿ±ÿßÿ™ ÿµÿ±ŸÅŸäÿ©.\n",
    "> ŸÖÿ´ŸÑŸãÿß:\n",
    ">\n",
    "> * book ‚Üí ŸáŸà ÿßŸÑŸÄ lemma ŸÖŸÜ: booked, booking\n",
    "> * run ‚Üí ŸÖŸÜ: running, ran\n",
    ">\n",
    "> üá¨üáß\n",
    "> The **lemma form** is the base or dictionary form of a word ‚Äî after removing tense or plural inflections.\n",
    "> Example:\n",
    ">\n",
    "> * *book* ‚Üí booked, booking\n",
    "> * *run* ‚Üí running, ran\n",
    "\n",
    "---\n",
    "\n",
    "### üìò üá∏üá¶ Word Form | üá¨üáß Inflected Word\n",
    "\n",
    "> üá∏üá¶\n",
    "> \"ÿßŸÑŸÄ Wordform\" ŸáŸä ÿßŸÑŸÉŸÑŸÖÿ© ÿ®ÿπÿØ ÿ£ŸÜ ÿ™ŸÖÿ™ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿ™ÿ∫ŸäŸäÿ±ÿßÿ™ ÿπŸÑŸäŸáÿß (ÿ≤ŸÖŸÜÿå ÿ¨ŸÖÿπÿå ÿ™ÿµÿ±ŸäŸÅ).\n",
    "> ŸÖÿ´ŸÑ: *bookings*, *runner*, *sung*.\n",
    ">\n",
    "> üá¨üáß\n",
    "> A **word form** is the inflected version of the lemma ‚Äî modified by tense, plural, or derivation.\n",
    "> Example: *bookings*, *runner*, *sung*.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò üá∏üá¶ ÿ™ÿπÿØÿØ ÿßŸÑŸÖÿπÿßŸÜŸä | üá¨üáß Multiple Meanings\n",
    "\n",
    "> üá∏üá¶\n",
    "> ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸàÿßÿ≠ÿØÿ© ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ≠ŸÖŸÑ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ŸÖÿπŸÜŸâ ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "> ŸÖÿ´ŸÑ ŸÉŸÑŸÖÿ© **\"run\"** ÿßŸÑÿ™Ÿä ŸÇÿØ ÿ™ÿπŸÜŸä \"Ÿäÿ¨ÿ±Ÿä\"ÿå ÿ£Ÿà \"ŸäÿØŸäÿ±\"ÿå ÿ£Ÿà \"Ÿäÿ¥ÿ∫ŸëŸÑ\".\n",
    "> ŸÑŸáÿ∞ÿß ÿßŸÑÿ≥ÿ®ÿ®ÿå Ÿäÿ≠ÿ™ÿßÿ¨ ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ÿ•ŸÑŸâ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ŸäÿßŸÇ ŸÇÿ®ŸÑ ÿ£ŸÜ ŸäŸÇÿ±ÿ± ÿ£Ÿä ŸÖÿπŸÜŸâ ŸáŸà ÿßŸÑŸÖŸÇÿµŸàÿØ.\n",
    "\n",
    "> üá¨üáß\n",
    "> A single word can have multiple meanings depending on the context.\n",
    "> For example, **\"run\"** can mean \"to jog\", \"to manage\", or \"to operate\".\n",
    "> That‚Äôs why contextual understanding is essential in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© üá∏üá¶ ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ŸÅŸä ÿßŸÑŸÖÿπÿßŸÜŸä | üá¨üáß Key Word Meaning Concepts\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Homographs | ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÉÿ™ÿßÿ®ÿ©Ÿã\n",
    "\n",
    "> üá∏üá¶\n",
    "> ŸáŸä ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸèŸÉÿ™ÿ® ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ¥ŸÉŸÑ ŸÑŸÉŸÜ ÿ™ÿ≠ŸÖŸÑ ŸÖÿπÿßŸÜŸä ŸÖÿÆÿ™ŸÑŸÅÿ©ÿå ŸÖÿ´ŸÑ:\n",
    "> *book*, *bank*, *run*.\n",
    "> ŸÖÿ´ÿßŸÑ:\n",
    ">\n",
    "> * Ahmed runs 2 km daily.\n",
    "> * Ahmed runs the restaurant.\n",
    ">\n",
    "> Ÿáÿ∞Ÿá ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ™Ÿèÿ∏Ÿáÿ± ŸÉŸäŸÅ ÿ£ŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ŸÜŸÅÿ≥Ÿáÿß ÿ™ÿ∫ŸäŸëÿ± ŸÖÿπŸÜÿßŸáÿß ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    ">\n",
    "> üá¨üáß\n",
    "> **Homographs** are words that share the same spelling but have different meanings.\n",
    "> Example:\n",
    ">\n",
    "> * Ahmed runs 2 km daily.\n",
    "> * Ahmed runs the restaurant.\n",
    ">   The meaning of *run* changes based on its context.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Homophones | ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÜÿ∑ŸÇŸãÿß\n",
    "\n",
    "> üá∏üá¶\n",
    "> ŸÉŸÑŸÖÿßÿ™ ÿ™ŸèŸÜÿ∑ŸÇ ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ¥ŸÉŸÑ ŸàŸÑŸÉŸÜ ÿ™ŸèŸÉÿ™ÿ® ÿ®ÿ¥ŸÉŸÑ ŸÖÿÆÿ™ŸÑŸÅ ŸàŸÑŸáÿß ŸÖÿπÿßŸÜŸç ŸÖÿÆÿ™ŸÑŸÅÿ©:\n",
    ">\n",
    "> * *peace* / *piece*\n",
    "> * *right* / *write*\n",
    ">\n",
    "> Ÿáÿ∞Ÿá ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ™ÿ≥ÿ®ÿ® ŸÖÿ¥ÿßŸÉŸÑ ÿ£ÿ≠ŸäÿßŸÜŸãÿß ŸÅŸä ÿ£ŸÜÿ∏ŸÖÿ© **Text-to-Speech** ŸÑÿ£ŸÜŸáÿß ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿµŸàÿ™ ŸÅŸÇÿ∑.\n",
    ">\n",
    "> üá¨üáß\n",
    "> **Homophones** are words that sound the same but are spelled differently and mean different things.\n",
    "> Examples:\n",
    ">\n",
    "> * *peace* vs. *piece*\n",
    "> * *right* vs. *write*\n",
    ">\n",
    "> These often confuse **speech systems**, since pronunciation alone isn‚Äôt enough to disambiguate meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Synonyms | ÿßŸÑŸÖÿ±ÿßÿØŸÅÿßÿ™\n",
    "\n",
    "> üá∏üá¶\n",
    "> ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ≠ŸÑ ŸÖÿ≠ŸÑ ÿ®ÿπÿ∂Ÿáÿß ŸÅŸä ÿ£ÿ∫ŸÑÿ® ÿßŸÑŸÖŸàÿßÿ∂ÿπ:\n",
    ">\n",
    "> * *big* ‚Üî *large*\n",
    "> * *car* ‚Üî *automobile*\n",
    ">\n",
    "> ŸÑŸÉŸÜ ŸÜÿßÿØÿ±Ÿãÿß ŸÖÿß ÿ™ŸÉŸàŸÜ ŸÉŸÑŸÖÿ™ÿßŸÜ ŸÖÿ™ÿ∑ÿßÿ®ŸÇÿ™ŸäŸÜ ÿ™ŸÖÿßŸÖŸãÿß ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâÿõ ŸÅŸÄ ‚Äúbig brother‚Äù ŸÑÿß ŸäŸÖŸÉŸÜ ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑŸáÿß ÿ®ŸÄ ‚Äúlarge brother‚Äù.\n",
    ">\n",
    "> üá¨üáß\n",
    "> **Synonyms** are words with similar meanings that can often replace each other:\n",
    ">\n",
    "> * *big* ‚Üî *large*\n",
    "> * *car* ‚Üî *automobile*\n",
    ">\n",
    "> Yet, perfect synonymy is rare ‚Äî for example, ‚Äúbig brother‚Äù ‚â† ‚Äúlarge brother‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Antonyms | ÿßŸÑÿ£ÿ∂ÿØÿßÿØ\n",
    "\n",
    "> üá∏üá¶\n",
    "> ŸáŸä ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ŸÖŸÑ ŸÖÿπŸÜŸâ ŸÖÿπÿßŸÉÿ≥Ÿãÿß ŸÑŸÉŸÑŸÖÿ© ÿ£ÿÆÿ±Ÿâÿå ŸÖÿ´ŸÑ:\n",
    ">\n",
    "> * *dark* ‚Üî *light*\n",
    "> * *short* ‚Üî *long*\n",
    ">\n",
    "> Ÿàÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ™ÿÆÿ™ŸÑŸÅ ÿßŸÑÿ£ÿ∂ÿØÿßÿØ ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇÿå ŸÅŸÖÿ´ŸÑÿßŸã ŸÉŸÑŸÖÿ© *big* ÿ∂ÿØŸáÿß *small* ŸÅŸä ÿßŸÑÿ≠ÿ¨ŸÖÿå ŸÑŸÉŸÜ *little* ŸÅŸä ÿßŸÑÿπŸÖÿ±.\n",
    ">\n",
    "> üá¨üáß\n",
    "> **Antonyms** are words with opposite meanings:\n",
    ">\n",
    "> * *dark* ‚Üî *light*\n",
    "> * *short* ‚Üî *long*\n",
    ">\n",
    "> The antonym depends on context ‚Äî *big* vs. *small* (size), or *big* vs. *little* (age).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Hyponyms & Hypernyms | ÿßŸÑŸÅÿ±Ÿàÿπ ŸàÿßŸÑÿ£ÿµŸàŸÑ ÿßŸÑÿØŸÑÿßŸÑŸäÿ©\n",
    "\n",
    "> üá∏üá¶\n",
    "> ÿßŸÑŸÄ **Hyponym** ŸáŸà ÿßŸÑŸÉŸÑŸÖÿ© ‚ÄúÿßŸÑŸÅÿ±ÿπŸäÿ©‚Äù ÿ∂ŸÖŸÜ ŸÅÿ¶ÿ© ÿ£Ÿàÿ≥ÿπ (Hypernym).\n",
    ">\n",
    "> * üçì ÿßŸÑŸÅÿ±ÿßŸàŸÑÿ© ‚Üí Hyponym ŸÖŸÜ ÿßŸÑŸÅŸàÿßŸÉŸá\n",
    "> * üöó ÿßŸÑÿ≥Ÿäÿßÿ±ÿ© ‚Üí Hyponym ŸÖŸÜ ÿßŸÑŸÖÿ±ŸÉÿ®ÿßÿ™\n",
    ">\n",
    "> ŸàÿßŸÑÿπŸÉÿ≥ÿå ÿßŸÑŸÅŸàÿßŸÉŸá ŸàÿßŸÑŸÖÿ±ŸÉÿ®ÿßÿ™ ŸáŸä **Hypernyms** ŸÑÿ™ŸÑŸÉ ÿßŸÑŸÉŸÑŸÖÿßÿ™.\n",
    ">\n",
    "> üá¨üáß\n",
    "> A **Hyponym** is a more specific term within a broader category (**Hypernym**).\n",
    ">\n",
    "> * üçì *Strawberry* ‚Üí Hyponym of *Fruit*\n",
    "> * üöó *Car* ‚Üí Hyponym of *Vehicle*\n",
    ">\n",
    "> Conversely, *Fruit* and *Vehicle* are **Hypernyms**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ŸÑŸÖÿßÿ∞ÿß Ÿáÿ∞ÿß ŸÖŸáŸÖ ŸÅŸä ÿπÿßŸÖ 2025ÿü\n",
    "\n",
    "## üß† üá¨üáß Why This Still Matters in 2025\n",
    "\n",
    "> üá∏üá¶\n",
    "> Ÿáÿ∞Ÿá ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ŸÑŸäÿ≥ÿ™ ŸÑÿ∫ŸàŸäÿ© ŸÅŸÇÿ∑ÿå ÿ®ŸÑ ÿ£ÿ≥ÿßÿ≥Ÿäÿ© ŸÅŸä ÿ™ÿØÿ±Ÿäÿ® ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ≠ÿØŸäÿ´ÿ©.\n",
    "> ŸÅŸä ÿπÿßŸÖ 2025ÿå Ÿäÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ (Synonyms, Hypernyms...) ÿØÿßÿÆŸÑ ŸÜŸÖÿßÿ∞ÿ¨ **ÿßŸÑÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿØŸÑÿßŸÑŸä** ŸÖÿ´ŸÑ\n",
    "> **Word2Vecÿå GloVeÿå BERTÿå ŸàAraBERT** ŸÑÿ™ŸÇÿØŸäÿ± ÿßŸÑŸÖÿπŸÜŸâ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    ">\n",
    "> ÿ£Ÿäÿ∂Ÿãÿßÿå ÿ™ÿ≥ÿßÿπÿØ Ÿáÿ∞Ÿá ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ŸÅŸä ÿ™ÿ≠ÿ≥ŸäŸÜ **ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ¥ÿßÿπÿ± (Sentiment Analysis)**ÿå Ÿà**ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ© (Chatbots)**ÿå\n",
    "> Ÿà**ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä (Semantic Search)** ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä ŸÖÿ≠ÿ±ŸÉÿßÿ™ ŸÖÿ´ŸÑ Google ŸàOpenAI.\n",
    "\n",
    "> üá¨üáß\n",
    "> These semantic relationships power modern NLP models.\n",
    "> In **2025**, systems like **BERT, AraBERT, and GloVe** use synonymy, antonymy, and hypernymy to model meaning in vector space.\n",
    "> They‚Äôre crucial for tasks like **Sentiment Analysis**, **Chatbots**, and **Semantic Search**,\n",
    "> enabling machines to infer meaning even when exact words differ.\n",
    "\n",
    "---\n",
    "\n",
    "## ü™Ñ üá∏üá¶ ÿÆŸÑÿßÿµÿ© ÿßŸÑŸÇÿ≥ŸÖ | üá¨üáß Section Summary\n",
    "\n",
    "> üá∏üá¶\n",
    "> ÿ™ÿπŸÑŸÖŸÜÿß ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ ÿ£ŸÜ ŸÅŸáŸÖ ŸÖÿπÿßŸÜŸä ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸáŸà ÿ≠ÿ¨ÿ± ÿßŸÑÿ£ÿ≥ÿßÿ≥ ŸÑÿ£Ÿä ŸÖÿπÿßŸÑÿ¨ÿ© ŸÑÿ∫ŸàŸäÿ©.\n",
    "> ŸÅŸÇÿ®ŸÑ ÿ£ŸÜ ‚ÄúŸäŸÅŸáŸÖ‚Äù ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ÿßŸÑŸÜÿµÿå Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿπÿ±ŸÅ ÿßŸÑÿπŸÑÿßŸÇÿ© ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ‚Äî ŸÖŸÜ ŸáŸä ÿßŸÑÿ¨ÿ∞ÿ±ÿå ŸÖŸÜ ŸáŸä ÿßŸÑŸÅÿ±ÿπÿå ŸàŸÖŸÜ ÿ™ÿ¥ÿ™ÿ±ŸÉ ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿ™ÿπÿßÿ±ÿ∂Ÿá.\n",
    ">\n",
    "> üá¨üáß\n",
    "> In summary, understanding word meaning and relationships is the foundation of NLP.\n",
    "> Before a computer can ‚Äúunderstand‚Äù text, it must know how words relate ‚Äî which share meaning, which oppose, and which belong together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107035e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò ÿßŸÑŸÇÿ≥ŸÖ ÿßŸÑÿ´ÿßŸÜŸä : üá∏üá¶ ÿ™ÿ∂ŸÖŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ | üá¨üáß Word Embedding\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä\n",
    "\n",
    "ÿ™ÿ∂ŸÖŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ **(Word Embedding)** ŸáŸà ŸÖŸÜ ÿ£ŸáŸÖ ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ© (NLP).\n",
    "ÿßŸÑŸÅŸÉÿ±ÿ© ÿ®ÿ®ÿ≥ÿßÿ∑ÿ©: ŸÉŸÑ ŸÉŸÑŸÖÿ© Ÿäÿ™ŸÖ ÿ™ÿ≠ŸàŸäŸÑŸáÿß ÿ•ŸÑŸâ **ŸÖÿ¨ŸÖŸàÿπÿ© ÿ£ÿ±ŸÇÿßŸÖ (Vector)** ÿ™ŸÖÿ´ŸÑ ŸÖÿπŸÜÿßŸáÿß ŸÅŸä ŸÅÿ∂ÿßÿ° ÿπÿØÿØŸäÿå\n",
    "ŸàŸáÿ∞Ÿá ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ÿ™Ÿèÿπÿ®Ÿëÿ± ÿπŸÜ ÿπŸÑÿßŸÇÿßÿ™ ÿßŸÑŸÉŸÑŸÖÿ© ŸÖÿπ ÿ∫Ÿäÿ±Ÿáÿß ‚Äî ŸÖÿØŸâ ŸÇÿ±ÿ®Ÿáÿß ÿ£Ÿà ÿ®ÿπÿØŸáÿß ŸÖŸÜ ŸÉŸÑŸÖÿßÿ™ ÿ£ÿÆÿ±Ÿâ.\n",
    "\n",
    "ÿ™ÿÆŸäŸÑ ÿ£ŸÜ ŸÑÿØŸäŸÜÿß ŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ:\n",
    "**\"ÿµÿ®ÿ±\" ‚Äì \"ÿ±ÿ¨ŸÑ\" ‚Äì \"ÿ™ŸÅÿßÿ≠ÿ©\" ‚Äì \"ŸÉŸÑÿ®\" ‚Äì \"ŸÉÿ™ÿßÿ®\"**\n",
    "ŸäŸÖŸÉŸÜŸÜÿß ÿ∑ÿ±ÿ≠ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ™ÿµŸÅ Ÿáÿ∞Ÿá ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ:\n",
    "\n",
    "| ÿßŸÑÿ≥ÿ§ÿßŸÑ               | ÿµÿ®ÿ± | ÿ±ÿ¨ŸÑ | ÿ™ŸÅÿßÿ≠ÿ© | ŸÉŸÑÿ® | ŸÉÿ™ÿßÿ® |\n",
    "| -------------------- | --- | --- | ----- | --- | ---- |\n",
    "| ŸáŸÑ Ÿáÿ∞ÿß ÿßŸÑÿ¥Ÿäÿ° ÿ≠Ÿäÿü     | ŸÑÿß  | ŸÜÿπŸÖ | ŸÑÿß    | ŸÜÿπŸÖ | ŸÑÿß   |\n",
    "| ŸáŸÑ Ÿäÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ™ÿ≠ÿØÿ´ÿü    | ŸÑÿß  | ŸÜÿπŸÖ | ŸÑÿß    | ŸÑÿß  | ŸÑÿß   |\n",
    "| ŸáŸÑ ŸáŸà ŸÖŸÑŸÖŸàÿ≥ÿü         | ŸÑÿß  | ŸÜÿπŸÖ | ŸÜÿπŸÖ   | ŸÜÿπŸÖ | ŸÜÿπŸÖ  |\n",
    "| ŸáŸÑ ŸäŸÖŸÉŸÜ ÿ£ŸÉŸÑŸáÿü        | ŸÑÿß  | ŸÑÿß  | ŸÜÿπŸÖ   | ŸÑÿß  | ŸÑÿß   |\n",
    "| ŸáŸÑ ŸäŸÖŸÉŸÜ ÿ®ŸäÿπŸá Ÿàÿ¥ÿ±ÿßÿ°Ÿáÿü | ŸÑÿß  | ŸÑÿß  | ŸÜÿπŸÖ   | ŸÜÿπŸÖ | ŸÜÿπŸÖ  |\n",
    "\n",
    "ŸÉŸÑ \"ŸÜÿπŸÖ\" Ÿà\"ŸÑÿß\" ŸäŸÖŸÉŸÜ ÿ™ÿ≠ŸàŸäŸÑŸáÿß ÿ•ŸÑŸâ ŸÇŸäŸÖ ÿ±ŸÇŸÖŸäÿ© (ŸÖÿ´ŸÑ 1 Ÿà0).\n",
    "ŸàŸáŸÉÿ∞ÿß ŸÜŸÉŸàŸëŸÜ **ŸÖÿµŸÅŸàŸÅÿ© ÿ±ŸÇŸÖŸäÿ© (Embedding Matrix)** ÿ™ŸÖÿ´ŸÑ ŸÉŸÑ ŸÉŸÑŸÖÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿÆÿµÿßÿ¶ÿµŸáÿß.\n",
    "ŸÑŸÉŸÜ ŸÅŸä ÿßŸÑŸàÿßŸÇÿπÿå ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ÿßŸÑÿ≠ÿØŸäÿ´ ŸÑÿß Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ© ŸÖÿ≠ÿØŸàÿØÿ© ŸÉŸáÿ∞Ÿáÿå\n",
    "ÿ®ŸÑ ÿπŸÑŸâ **ŸÖŸÑÿßŸäŸäŸÜ \"ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ©\"** ÿßŸÑÿ™Ÿä Ÿäÿ™ÿπŸÑŸÖŸáÿß ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖŸÜ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿ∂ÿÆŸÖÿ©.\n",
    "ÿ®ŸÖÿπŸÜŸâ ÿ¢ÿÆÿ±:\n",
    "ŸÉŸÑ ŸÉŸÑŸÖÿ© ŸÑŸáÿß \"ÿ®ÿµŸÖÿ© ÿ±ŸÇŸÖŸäÿ©\" ÿÆÿßÿµÿ© ÿ®Ÿáÿß ‚Äî ÿ™ÿ¥ÿ®Ÿá **ÿßŸÑŸÄDNA ÿßŸÑŸÑÿ∫ŸàŸä** ŸÑŸÑŸÉŸÑŸÖÿ©. üß¨\n",
    "\n",
    "ŸÉŸÑ ÿ±ŸÇŸÖ ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÖÿ™ÿ¨Ÿá (vector) ŸäŸÖÿ´ŸÑ \"ÿ•ÿ¨ÿßÿ®ÿ©\" ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ ÿ∂ŸÖŸÜŸä ÿπŸÜ ÿßŸÑŸÉŸÑŸÖÿ©ÿå\n",
    "ŸÑŸäÿ≥ ŸÅŸÇÿ∑ ŸÜÿπŸÖ ÿ£Ÿà ŸÑÿß (0 ÿ£Ÿà 1)ÿå ÿ®ŸÑ ŸÇŸäŸÖ ŸÜÿ≥ÿ®Ÿäÿ© ŸÖÿ´ŸÑ **0.2 ÿ£Ÿà 0.87**ÿå\n",
    "ÿ™ÿπŸÉÿ≥ ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ£Ÿà ÿßŸÑÿßÿ±ÿ™ÿ®ÿßÿ∑ ÿ®ŸÖÿπŸÜŸâ ŸÖÿπŸäŸÜ (ŸÉÿßŸÑŸÇŸàÿ©ÿå ÿßŸÑŸÖÿ¥ÿßÿπÿ±ÿå ÿßŸÑÿ≤ŸÖŸÜÿå ÿ•ŸÑÿÆ).\n",
    "\n",
    "ŸÉŸÑŸÖÿß ŸÉÿßŸÜÿ™ Ÿáÿ∞Ÿá ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ÿ£ŸÉÿ´ÿ± ÿØŸÇÿ©ÿå ÿ£ÿµÿ®ÿ≠ ÿßŸÑŸÖÿ™ÿ¨Ÿá ÿ£ŸÉÿ´ÿ± ŸÇÿØÿ±ÿ© ÿπŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿ≠ŸÇŸäŸÇŸä ŸÑŸÑŸÉŸÑŸÖÿ©ÿå\n",
    "Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸäŸÖŸÉŸÜŸÜÿß ŸÖÿπÿ±ŸÅÿ© ŸÖÿØŸâ ÿ™ÿ¥ÿßÿ®ŸáŸáÿß ÿ£Ÿà ÿßÿÆÿ™ŸÑÿßŸÅŸáÿß ÿπŸÜ ŸÉŸÑŸÖÿßÿ™ ÿ£ÿÆÿ±Ÿâ.\n",
    "\n",
    "> ‚öôÔ∏è ŸÖÿ´ŸÑŸãÿß:\n",
    "> ŸäŸÖŸÉŸÜ ŸÑŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿπÿ±ŸÅÿ© ÿ£ŸÜ ÿßŸÑÿπŸÑÿßŸÇÿ© ÿ®ŸäŸÜ \"ŸÖŸÑŸÉ\" Ÿà\"ŸÖŸÑŸÉÿ©\"\n",
    "> ÿ™ÿ¥ÿ®Ÿá ÿßŸÑÿπŸÑÿßŸÇÿ© ÿ®ŸäŸÜ \"ÿ±ÿ¨ŸÑ\" Ÿà\"ÿßŸÖÿ±ÿ£ÿ©\" ‚Äî ÿ®ÿ¥ŸÉŸÑ ÿ£Ÿàÿ™ŸàŸÖÿßÿ™ŸäŸÉŸä ÿ™ŸÖÿßŸÖŸãÿß ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑÿ≠ÿ≥ÿßÿ®ÿßÿ™ ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿ© ÿØÿßÿÆŸÑ ŸÅÿ∂ÿßÿ° ÿßŸÑŸÉŸÑŸÖÿßÿ™.\n",
    "\n",
    "ŸÉŸÖÿß ÿ£ŸÜ ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÑÿß ÿ™ŸèŸÇÿßÿ≥ ŸÅŸÇÿ∑ ÿ®ÿßŸÑÿ®ÿπÿØ (Euclidean Distance)ÿå\n",
    "ŸÑÿ£ŸÜ ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ŸÇÿØ ÿ™ŸÉŸàŸÜ ÿÆÿßÿØÿπÿ© ÿ£ÿ≠ŸäÿßŸÜŸãÿßÿå ÿ®ŸÑ ÿßŸÑÿ£ŸÅÿ∂ŸÑ ŸÇŸäÿßÿ≥ **ÿßŸÑÿ≤ÿßŸàŸäÿ© ÿ®ŸäŸÜ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™** ÿπÿ®ÿ±\n",
    "**Cosine Similarity** ŸÑÿ™ÿ≠ÿØŸäÿØ ŸÖÿØŸâ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ŸÅŸä ÿßŸÑÿßÿ™ÿ¨ÿßŸá ŸàÿßŸÑŸÖÿπŸÜŸâ.\n",
    "\n",
    "ÿ£ŸÖÿß ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑÿ¨ŸÖŸÑÿå ŸÅÿßŸÑŸÄ *Word Embedding* ŸÑÿß ŸäŸÉŸàŸÜ ÿØŸÇŸäŸÇŸãÿß ÿ¨ÿØŸãÿßÿå\n",
    "ŸÑÿ£ŸÜŸá ŸäŸÇŸäÿ≥ ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÉŸÑŸÖÿßÿ™ÿå ŸàŸÑŸäÿ≥ ÿ®ÿßŸÑÿ∂ÿ±Ÿàÿ±ÿ© **ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿπÿßŸÖ ŸÑŸÑÿ≥ŸäÿßŸÇ**.\n",
    "ÿπŸÑŸâ ÿ≥ÿ®ŸäŸÑ ÿßŸÑŸÖÿ´ÿßŸÑÿå ÿßŸÑÿ¨ŸÖŸÑÿ™ÿßŸÜ:\n",
    "\n",
    "> ‚ÄúI love school‚Äù Ÿà ‚ÄúI hate school‚Äù\n",
    "> ŸÇÿØ ÿ™ŸÉŸàŸÜÿßŸÜ ŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ™ŸäŸÜ ÿ±ŸÇŸÖŸäŸãÿßÿå ÿ±ÿ∫ŸÖ ÿßÿÆÿ™ŸÑÿßŸÅ ÿßŸÑŸÖÿπŸÜŸâÿå ŸÑÿ£ŸÜ ŸÉŸÑÿ™ŸäŸáŸÖÿß ÿ™ÿ™ÿ≠ÿØÿ´ÿßŸÜ ÿπŸÜ ÿßŸÑŸÖÿØÿ±ÿ≥ÿ© ŸàÿßŸÑŸÖÿ¥ÿßÿπÿ±.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß The Basic Concept\n",
    "\n",
    "**Word Embedding** is one of the fundamental ideas in NLP.\n",
    "It transforms each word into a **vector of numbers** that captures its meaning and relationships with other words.\n",
    "These vectors act like the **word‚Äôs DNA**, encoding its semantic traits numerically. üß¨\n",
    "\n",
    "You can imagine this as answering a massive set of ‚Äúhidden questions‚Äù ‚Äî\n",
    "not just simple *yes/no* ones, but nuanced, continuous values like 0.27 or 0.89\n",
    "that describe various aspects of meaning such as tone, context, or sentiment.\n",
    "\n",
    "Thus, every word gets a **unique numerical fingerprint**,\n",
    "allowing models to compute how close or far words are from each other in meaning.\n",
    "\n",
    "> ‚öôÔ∏è For example, the model can figure out relationships like:\n",
    "> **king ‚Äì man + woman ‚âà queen**\n",
    "> automatically, because the embeddings capture deep semantic relationships.\n",
    "\n",
    "However, word embeddings aren‚Äôt perfect for **sentence-level similarity**.\n",
    "Two opposite sentences ‚Äî like *‚ÄúI love school‚Äù* and *‚ÄúI hate school‚Äù* ‚Äî\n",
    "might still appear similar numerically, because both discuss the same topic (*school*)\n",
    "and share emotional or contextual proximity.\n",
    "\n",
    "When measuring similarity, using **Cosine Similarity** is preferred over Euclidean distance,\n",
    "since cosine focuses on the *direction* (meaning) rather than just numerical distance,\n",
    "providing more accurate results in semantic space.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üìä üá∏üá¶ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ Euclidean Distance Ÿà Cosine Similarity\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/cosine & Euclidean.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ© ÿπŸÜÿØŸÉ ÿ´ŸÑÿßÿ´ ŸÖŸÇÿßŸÑÿßÿ™ (ÿ£Ÿà ŸÉŸàÿ±ÿ®ÿ≥):\n",
    "\n",
    "* **Food corpus** ÿπŸÜÿØ ÿßŸÑŸÜŸÇÿ∑ÿ© (5, 15)\n",
    "* **Agriculture corpus** ÿπŸÜÿØ ÿßŸÑŸÜŸÇÿ∑ÿ© (20, 40)\n",
    "* **History corpus** ÿπŸÜÿØ ÿßŸÑŸÜŸÇÿ∑ÿ© (30, 20)\n",
    "\n",
    "ŸÉŸÑ Ÿàÿßÿ≠ÿØÿ© ŸÖŸèŸÖÿ´ŸëŸéŸÑÿ© ŸÉŸÄ **ŸÖÿ™ÿ¨Ÿá (vector)** ŸÅŸä ŸÅÿ∂ÿßÿ° ÿ´ŸÜÿßÿ¶Ÿä ÿßŸÑÿ£ÿ®ÿπÿßÿØ:\n",
    "ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿ£ŸÅŸÇŸä = ŸÉŸÑŸÖÿ© *disease*\n",
    "ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑÿπŸÖŸàÿØŸä = ŸÉŸÑŸÖÿ© *eggs*\n",
    "\n",
    "### 1Ô∏è‚É£ ŸÖÿßÿ∞ÿß ŸäŸÅÿπŸÑ ÿßŸÑŸÄ Euclidean Distanceÿü\n",
    "\n",
    "**Euclidean distance** ŸáŸà ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÇŸäŸÖÿ© ÿ®ŸäŸÜ ŸÜŸÇÿ∑ÿ™ŸäŸÜ (ÿ≤Ÿä ÿßŸÑŸÖÿ≥ÿ∑ÿ±ÿ© ÿ®ŸäŸÜ ŸÜŸÇÿ∑ÿ™ŸäŸÜ ÿπŸÑŸâ ÿßŸÑŸàÿ±ŸÇÿ©).\n",
    "\n",
    "ŸÅŸä ÿßŸÑÿµŸàÿ±ÿ©:\n",
    "\n",
    "* (d_1) = ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ÿ®ŸäŸÜ **Food** Ÿà **Agriculture**\n",
    "* (d_2) = ÿßŸÑŸÖÿ≥ÿßŸÅÿ© ÿ®ŸäŸÜ **Agriculture** Ÿà **History**\n",
    "\n",
    "ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ŸÅŸä ÿßŸÑÿ±ÿ≥ŸÖ:\n",
    "\n",
    "> (d_2 < d_1)\n",
    "> ÿ£Ÿä ÿ£ŸÜ **Euclidean distance** ŸäŸÇŸàŸÑ ÿ•ŸÜ ŸÖŸÇÿßŸÑÿ© **History** ÿ£ŸÇÿ±ÿ® ÿ•ŸÑŸâ **Agriculture** ŸÖŸÜ ŸÖŸÇÿßŸÑÿ© **Food**.\n",
    "\n",
    "ŸÑŸÉŸÜ **ŸÖŸÜÿ∑ŸÇŸäŸãÿß**ÿü ü§î\n",
    "ŸÖŸÇÿßŸÑÿ© ÿπŸÜ **Food** (ÿ∑ÿπÿßŸÖ) ÿ∫ÿßŸÑÿ®Ÿãÿß ÿ£ŸÇÿ±ÿ® ŸÑŸÖŸÇÿßŸÑÿ© ÿπŸÜ **Agriculture** (ÿ≤ÿ±ÿßÿπÿ©)\n",
    "ŸÖŸÜ ŸÖŸÇÿßŸÑÿ© ÿπŸÜ **History** (ÿ™ÿßÿ±ŸäÿÆ).\n",
    "\n",
    "ÿ•ÿ∞ŸÜ ŸáŸÜÿß **ÿßŸÑŸÖÿ≥ÿßŸÅÿ© Ÿàÿ≠ÿØŸáÿß ÿÆÿØŸëÿßÿπÿ©**ÿõ ŸÑÿ£ŸÜŸáÿß ÿ™ÿ£ÿ´ÿ±ÿ™ ÿ®ÿ∑ŸàŸÑ ÿßŸÑŸÖÿ™ÿ¨Ÿá (ÿ≠ÿ¨ŸÖ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ)ÿå\n",
    "ŸàŸÑŸäÿ≥ *ŸÅŸÉÿ±ÿ© ÿßŸÑŸÖÿ™ÿ¨Ÿá Ÿàÿßÿ™ÿ¨ÿßŸáŸá*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ ŸÖÿßÿ∞ÿß ŸäŸÅÿπŸÑ ÿßŸÑŸÄ Cosine Similarityÿü\n",
    "\n",
    "**Cosine similarity** ŸÑÿß ŸäŸáÿ™ŸÖ ÿ®ÿ∑ŸàŸÑ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿå ÿ®ŸÑ **ÿ®ÿßŸÑÿ≤ÿßŸàŸäÿ© ÿ®ŸäŸÜ ÿßŸÑŸÖÿ™ÿ¨ŸáŸäŸÜ**:\n",
    "\n",
    "* ÿßŸÑÿ≤ÿßŸàŸäÿ© **Œ±** ÿ®ŸäŸÜ ŸÖÿ™ÿ¨Ÿá **Food** Ÿà **Agriculture** ÿµÿ∫Ÿäÿ±ÿ©\n",
    "* ÿßŸÑÿ≤ÿßŸàŸäÿ© **Œ≤** ÿ®ŸäŸÜ ŸÖÿ™ÿ¨Ÿá **History** Ÿà **Agriculture** ÿ£ŸÉÿ®ÿ± (Œ≤ > Œ±)\n",
    "\n",
    "ŸÉŸÑŸÖÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ≤ÿßŸàŸäÿ© ÿ£ÿµÿ∫ÿ± ‚Üí ÿßŸÑÿßÿ™ÿ¨ÿßŸáÿßŸÜ ŸÖÿ™ÿ¥ÿßÿ®ŸáÿßŸÜ ÿ£ŸÉÿ´ÿ± ‚Üí ÿßŸÑŸÖŸàÿ∂ŸàÿπÿßŸÜ ÿ£ŸÇÿ±ÿ® ŸÖÿπŸÜŸàŸäŸãÿß.\n",
    "ŸÑÿ∞ŸÑŸÉ **Cosine similarity** ÿ≥Ÿäÿπÿ™ÿ®ÿ±:\n",
    "\n",
    "> Food ÿ£ŸÇÿ±ÿ® ÿ•ŸÑŸâ Agriculture ŸÖŸÜ History\n",
    "\n",
    "ŸàŸáÿ∞ÿß Ÿäÿ∑ÿßÿ®ŸÇ **ÿßŸÑŸÖŸÜÿ∑ŸÇ ŸàÿßŸÑŸÖÿπŸÜŸâ** üëç\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ ŸÇŸäŸÖ Cosine ŸÖÿßÿ∞ÿß ÿ™ÿπŸÜŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ÿü\n",
    "\n",
    "Cosine similarity Ÿäÿ≠ÿ≥ÿ®:\n",
    "\n",
    "[\n",
    "\\cos(\\theta) = \\frac{\\text{dot product}}{|\\vec{a}| \\cdot |\\vec{b}|}\n",
    "]\n",
    "\n",
    "ÿ≠Ÿäÿ´ Œ∏ ŸáŸä ÿßŸÑÿ≤ÿßŸàŸäÿ© ÿ®ŸäŸÜ ÿßŸÑŸÖÿ™ÿ¨ŸáŸäŸÜ.\n",
    "ÿßŸÑŸÖŸáŸÖ ŸáŸä **ŸÇŸäŸÖÿ© cos(Œ∏)**:\n",
    "\n",
    "| ÿßŸÑÿ≤ÿßŸàŸäÿ© Œ∏ ÿ®ŸäŸÜ ÿßŸÑŸÖÿ™ÿ¨ŸáŸäŸÜ | ŸÇŸäŸÖÿ© cosine | ÿßŸÑÿ™ŸÅÿ≥Ÿäÿ±                                 |\n",
    "| ---------------------- | ----------- | --------------------------------------- |\n",
    "| 0¬∞                     | 1           | ŸÖÿ™ÿ∑ÿßÿ®ŸÇÿßŸÜ ŸÅŸä ÿßŸÑÿßÿ™ÿ¨ÿßŸá ‚Üí ÿ™ÿ¥ÿßÿ®Ÿá ÿ™ÿßŸÖ ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß |\n",
    "| 30¬∞ ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß            | ~0.87       | ÿ™ÿ¥ÿßÿ®Ÿá ŸÇŸàŸä ÿ¨ÿØŸãÿß                          |\n",
    "| 60¬∞ ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß            | ~0.5        | ÿ™ÿ¥ÿßÿ®Ÿá ŸÖÿ™Ÿàÿ≥ÿ∑                             |\n",
    "| 90¬∞                    | 0           | ŸÑÿß ŸäŸàÿ¨ÿØ ÿ™ÿ¥ÿßÿ®Ÿá (ŸÖÿ™ÿπÿßŸÖÿØÿßŸÜ)                |\n",
    "| > 90¬∞ ÿ≠ÿ™Ÿâ 180¬∞         | < 0         | ÿπŸÑÿßŸÇÿ© ÿπŸÉÿ≥Ÿäÿ© / ÿßÿ™ÿ¨ÿßŸá ŸÖÿπÿßŸÉÿ≥               |\n",
    "\n",
    "ŸÅŸÑŸà ŸÇŸÑŸÜÿß:\n",
    "\n",
    "* **cosine = 0.9** ‚Üí ÿßŸÑŸÜÿµÿßŸÜ/ÿßŸÑŸÉŸÑŸÖÿ™ÿßŸÜ ÿ¥ÿ®Ÿá ŸÖÿ™ÿ∑ÿßÿ®ŸÇÿ™ŸäŸÜ ŸÖÿπŸÜŸàŸäŸãÿß\n",
    "* **cosine = 0.4** ‚Üí ŸÅŸä ÿ¥ÿ®Ÿá ÿ®ÿ≥ ŸÖŸà ŸÇŸàŸä\n",
    "* **cosine = 0** ‚Üí ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß ŸÖÿß ÿ®ŸäŸÜŸáŸÖ ÿπŸÑÿßŸÇÿ©\n",
    "* **cosine = -0.5** ‚Üí ŸäŸÖŸäŸÑÿßŸÜ ŸÑÿßÿ™ÿ¨ÿßŸá ŸÖÿπÿßŸÉÿ≥ (ÿ£ŸÇÿ±ÿ® ŸÑÿ∂ÿØ/ÿ™ŸÜÿßŸÇÿ∂ ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ®ÿπÿßÿØ)\n",
    "\n",
    "ŸÑŸáÿ∞ÿß ÿßŸÑÿ≥ÿ®ÿ® ŸÅŸä ÿßŸÑŸÄ **Word Embeddings** ÿ∫ÿßŸÑÿ®Ÿãÿß ŸÜÿ≥ÿ™ÿÆÿØŸÖ **Cosine similarity** ÿ®ÿØŸÑ Euclidean distanceÿå\n",
    "ÿÆÿµŸàÿµŸãÿß ŸÅŸä ÿßŸÑŸÜÿµŸàÿµÿå ŸÑÿ£ŸÜ ÿ∑ŸàŸÑ ÿßŸÑŸÖÿ™ÿ¨Ÿá Ÿäÿ™ÿ£ÿ´ÿ± ÿ®ÿ£ÿ¥Ÿäÿßÿ° ŸÖÿ´ŸÑ **ÿ∑ŸàŸÑ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ£Ÿà ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™**ÿå\n",
    "ÿ®ŸäŸÜŸÖÿß ÿßŸÑÿ≤ÿßŸàŸäÿ© ÿ™ÿπŸÉÿ≥ **ÿßŸÑŸÖÿπŸÜŸâ ŸàÿßŸÑÿßÿ™ÿ¨ÿßŸá** ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿ≠ÿ¨ŸÖ.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä üá¨üáß Euclidean Distance vs. Cosine Similarity\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/cosine & Euclidean.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "In the figure, we have three corpora represented as vectors:\n",
    "\n",
    "* **Food corpus** at (5, 15)\n",
    "* **Agriculture corpus** at (20, 40)\n",
    "* **History corpus** at (30, 20)\n",
    "\n",
    "Each point is a **vector** in a 2D space:\n",
    "x-axis = frequency of *‚Äúdisease‚Äù*\n",
    "y-axis = frequency of *‚Äúeggs‚Äù*.\n",
    "\n",
    "### 1Ô∏è‚É£ What does Euclidean Distance do?\n",
    "\n",
    "**Euclidean distance** is just the straight-line distance between two points.\n",
    "\n",
    "In the plot:\n",
    "\n",
    "* (d_1) = distance between **Food** and **Agriculture**\n",
    "* (d_2) = distance between **Agriculture** and **History**\n",
    "\n",
    "The picture shows:\n",
    "\n",
    "> (d_2 < d_1)\n",
    "\n",
    "So Euclidean distance claims **History** is closer to **Agriculture** than **Food** is.\n",
    "\n",
    "But semantically? ü§î\n",
    "A **Food** article is usually more related to **Agriculture** than a **History** article.\n",
    "So distance alone can be **misleading**, because it‚Äôs affected by **vector length**,\n",
    "not only the *direction* of meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ What does Cosine Similarity do?\n",
    "\n",
    "**Cosine similarity** ignores the length and looks at the **angle between vectors**:\n",
    "\n",
    "* Angle **Œ±** between **Food** and **Agriculture** is small\n",
    "* Angle **Œ≤** between **History** and **Agriculture** is larger (Œ≤ > Œ±)\n",
    "\n",
    "Smaller angle ‚Üí more similar direction ‚Üí more similar topic.\n",
    "So cosine similarity will say:\n",
    "\n",
    "> Food is more similar to Agriculture than History is\n",
    "\n",
    "which matches our **intuitive understanding** üëç\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ What do cosine values actually mean?\n",
    "\n",
    "Cosine similarity is defined as:\n",
    "\n",
    "[\n",
    "\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| \\cdot |\\vec{b}|}\n",
    "]\n",
    "\n",
    "Where Œ∏ is the angle between the two vectors.\n",
    "What matters is the **cosine value**:\n",
    "\n",
    "| Angle Œ∏ between vectors | Cosine value | Interpretation                            |\n",
    "| ----------------------- | ------------ | ----------------------------------------- |\n",
    "| 0¬∞                      | 1            | Same direction ‚Üí almost identical meaning |\n",
    "| ~30¬∞                    | ~0.87        | Very strong similarity                    |\n",
    "| ~60¬∞                    | ~0.5         | Moderate similarity                       |\n",
    "| 90¬∞                     | 0            | No similarity (orthogonal)                |\n",
    "| > 90¬∞ to 180¬∞           | < 0          | Opposite / contradictory directions       |\n",
    "\n",
    "So:\n",
    "\n",
    "* **cosine = 0.9** ‚Üí very similar\n",
    "* **cosine = 0.4** ‚Üí somewhat related\n",
    "* **cosine = 0** ‚Üí unrelated\n",
    "* **cosine = -0.5** ‚Üí somewhat opposite in meaning\n",
    "\n",
    "That‚Äôs why in **word embeddings** and text similarity,\n",
    "we usually prefer **cosine similarity** over Euclidean distance:\n",
    "length can be influenced by sentence length or word frequency,\n",
    "but the **angle** better reflects semantic direction and meaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd584a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: spacy 3.7.2\n",
      "Uninstalling spacy-3.7.2:\n",
      "  Successfully uninstalled spacy-3.7.2\n",
      "Found existing installation: thinc 8.2.2\n",
      "Uninstalling thinc-8.2.2:\n",
      "  Successfully uninstalled thinc-8.2.2\n",
      "Found existing installation: blis 0.7.11\n",
      "Uninstalling blis-0.7.11:\n",
      "  Successfully uninstalled blis-0.7.11\n",
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting blis==0.7.11\n",
      "  Using cached blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting thinc==8.2.2\n",
      "  Using cached thinc-8.2.2-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting spacy==3.7.2\n",
      "  Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=1.0.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (2.0.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (0.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (80.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (2.12.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc==8.2.2) (25.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (2.32.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (3.1.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy==3.7.2) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.2) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.2) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.2) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.2) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from jinja2->spacy==3.7.2) (3.0.3)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "Using cached thinc-8.2.2-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Installing collected packages: numpy, blis, thinc, spacy\n",
      "\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------- ----------------------------- 1/4 [blis]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   -------------------- ------------------- 2/4 [thinc]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ------------------------------ --------- 3/4 [spacy]\n",
      "   ---------------------------------------- 4/4 [spacy]\n",
      "\n",
      "Successfully installed blis-0.7.11 numpy-1.26.4 spacy-3.7.2 thinc-8.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of scipy: [Errno 2] No such file or directory: 'c:\\\\users\\\\shosh\\\\.conda\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\scipy-1.15.3.dist-info\\\\METADATA'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.40 requires matplotlib>=3.3.0, which is not installed.\n",
      "ultralytics 8.3.40 requires pandas>=1.1.4, which is not installed.\n",
      "ultralytics 8.3.40 requires pillow>=7.1.2, which is not installed.\n",
      "ultralytics 8.3.40 requires pyyaml>=5.3.1, which is not installed.\n",
      "ultralytics 8.3.40 requires scipy>=1.4.1, which is not installed.\n",
      "ultralytics 8.3.40 requires seaborn>=0.11.0, which is not installed.\n",
      "ultralytics 8.3.40 requires torch!=2.4.0,>=1.8.0; sys_platform == \"win32\", which is not installed.\n",
      "ultralytics 8.3.40 requires torch>=1.8.0, which is not installed.\n",
      "ultralytics 8.3.40 requires torchvision>=0.9.0, which is not installed.\n",
      "ultralytics-thop 2.0.12 requires torch, which is not installed.\n",
      "scikit-learn 1.7.2 requires scipy>=1.8.0, which is not installed.\n",
      "scikit-learn 1.7.2 requires threadpoolctl>=3.1.0, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from en-core-web-lg==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.10.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shosh\\.conda\\envs\\nlp\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.3)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of scipy: [Errno 2] No such file or directory: 'c:\\\\users\\\\shosh\\\\.conda\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\scipy-1.15.3.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y spacy thinc blis numpy\n",
    "# !pip install numpy==1.26.4 blis==0.7.11 thinc==8.2.2 spacy==3.7.2\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the large English model\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951815fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2657e+00,  3.0703e+00, -1.7375e+00, -2.0443e+00, -1.1928e+00,\n",
       "        4.7458e+00,  4.1327e-03, -9.9579e-01,  2.7430e+00,  1.5209e+00,\n",
       "       -1.5743e+00, -1.0238e+00, -4.1670e-01,  3.0210e+00,  3.3419e+00,\n",
       "       -1.4513e+00,  1.3891e+00, -2.6773e+00,  1.5304e+00, -1.2868e+00,\n",
       "        2.2221e+00, -1.8497e-01, -3.6390e-01, -2.1370e+00,  3.3337e+00,\n",
       "        4.2660e-01,  2.4932e+00, -1.4288e+00,  2.7081e+00,  3.7910e-01,\n",
       "        3.6824e+00, -3.4750e+00,  2.6784e+00, -3.1675e+00,  7.1397e-01,\n",
       "        3.7714e-01, -2.3996e+00,  3.0475e-01,  9.8345e-01, -1.6483e+00,\n",
       "       -2.1484e+00,  2.0593e+00, -1.2598e+00,  5.4024e-01, -1.1401e+00,\n",
       "       -2.3316e+00, -1.2535e+00, -4.0506e+00, -2.4282e-02,  3.3028e+00,\n",
       "       -7.1632e-01,  4.8101e-02,  1.2775e+00, -2.2964e-02,  1.7544e+00,\n",
       "       -2.6502e+00,  9.9385e-01,  3.8857e-02, -1.0332e+00, -7.2275e-01,\n",
       "       -1.9981e+00, -2.3808e+00, -9.7850e-02, -7.5982e-01, -6.7057e-01,\n",
       "        5.1752e-01,  2.4360e-01, -3.1495e+00,  2.9339e+00, -9.4296e-01,\n",
       "        6.6251e-01,  2.4499e+00,  3.1315e-02,  1.4734e+00, -1.8636e+00,\n",
       "       -7.2413e-01, -4.9318e-01,  2.7718e+00,  1.1353e+00, -1.6181e+00,\n",
       "        1.0753e-01, -1.7407e+00, -7.0819e-01, -2.1883e+00,  1.4590e+00,\n",
       "        2.6207e+00,  1.5006e+00, -6.5071e-01, -1.2046e+00,  4.3139e-01,\n",
       "        1.6799e+00, -1.1989e+00,  3.4300e-03, -5.1471e+00, -4.3013e-01,\n",
       "       -2.8456e+00,  8.5887e-01, -6.9112e-01, -3.5415e+00, -1.7239e+00,\n",
       "        1.5720e+00,  1.4416e+00,  1.8991e+00, -1.3548e+00, -4.3791e+00,\n",
       "       -2.8694e+00, -1.8214e+00, -1.3325e-01,  1.0635e+00,  5.0961e-01,\n",
       "       -2.4572e+00, -1.0423e+00, -2.1225e+00, -5.9131e-01,  1.5448e+00,\n",
       "        3.0062e-01, -7.2216e-01,  3.4901e-01,  7.8028e-01,  1.5921e+00,\n",
       "       -3.1996e+00, -1.5762e-01, -1.9552e+00,  2.4258e+00,  1.4999e+00,\n",
       "        2.6864e+00, -1.6646e-01, -2.9196e+00,  2.8713e+00,  3.1668e+00,\n",
       "        2.1175e+00, -3.0662e+00, -9.2504e-01, -1.4213e+00, -6.9682e-01,\n",
       "        5.9271e+00, -1.4935e+00,  8.3195e-01,  2.4227e-01, -9.8318e-01,\n",
       "       -9.8516e-01, -5.8281e-01,  7.9095e-01,  3.9312e+00, -3.4781e+00,\n",
       "        7.6283e-01, -4.4371e+00,  1.2370e+00, -9.8632e-01,  1.7430e+00,\n",
       "        3.0412e-01, -1.0613e-01, -1.0171e+00, -9.1912e-01, -9.3939e-01,\n",
       "        4.1394e+00,  4.3527e+00,  3.1244e+00, -5.8540e-02, -1.0129e+00,\n",
       "        1.1795e-01, -3.1556e+00, -2.2050e+00, -9.6567e-01, -2.4015e+00,\n",
       "       -6.5535e-01,  1.9342e+00,  4.1939e-01, -2.4190e+00,  9.0822e-03,\n",
       "        2.7983e+00,  1.2368e+00,  3.4277e+00, -1.8361e+00,  1.3205e+00,\n",
       "       -2.4466e+00, -6.0411e-01,  1.3802e+00,  5.8310e-01, -2.0282e+00,\n",
       "       -8.7731e-01,  1.0356e-02,  1.8055e+00, -3.8192e-01,  2.5048e+00,\n",
       "        2.4359e+00, -6.1424e-02,  2.4013e-01, -2.9712e-01,  1.6351e+00,\n",
       "        4.5158e+00,  3.5583e+00, -1.8842e+00,  2.5951e+00, -1.4705e+00,\n",
       "        3.9217e-01, -1.0601e+00,  2.3595e+00, -2.8224e+00,  4.4224e+00,\n",
       "        1.7848e+00, -3.0513e-01, -2.1014e+00,  9.4107e-01, -6.1551e+00,\n",
       "       -2.6918e-01,  2.2497e+00,  1.0999e+00, -7.3644e-01,  1.6342e+00,\n",
       "       -1.3370e+00,  3.6173e+00, -1.9966e+00, -2.4698e+00, -2.5542e+00,\n",
       "        3.2355e+00, -6.0477e-01,  4.2956e+00, -5.8369e-01, -3.9479e-01,\n",
       "        6.1773e-01,  1.2418e+00,  1.6261e+00,  3.6433e+00,  1.3408e-01,\n",
       "       -1.5647e-01, -3.7894e+00, -1.6322e+00, -4.6426e-01, -6.9779e-01,\n",
       "       -3.0937e+00, -2.0869e-01, -2.4956e+00, -1.1007e+00, -1.4291e+00,\n",
       "       -3.3029e+00, -2.5400e+00, -1.2346e+00, -2.7839e+00,  1.0052e+00,\n",
       "        3.5795e+00, -2.3398e-01,  6.4070e-01, -2.3847e+00, -1.1826e+00,\n",
       "       -2.6635e+00, -3.4609e-01, -9.6888e-01,  3.8689e+00,  4.0255e+00,\n",
       "       -1.9424e+00,  1.3584e+00, -2.7454e+00, -2.1404e+00,  1.7877e+00,\n",
       "       -4.9686e+00,  1.3299e+00, -8.0575e-01, -3.1452e+00,  1.4733e+00,\n",
       "        2.8045e+00, -4.5535e+00,  1.3212e+00,  1.3945e+00,  6.9748e-01,\n",
       "        2.5289e+00,  7.1475e-01,  7.1169e-01, -7.0409e-01,  2.0307e+00,\n",
       "        1.4976e+00,  7.2558e-02, -1.6896e+00, -2.0963e+00, -1.1409e+00,\n",
       "       -1.2158e+00, -6.0341e-02, -1.5625e+00, -1.7409e+00,  3.4096e+00,\n",
       "       -6.8042e-02, -4.2241e+00, -1.2762e+00, -1.9332e+00,  8.8637e-01,\n",
       "       -2.9335e+00,  1.5603e+00, -2.9996e+00,  2.2201e+00,  3.0092e+00,\n",
       "        1.7960e+00, -7.6659e-01,  8.6161e-01,  5.0651e-01,  1.7610e+00,\n",
       "       -3.0670e+00, -3.9424e+00,  2.2632e+00, -4.7729e+00,  3.2085e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the 300-dimensional vector for a given word\n",
    "nlp(u'brave').vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8aa2e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the vector length (should be 300)\n",
    "len(nlp(u'lion').vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c312a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7769655 ,  0.39714497, -1.695121  , -0.1089559 ,  3.861494  ,\n",
       "       -0.10778303, -0.02750097,  3.191314  ,  1.0857747 , -0.2615487 ,\n",
       "        4.0720797 ,  1.5932049 , -2.7569218 ,  0.70982707,  2.0976841 ,\n",
       "        0.08150103,  0.8847861 , -0.505237  ,  0.767067  , -2.88911   ,\n",
       "       -0.28514975, -0.331664  ,  0.306348  , -2.25347   ,  0.96798134,\n",
       "       -0.030282  , -3.765162  , -2.168157  ,  1.3985709 ,  2.175709  ,\n",
       "       -0.81103534, -0.55156004, -1.033463  , -2.3130198 , -2.892054  ,\n",
       "       -2.843568  , -0.33247897,  1.620013  ,  3.03307   , -0.42730814,\n",
       "        1.298548  ,  0.18969259,  1.234282  , -0.14263602, -1.427765  ,\n",
       "       -0.05807757,  0.33836406, -1.6987331 , -2.13661   ,  0.10412004,\n",
       "        0.62479395,  3.9712129 , -0.31110606, -1.9676571 , -0.11860895,\n",
       "        0.55582994, -0.660888  ,  1.947435  ,  1.6391805 ,  0.6569032 ,\n",
       "        0.054408  , -2.08993   ,  1.0370519 ,  0.5363236 ,  0.00807395,\n",
       "       -0.91060096, -3.3870788 , -1.4823462 ,  1.4170542 ,  0.32670596,\n",
       "        0.602952  , -1.047483  , -0.3633799 , -0.09132097,  0.722465  ,\n",
       "       -0.74786097, -1.8563267 ,  0.827946  , -0.99609107, -0.3298212 ,\n",
       "       -3.472247  , -0.94788504, -0.46503416,  0.6626488 ,  2.5601602 ,\n",
       "       -1.2395241 ,  0.61391175, -0.61511296, -1.459838  ,  0.14399411,\n",
       "       -1.1767578 , -0.10416207,  1.6930513 , -1.6688293 ,  0.26341552,\n",
       "       -0.84874   ,  1.5149789 ,  0.03238799,  1.768853  ,  0.42900094,\n",
       "        1.43253   , -0.53126115,  2.916109  ,  1.7515122 ,  0.62033385,\n",
       "        1.7113819 , -1.5602219 ,  1.0098228 , -1.277114  , -0.816329  ,\n",
       "       -0.28804332,  0.0696778 , -1.0609343 ,  0.960142  ,  1.765322  ,\n",
       "       -0.18756203,  0.75897396, -0.64144975,  1.7536061 ,  0.10283799,\n",
       "       -0.31856102, -1.091177  , -1.6496038 ,  1.657584  , -1.5179831 ,\n",
       "       -1.886636  ,  1.6479073 , -1.196546  ,  3.3722177 , -0.22696403,\n",
       "       -3.203148  , -1.708502  ,  2.9725442 , -0.320748  ,  0.466757  ,\n",
       "        1.6034908 , -1.6832211 , -2.80418   ,  1.4318919 , -1.0059    ,\n",
       "       -2.9437232 , -1.430186  , -1.3512889 , -0.09892895, -0.38347203,\n",
       "        1.071111  , -1.02584   ,  0.30393195,  1.5452302 ,  0.28571904,\n",
       "        0.10674398,  2.7844248 ,  0.43838865, -0.21123195, -0.69085443,\n",
       "        0.04283998,  2.3449368 , -0.8889019 , -0.2773927 ,  0.3125515 ,\n",
       "        0.264121  , -2.649219  ,  0.33148596,  0.39187098, -2.2444918 ,\n",
       "       -1.1317899 , -1.935579  ,  0.52987385,  0.303826  ,  1.681892  ,\n",
       "        1.0111669 ,  1.5380409 ,  1.7308229 , -0.080899  , -0.892347  ,\n",
       "       -0.23734598, -0.21603405, -0.69041294, -1.6095406 , -0.8730756 ,\n",
       "       -1.5468609 , -0.17627892, -0.1907803 ,  0.17952934, -0.803991  ,\n",
       "       -0.03223496, -1.9790027 , -1.6414549 ,  0.20527199, -1.104895  ,\n",
       "        0.24875899,  1.186315  , -0.8186741 , -2.410284  ,  0.5833171 ,\n",
       "       -0.784715  , -2.627035  , -0.15004005, -1.4183891 ,  0.442288  ,\n",
       "       -0.62965304, -1.113676  , -0.46552286, -1.651365  ,  1.8519121 ,\n",
       "        1.1279902 , -3.2509658 ,  0.04463506,  0.11902802,  0.68590844,\n",
       "        0.77328   ,  1.3373908 , -1.7096472 , -0.5530244 , -0.02548897,\n",
       "        1.19019   ,  0.10446696, -1.351313  , -1.7751659 , -0.44060117,\n",
       "       -1.4011772 ,  0.9672308 , -0.97586805,  1.2529299 , -1.9867659 ,\n",
       "       -0.7944795 ,  0.6362045 ,  2.010871  ,  3.6185539 ,  0.62658596,\n",
       "        0.9231445 , -3.4679253 ,  0.42413408,  1.219738  , -0.0507701 ,\n",
       "        2.0426223 , -1.5670619 ,  1.5156981 , -0.09616538,  1.6220791 ,\n",
       "        0.775658  , -1.8741112 ,  2.0503078 , -0.7602806 , -1.442644  ,\n",
       "        1.69713   , -1.6083733 , -0.278282  , -1.431804  ,  0.60193396,\n",
       "       -0.51194596, -2.1616828 , -4.4544115 , -0.3637256 , -0.98130643,\n",
       "       -2.1879544 ,  1.471617  ,  3.2054775 , -0.562816  , -1.099867  ,\n",
       "        2.353531  ,  1.6450421 ,  2.8005474 ,  1.1693535 ,  0.086842  ,\n",
       "       -2.5300715 , -1.5670309 ,  0.31365603, -2.450636  ,  1.4588621 ,\n",
       "        1.24279   ,  0.08502197,  1.06439   , -0.86917496,  0.32117197,\n",
       "       -0.909351  ,  0.52400905, -1.35512   , -0.72316396,  1.9528949 ,\n",
       "        0.49172202,  0.4663611 , -0.43013096, -0.09305   ,  1.265938  ,\n",
       "       -1.251461  , -0.85874796, -0.2679918 , -0.173774  ,  0.56781405,\n",
       "       -0.5377843 ,  0.871497  ,  2.4677212 ,  0.6728541 , -0.97043693,\n",
       "        2.501867  , -0.572828  ,  0.504657  , -3.4507267 ,  0.45634192],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence vector (average of all word vectors)\n",
    "doc = nlp(u'The quick brown fox jumped over the lazy dogs.')\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75e3389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.3854507803916931\n",
      "lion pet 0.20031583309173584\n",
      "cat lion 0.3854507803916931\n",
      "cat cat 1.0\n",
      "cat pet 0.732966423034668\n",
      "pet lion 0.20031583309173584\n",
      "pet cat 0.732966423034668\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare similarity between multiple words\n",
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822335b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8273442557707831\n",
      "0.24762304993106887\n",
      "0.7136095195259959\n",
      "0.4145525929565585\n"
     ]
    }
   ],
   "source": [
    "# Word-to-word similarity examples\n",
    "print(nlp(u'man').similarity(nlp(u'woman')))\n",
    "print(nlp(u'man').similarity(nlp(u'tree')))\n",
    "print(nlp(u'lion').similarity(nlp(u'tiger')))\n",
    "print(nlp(u'lion').similarity(nlp(u'dandelion')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfa69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9503183859344599\n",
      "0.9793199767254251\n"
     ]
    }
   ],
   "source": [
    "# Sentence similarity comparison\n",
    "print(nlp(u'I love school').similarity(nlp(u'I hate school')))\n",
    "print(nlp(u'this film is awesome. I love it').similarity(nlp(u'this film is boring. I hate it')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c234d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "nargle False 0.0 True\n",
      "hesham False 0.0 True\n",
      "lksdvsk False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "# Check vector existence, vector norm, and OOV (out of vocabulary)\n",
    "tokens = nlp(u'dog cat nargle hesham lksdvsk')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ea427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-win_amd64.whl.metadata (5.7 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-win_amd64.whl (15.6 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.25.2\n",
      "Collecting scipy==1.15.3\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Installing collected packages: scipy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Users\\\\shosh\\\\.conda\\\\envs\\\\NLP\\\\Lib\\\\site-packages\\\\scipy\\\\spatial\\\\_distance_wrap.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install --ignore-installed --no-deps numpy==1.25.2\n",
    "# !pip install --ignore-installed --no-deps scipy==1.15.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c40ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word prince , has similarity 0.6562829656127154\n",
      "Word princess , has similarity 0.6480762222378031\n",
      "Word queen , has similarity 0.6178013742941038\n",
      "Word crown , has similarity 0.5205165367683633\n",
      "Word castle , has similarity 0.3856709507986278\n",
      "Word land , has similarity 0.37457909091965713\n",
      "Word elizabeth , has similarity 0.3294205394308072\n",
      "Word great , has similarity 0.2888008262644557\n",
      "Word white , has similarity 0.2524483583823236\n",
      "Word study , has similarity 0.23255463266352316\n"
     ]
    }
   ],
   "source": [
    "# Example: king - man + woman ‚âà queen\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Define cosine similarity function\n",
    "cosine_similarity = lambda x, y: 1 - distance.cosine(x, y)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Compute new vector\n",
    "new_vector = king - man + woman\n",
    "\n",
    "computed_similarities = []\n",
    "words = ['cat','apple','queen','castle','sea','shell','orange','phone',\n",
    "         'tiffany','angry','book','white','land','study','crown','prince',\n",
    "         'dog','great','princess','elizabeth','wow','eat','dead','horrible']\n",
    "\n",
    "# Compare the new vector with all words\n",
    "for word in words:\n",
    "    similarity = cosine_similarity(new_vector, nlp.vocab[word].vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "# Sort and show the most similar words\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "for a, b in computed_similarities[:10]:\n",
    "    print(f'Word {a} , has similarity {b}')\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Define cosine similarity using numpy\n",
    "# def cosine_similarity(x, y):\n",
    "#     return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "# king = nlp.vocab['king'].vector\n",
    "# man = nlp.vocab['man'].vector\n",
    "# woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# # Compute new vector\n",
    "# new_vector = king - man + woman\n",
    "\n",
    "# words = ['cat','apple','queen','castle','sea','shell','orange','phone',\n",
    "#          'tiffany','angry','book','white','land','study','crown','prince',\n",
    "#          'dog','great','princess','elizabeth','wow','eat','dead','horrible']\n",
    "\n",
    "# computed_similarities = []\n",
    "\n",
    "# for word in words:\n",
    "#     similarity = cosine_similarity(new_vector, nlp.vocab[word].vector)\n",
    "#     computed_similarities.append((word, similarity))\n",
    "\n",
    "# # Sort and show the most similar words\n",
    "# computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "# for a, b in computed_similarities[:10]:\n",
    "#     print(f'Word {a} , has similarity {b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24842235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0832475 , -0.054465  ,  0.307     , -0.417525  , -0.11432   ,\n",
       "        0.25995   , -0.1459275 ,  0.1574625 ,  0.0258875 ,  0.1498425 ,\n",
       "        0.099775  ,  0.06666   ,  0.303325  , -0.04788   ,  0.229525  ,\n",
       "        0.00786325, -0.07418   ,  0.15416   , -0.004022  ,  0.29705   ,\n",
       "       -0.400225  ,  0.007079  ,  0.15236   ,  0.30375   , -0.038155  ,\n",
       "       -0.1529925 ,  0.0514875 ,  0.3278    , -0.00083627, -0.1376675 ,\n",
       "       -0.244045  ,  0.222105  ,  0.180235  , -0.0543225 ,  0.301275  ,\n",
       "       -0.14083   , -0.00201715,  0.55655   , -0.218385  , -0.1699    ,\n",
       "        0.223975  ,  0.1761425 ,  0.2013475 ,  0.1498725 ,  0.155475  ,\n",
       "       -0.0435075 , -0.364775  ,  0.161775  , -0.0255625 ,  0.188165  ,\n",
       "       -0.2082325 , -0.1933925 , -0.254425  , -0.143445  , -0.274175  ,\n",
       "       -0.0765625 , -0.083485  , -0.1528975 ,  0.295275  ,  0.0310625 ,\n",
       "        0.0740225 , -0.0315975 , -0.297325  ,  0.12227   ,  0.1183175 ,\n",
       "       -0.088645  ,  0.0793925 ,  0.1314175 , -0.25975   ,  0.1403725 ,\n",
       "       -0.0342025 , -0.06369   , -0.013837  , -0.1198875 ,  0.2853    ,\n",
       "       -0.0409725 ,  0.0012649 , -0.01228225, -0.057965  ,  0.0564425 ,\n",
       "       -0.12038   ,  0.0515175 , -0.1413875 , -0.0677025 , -0.00881625,\n",
       "        0.529325  ,  0.0755075 , -0.011041  , -0.188845  ,  0.18183   ,\n",
       "       -0.1038575 ,  0.03532   ,  0.21714   , -0.18106   , -0.2814    ,\n",
       "       -0.12644   , -0.0873575 , -0.0941725 , -0.1473025 ,  0.266925  ,\n",
       "       -0.0447725 , -0.351175  , -0.056085  , -0.208575  ,  0.1202825 ,\n",
       "       -0.044425  ,  0.17601   ,  0.107245  , -0.231605  , -0.154165  ,\n",
       "        0.36345   ,  0.3485    ,  0.2885    , -0.1397575 , -0.122305  ,\n",
       "        0.0065425 , -0.1227825 , -0.046405  ,  0.14912   , -0.0688525 ,\n",
       "        0.463225  , -0.3833    ,  0.00183912,  0.449125  ,  0.385925  ,\n",
       "        0.135055  ,  0.1220675 , -0.12052   ,  0.1419525 , -0.095255  ,\n",
       "       -0.1550675 ,  0.23201   , -0.0621525 , -0.265     ,  0.038225  ,\n",
       "       -0.137035  , -0.21632   ,  0.297     , -0.0884075 , -0.467075  ,\n",
       "       -0.041825  , -0.0413325 , -0.306475  , -0.059065  , -0.2295875 ,\n",
       "       -0.3152    ,  0.213555  , -0.04444   ,  0.0542775 , -0.0583925 ,\n",
       "        0.1964375 , -0.087105  ,  0.23244   , -0.3043    ,  0.4195    ,\n",
       "       -0.0062325 , -0.01475075, -0.18561   , -0.1327325 , -0.1604125 ,\n",
       "        0.20402   ,  0.18257   , -0.102415  ,  0.2331575 , -0.1005125 ,\n",
       "       -0.121265  , -0.1513975 ,  0.093965  ,  0.01270775,  0.133215  ,\n",
       "        0.2087875 , -0.052455  , -0.255525  ,  0.332225  ,  0.2826    ,\n",
       "       -0.210195  ,  0.14053   , -0.1450975 , -0.0943625 ,  0.1411975 ,\n",
       "       -0.15611   ,  0.0458175 , -0.37045   , -0.1543225 , -0.0014575 ,\n",
       "        0.104165  ,  0.00144182, -0.35715   ,  0.049475  , -0.4489    ,\n",
       "        0.0409525 , -0.02788   ,  0.044515  ,  0.1527625 ,  0.1117725 ,\n",
       "        0.27385   , -0.02396675, -0.2499    , -0.1762775 ,  0.0385775 ,\n",
       "        0.0665975 , -0.079345  , -0.17671   , -0.192575  ,  0.04455   ,\n",
       "       -0.06948   ,  0.496925  ,  0.1044875 , -0.09014   , -0.200745  ,\n",
       "       -0.093755  , -0.23088   ,  0.35205   ,  0.10274   ,  0.06779   ,\n",
       "        0.0269525 ,  0.1526875 , -0.1009175 ,  0.04309   ,  0.1569875 ,\n",
       "        0.363525  , -0.00211473,  0.139235  , -0.18139   , -0.0454825 ,\n",
       "        0.02465875, -0.017711  ,  0.01341075,  0.08329   , -0.4339    ,\n",
       "        0.05378   , -0.1684175 , -0.000661  , -0.037525  , -0.0823175 ,\n",
       "       -0.02349675,  0.1471475 , -0.031875  ,  0.01351075, -0.08159   ,\n",
       "        0.083855  ,  0.363275  ,  0.1395225 , -0.2977    , -0.1955975 ,\n",
       "       -0.1601425 ,  0.0871125 , -0.2747    ,  0.0809275 , -0.23361   ,\n",
       "        0.20913   ,  0.065365  ,  0.245925  ,  0.075485  ,  0.03828   ,\n",
       "        0.39865   ,  0.1066325 , -0.00772825, -0.29545   , -0.31165   ,\n",
       "        0.2791    , -0.1973825 ,  0.08646   , -0.06461   , -0.0786925 ,\n",
       "       -0.230155  , -0.00873725,  0.1357625 , -0.00967125, -0.24884   ,\n",
       "       -0.0479    ,  0.372025  , -0.1908875 ,  0.2688    ,  0.0453175 ,\n",
       "       -0.01555825, -0.038925  , -0.19097   ,  0.150095  , -0.29385   ,\n",
       "        0.2176325 , -0.209565  ,  0.24076   ,  0.4475    ,  0.01198825,\n",
       "       -0.1187825 ,  0.1233075 ,  0.1965025 ,  0.0413025 , -0.273425  ,\n",
       "        0.322225  ,  0.123795  , -0.13658   , -0.0898025 ,  0.078095  ,\n",
       "       -0.1320375 ,  0.1051875 , -0.354325  , -0.047065  , -0.058455  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arabic embeddings are not supported in this English model\n",
    "nlp(u'ÿ£ÿ≥ÿØ').vector\n",
    "\n",
    "doc = nlp(u'ÿ∞Ÿáÿ® ŸÖÿ≠ŸÖÿØ ÿßŸÑŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©')\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e724652",
   "metadata": {},
   "source": [
    "\n",
    "> üá∏üá¶\n",
    "> üìçÿßŸÑŸÇŸäŸÖ ÿ≥ÿ™ŸÉŸàŸÜ ŸÉŸÑŸáÿß **ÿµŸÅÿ±Ÿãÿß**ÿå ŸÑÿ£ŸÜ ŸÖŸÉÿ™ÿ®ÿ© **spaCy** ŸÑÿß ÿ™ŸàŸÅÿ± ÿ≠ÿßŸÑŸäŸãÿß ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ *Word Embeddings* ŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©.\n",
    "> ŸÑÿßÿ≠ŸÇŸãÿß ÿ≥ŸÜÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿ£ÿØŸàÿßÿ™ ŸÇŸàŸäÿ© ŸÖÿ´ŸÑ **GloVe** Ÿà **Word2Vec** Ÿà **FastText** Ÿà **AraVec**ÿå\n",
    "> ŸàÿßŸÑÿ™Ÿä ÿ™ŸÇÿØŸÖ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿ±ŸÇŸÖŸäÿ© ÿØŸÇŸäŸÇÿ© ÿ¨ÿØŸãÿß ŸÑŸÑŸÜÿµŸàÿµ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿ™ÿ≥ÿßÿπÿØŸÜÿß ÿπŸÑŸâ ŸÅŸáŸÖ ÿßŸÑŸÖÿπÿßŸÜŸä ÿ®ÿ¥ŸÉŸÑ ÿ£ÿπŸÖŸÇ.\n",
    "\n",
    "> üá¨üáß\n",
    "> üìçAll values will appear as **zeros**, because **spaCy** currently does not provide *Word Embeddings* for Arabic.\n",
    "> Later, we‚Äôll explore powerful tools like **GloVe**, **Word2Vec**, **FastText**, and **AraVec**,\n",
    "> which offer highly accurate numeric representations for Arabic text, enabling deeper semantic understanding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e3d39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ‚ú® ÿßŸÑÿÆÿßÿ™ŸÖÿ©\n",
    "\n",
    "### üá∏üá¶\n",
    "\n",
    "ÿßŸÑÿ¢ŸÜ ÿ£ÿµÿ®ÿ≠ÿ™ ÿ™ÿπÿ±ŸÅ ÿ£ŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÑŸäÿ≥ÿ™ ŸÖÿ¨ÿ±ÿØ ÿ≠ÿ±ŸàŸÅÿå ÿ®ŸÑ **ÿ£ÿ±ŸÇÿßŸÖ ÿ™ÿπÿ®Ÿëÿ± ÿπŸÜ ŸÖÿπÿßŸÜŸç ÿπŸÖŸäŸÇÿ©** ŸäŸÅŸáŸÖŸáÿß ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ®!\n",
    "ŸÅŸä ÿßŸÑŸÇÿ≥ŸÖ ÿßŸÑÿ™ÿßŸÑŸäÿå ÿ≥ŸÜÿ®ÿØÿ£ ÿ±ÿ≠ŸÑÿ© **ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© ŸÑŸÑŸÜÿµŸàÿµ (Basic Text Processing)**ÿå\n",
    "ŸÑŸÜÿ™ÿπŸÑŸÖ ŸÉŸäŸÅ ŸÜŸèÿ≠ÿ∂Ÿëÿ± ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÇÿ®ŸÑ ÿ®ŸÜÿßÿ° ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉŸäÿ©.\n",
    "ÿßÿ≥ÿ™ÿπÿØ üî• ‚Äî ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© ÿßŸÑŸÇÿßÿØŸÖÿ© ŸáŸä ÿ£ŸàŸÑ ÿÆÿ∑Ÿàÿ© ŸÜÿ≠Ÿà ŸÅŸáŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿ®ÿπŸÖŸÇ!\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß\n",
    "\n",
    "Now you know that words are **not just letters**, but **vectors of meaning** that computers can understand!\n",
    "In the next section, we‚Äôll move on to **Basic Text Processing**,\n",
    "where we‚Äôll prepare our data for building smart NLP models.\n",
    "Get ready üöÄ ‚Äî the exciting part starts now!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22355f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò ÿßŸÑŸÇÿ≥ŸÖ ÿßŸÑÿ´ÿßŸÑÿ´: üá∏üá¶ ŸÉŸäÿ≥ ÿßŸÑŸÉŸÑŸÖÿßÿ™ | üá¨üáß Bag of Words (BoW)\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä\n",
    "\n",
    "ÿ™ÿÆŸäŸÑ ÿ£ŸÜ ÿπŸÜÿØŸÉ ŸÇÿµÿ© ŸÇÿµŸäÿ±ÿ© ÿ£Ÿà ÿ™ÿ∫ÿ±ŸäÿØÿ©ÿå Ÿàÿ™ÿ®ÿ∫Ÿâ ÿ™ÿπÿ∑ŸäŸáÿß ŸÑŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ± ŸäŸÅŸáŸÖŸáÿß... ŸÑŸÉŸÜ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ± ŸÖÿß Ÿäÿπÿ±ŸÅ ÿ•ŸÑÿß ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ.\n",
    "ŸÅŸÉŸäŸÅ ŸÜÿÆŸÑŸäŸá ŸäŸÅŸáŸÖ ÿßŸÑŸÉŸÑŸÖÿßÿ™ÿü ü§î\n",
    "ŸáŸÜÿß Ÿäÿ£ÿ™Ÿä ÿØŸàÿ± **Bag of Words (ŸÉŸäÿ≥ ÿßŸÑŸÉŸÑŸÖÿßÿ™)** ‚Äî ŸÅŸÉÿ±ÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© ÿ¨ÿØŸãÿß ŸÑŸÉŸÜŸáÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥ ŸÑŸÉŸÑ ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÇÿØŸäŸÖÿ©.\n",
    "\n",
    "ÿßŸÑŸÅŸÉÿ±ÿ© ÿ£ŸÜŸÉ **ÿ™ÿ≠ŸàŸëŸÑ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ£ÿ±ŸÇÿßŸÖ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ Ÿàÿ™ŸÉÿ±ÿßÿ±Ÿáÿß** ŸÅŸÇÿ∑ÿå\n",
    "ÿ®ÿØŸàŸÜ ÿßŸÑŸÜÿ∏ÿ± ŸÑŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®.\n",
    "\n",
    "üëù ÿ™ÿÆŸäŸëŸÑ ÿπŸÜÿØŸÉ ‚ÄúŸÉŸäÿ≥‚Äù ŸÅŸäŸá ŸÉŸÑ ŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿµŸàÿµÿå ŸàŸÉŸÑ ÿ¨ŸÖŸÑÿ© ŸÖÿ¨ÿ±ÿØ **ŸÖÿ¨ŸÖŸàÿπÿ© ŸÉŸÑŸÖÿßÿ™ ŸÖŸàÿ¨ŸàÿØÿ© ÿ£Ÿà ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ©** ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÉŸäÿ≥.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß The Basic Idea\n",
    "\n",
    "Imagine you give your computer a sentence, but it can‚Äôt read ‚Äî it only understands numbers.\n",
    "How can we represent text as numbers?\n",
    "That‚Äôs what **Bag of Words (BoW)** does!\n",
    "\n",
    "It simply **counts how many times each word appears** in a text ‚Äî ignoring grammar, order, or meaning.\n",
    "Think of it like a big **bag of words**, where every document is represented by word presence and frequency.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ™ÿßŸÜ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ™ÿßŸÜ | üá¨üáß The Two Main Steps\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ©                | üá¨üáß Step                          | üá∏üá¶ ÿßŸÑÿ¥ÿ±ÿ≠                                                                                           | üá¨üáß Explanation                                                                                           |\n",
    "| -------------------------- | ---------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| 1Ô∏è‚É£ ÿ•ŸÜÿ¥ÿßÿ° ŸÇÿßÿ¶ŸÖÿ© ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™   | Create the vocabulary              | ŸÜÿ¨ŸÖÿπ ŸÉŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ∏Ÿáÿ±ÿ™ ŸÅŸä ÿßŸÑŸÜÿµŸàÿµ ÿ®ÿπÿØ ÿ™ŸÜÿ∏ŸäŸÅŸáÿß (ÿ™ÿ≠ŸàŸäŸÑ ŸÑÿ≠ÿ±ŸàŸÅ ÿµÿ∫Ÿäÿ±ÿ© ‚Äì ÿ≠ÿ∞ŸÅ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ‚Äì ÿ•ÿ≤ÿßŸÑÿ© stopwords). | Collect all unique words from the dataset after cleaning (lowercasing, removing duplicates and stopwords). |\n",
    "| 2Ô∏è‚É£ ÿ™ŸÖÿ´ŸäŸÑ ŸÉŸÑ ÿ¨ŸÖŸÑÿ© ÿ®ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ | Represent each sentence as numbers | ŸÜÿ≠ÿØÿØ ŸáŸÑ ÿßŸÑŸÉŸÑŸÖÿ© ŸÖŸàÿ¨ŸàÿØÿ© (1) ÿ£Ÿà ŸÑÿß (0) ÿ£Ÿà ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿ∏ŸáŸàÿ±Ÿáÿß.                                              | For each sentence, mark presence (1) or absence (0) of each word ‚Äî or use counts if needed.                |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è üá∏üá¶ ŸÖÿ´ÿßŸÑ ŸÖŸÜ ÿßŸÑÿ£ÿØÿ® ÿßŸÑŸÉŸÑÿßÿ≥ŸäŸÉŸä | üá¨üáß Classic Example\n",
    "\n",
    "ŸÖŸÜ ÿ±ŸàÿßŸäÿ© ÿ™ÿ¥ÿßÿ±ŸÑÿ≤ ÿØŸäŸÉŸÜÿ≤ **‚ÄúŸÇÿµÿ© ŸÖÿØŸäŸÜÿ™ŸäŸÜ‚Äù**\n",
    "\n",
    "> It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\n",
    "\n",
    "ÿ®ÿπÿØ ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµ ŸÖŸÜ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ŸàÿßŸÑÿ±ŸÖŸàÿ≤ÿå ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÇÿßÿ¶ŸÖÿ© ŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ:\n",
    "**[it, was, the, best, of, times, worst, age, wisdom, foolishness]**\n",
    "\n",
    "ÿ´ŸÖ ŸÜÿπŸÖŸÑ ÿ¨ÿØŸàŸÑ (ÿ£Ÿà ŸÖÿµŸÅŸàŸÅÿ©) ŸäŸàÿ∂ÿ≠ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸä ŸÉŸÑ ÿ¨ŸÖŸÑÿ©:\n",
    "\n",
    "| ÿßŸÑÿ¨ŸÖŸÑÿ©                        | it | was | the | best | of | times | worst | age | wisdom | foolishness |\n",
    "| ----------------------------- | -- | --- | --- | ---- | -- | ----- | ----- | --- | ------ | ----------- |\n",
    "| It was the best of times      | 1  | 1   | 1   | 1    | 1  | 1     | 0     | 0   | 0      | 0           |\n",
    "| It was the worst of times     | 1  | 1   | 1   | 0    | 1  | 1     | 1     | 0   | 0      | 0           |\n",
    "| It was the age of wisdom      | 1  | 1   | 1   | 0    | 1  | 0     | 0     | 1   | 1      | 0           |\n",
    "| It was the age of foolishness | 1  | 1   | 1   | 0    | 1  | 0     | 0     | 1   | 0      | 1           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### üí¨ üá∏üá¶ ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ®ÿ≥Ÿäÿ∑ ÿ®ŸÖÿ´ÿßŸÑ ŸàÿßŸÇÿπŸä | üá¨üáß Simple Real-World Example\n",
    "\n",
    "üá∏üá¶ **ÿ™ÿÆŸäŸÑ ÿπŸÜÿØŸÉ ŸÖÿ≠ÿßÿØÿ´ÿ™ŸäŸÜ:**\n",
    "\n",
    "1. ‚ÄúÿßŸÑŸäŸàŸÖ ÿßŸÑÿ¨Ÿà ÿ¨ŸÖŸäŸÑ ÿ¨ÿØŸãÿß ŸàÿßŸÑÿ¥ŸÖÿ≥ ŸÖÿ¥ÿ±ŸÇÿ©‚Äù\n",
    "2. ‚ÄúÿßŸÑŸäŸàŸÖ ŸÖŸÖÿ∑ÿ± Ÿàÿ∫ÿßÿ¶ŸÖ‚Äù\n",
    "\n",
    "ŸÉŸäÿ≥ ÿßŸÑŸÉŸÑŸÖÿßÿ™ (**Bag of Words**) ÿ≥ŸäÿπÿßŸÖŸÑ ŸÉŸÑ ÿ¨ŸÖŸÑÿ© ŸÉŸÄ **ŸÇÿßÿ¶ŸÖÿ© ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™** ŸÅŸÇÿ∑:\n",
    "\n",
    "> [\"ÿßŸÑŸäŸàŸÖ\", \"ÿßŸÑÿ¨Ÿà\", \"ÿ¨ŸÖŸäŸÑ\", \"ŸÖŸÖÿ∑ÿ±\", \"ÿ∫ÿßÿ¶ŸÖ\", \"ÿßŸÑÿ¥ŸÖÿ≥\", \"ŸÖÿ¥ÿ±ŸÇÿ©\"]\n",
    "\n",
    "ŸÉŸÑ ÿ¨ŸÖŸÑÿ© ÿ™ÿ™ÿ≠ŸàŸÑ ÿ•ŸÑŸâ **ŸÖÿ™ÿ¨Ÿá (Vector)** ŸÖŸÜ ÿßŸÑÿ£ÿµŸÅÿßÿ± ŸàÿßŸÑŸàÿßÿ≠ÿØÿßÿ™ ÿ≠ÿ≥ÿ® Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿ©.\n",
    "ŸÑŸÉŸÜ ŸÑÿßÿ≠ÿ∏ üëÄ: ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÑÿß Ÿäÿπÿ±ŸÅ ÿ£ŸÜ ‚Äúÿ¨ŸÖŸäŸÑ‚Äù Ÿà‚ÄúŸÖÿ¥ÿ±ŸÇ‚Äù ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ™ÿßŸÜ ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ üå§Ô∏è\n",
    "ŸàŸáŸÜÿß ÿ™ÿ∏Ÿáÿ± **ÿπŸäŸàÿ® BoW** ŸÑÿßÿ≠ŸÇŸãÿß.\n",
    "\n",
    "---\n",
    "\n",
    "üá¨üáß **Imagine you have two sentences:**\n",
    "\n",
    "1. ‚ÄúThe weather is very nice and sunny today.‚Äù\n",
    "2. ‚ÄúToday is rainy and cloudy.‚Äù\n",
    "\n",
    "The **Bag of Words (BoW)** model treats each sentence as just a **list of words**:\n",
    "\n",
    "> [\"today\", \"weather\", \"nice\", \"rainy\", \"cloudy\", \"sunny\"]\n",
    "\n",
    "Each sentence becomes a **vector of 0s and 1s** depending on which words appear.\n",
    "But notice üëÄ: the model doesn‚Äôt understand that *‚Äúnice‚Äù* and *‚Äúsunny‚Äù* are semantically related üå§Ô∏è ‚Äî\n",
    "and that‚Äôs where **BoW‚Äôs main limitations** show up.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### üß† üá∏üá¶ ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÖÿ≥ÿ®ŸÇÿ© ŸÑŸÑŸÜÿµŸàÿµ | üá¨üáß Text Preprocessing\n",
    "\n",
    "ŸÇÿ®ŸÑ ÿ™ÿ∑ÿ®ŸäŸÇ BoWÿå ŸÜÿπŸÖŸÑ ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÑÿ™ÿ≠ÿ≥ŸäŸÜ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ©            | üá¨üáß Step          | üá∏üá¶ ÿßŸÑŸáÿØŸÅ                           | üá¨üáß Purpose                                      |\n",
    "| ---------------------- | ------------------ | ------------------------------------ | ------------------------------------------------- |\n",
    "| ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ≠ÿ±ŸàŸÅ ÿ•ŸÑŸâ ÿµÿ∫Ÿäÿ±ÿ© | Lowercasing        | ŸÑÿ™Ÿàÿ≠ŸäÿØ ÿ¥ŸÉŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™                   | Normalize word form                               |\n",
    "| ÿ≠ÿ∞ŸÅ ŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™ŸàŸÇŸÅ       | Remove stopwords   | ŸÑÿ£ŸÜŸáÿß ŸÑÿß ÿ™ÿ∂ŸäŸÅ ŸÖÿπŸÜŸâ ŸÖÿ´ŸÑ ‚ÄúŸÖŸÜÿå ŸÅŸäÿå ÿπŸÑŸâ‚Äù | Remove uninformative words like ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúin‚Äù |\n",
    "| ÿ≠ÿ∞ŸÅ ÿßŸÑÿπŸÑÿßŸÖÿßÿ™           | Remove punctuation | ŸÑÿ£ŸÜ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑÿß ÿ™ÿπŸÜŸä ÿ¥Ÿäÿ¶Ÿãÿß ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ  | Clean non-word characters                         |\n",
    "\n",
    "---\n",
    "\n",
    "-\n",
    "\n",
    "### üßÆ üá∏üá¶ ŸÖŸÅŸáŸàŸÖ ÿßŸÑŸÄ N-Gram ŸàÿπŸÑÿßŸÇÿ™Ÿá ÿ®ŸÄ Bag of Words\n",
    "\n",
    "### üá¨üáß The Concept of N-Gram and Its Relation to Bag of Words\n",
    "\n",
    "---\n",
    "\n",
    "#### üá∏üá¶ ÿ£ŸàŸÑŸãÿß: ŸÖÿß ŸáŸä ÿßŸÑŸÄ N-Gramÿü\n",
    "\n",
    "ÿßŸÑŸÄ **N-Gram** ŸáŸä ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÜ **ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ™ÿßŸÑŸäÿ©** ÿ™ŸèÿπÿßŸÖŸÑ ŸÉŸàÿ≠ÿØÿ© Ÿàÿßÿ≠ÿØÿ©.\n",
    "\n",
    "* ÿπŸÜÿØŸÖÿß ŸÜÿ£ÿÆÿ∞ **ŸÉŸÑŸÖÿ© Ÿàÿßÿ≠ÿØÿ© ŸÅŸÇÿ∑** ŸÅŸä ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑÿå ŸÅŸáÿ∞ÿß Ÿäÿ≥ŸÖŸâ **Unigram**.\n",
    "* ÿπŸÜÿØŸÖÿß ŸÜÿ£ÿÆÿ∞ **ŸÉŸÑ ŸÉŸÑŸÖÿ™ŸäŸÜ ŸÖÿ™ÿ¨ÿßŸàÿ±ÿ™ŸäŸÜ**ÿå ŸÅŸáÿ∞ÿß Ÿäÿ≥ŸÖŸâ **Bigram**.\n",
    "* ŸàÿπŸÜÿØŸÖÿß ŸÜÿ£ÿÆÿ∞ **ŸÉŸÑ ÿ´ŸÑÿßÿ´ ŸÉŸÑŸÖÿßÿ™ ŸÖÿ™ÿ¨ÿßŸàÿ±ÿ©**ÿå Ÿäÿµÿ®ÿ≠ **Trigram** ‚Ä¶ ŸàŸáŸÉÿ∞ÿß.\n",
    "\n",
    "üëÄ ŸÖÿ´ÿßŸÑ ÿ®ÿ≥Ÿäÿ∑:\n",
    "\n",
    "> ÿßŸÑÿ¨ŸÖŸÑÿ©: ‚ÄúI love natural language processing‚Äù\n",
    "\n",
    "* Unigrams ‚Üí [I], [love], [natural], [language], [processing]\n",
    "* Bigrams ‚Üí [I love], [love natural], [natural language], [language processing]\n",
    "* Trigrams ‚Üí [I love natural], [love natural language], [natural language processing]\n",
    "\n",
    "ÿ•ÿ∞Ÿãÿßÿå ÿ®ÿØŸÑ ÿ£ŸÜ ŸÜŸÜÿ∏ÿ± ÿ•ŸÑŸâ ÿßŸÑŸÉŸÑŸÖÿ© ŸÑŸàÿ≠ÿØŸáÿßÿå ŸÜÿ®ÿØÿ£ ŸÜŸÜÿ∏ÿ± ÿ•ŸÑŸâ **ÿßŸÑÿπŸÑÿßŸÇÿ© ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨ÿßŸàÿ±ÿ©**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üá¨üáß What is an N-Gram?\n",
    "\n",
    "An **N-Gram** is simply a group of **N consecutive words** treated as a single unit.\n",
    "\n",
    "* 1 word ‚Üí **Unigram**\n",
    "* 2 words ‚Üí **Bigram**\n",
    "* 3 words ‚Üí **Trigram**, and so on.\n",
    "\n",
    "üí° Example:\n",
    "\n",
    "> Sentence: ‚ÄúI love natural language processing‚Äù\n",
    "\n",
    "* Unigrams ‚Üí [I], [love], [natural], [language], [processing]\n",
    "* Bigrams ‚Üí [I love], [love natural], [natural language], [language processing]\n",
    "* Trigrams ‚Üí [I love natural], [love natural language], [natural language processing]\n",
    "\n",
    "So instead of analyzing words individually, we analyze **word sequences** to capture short phrases and context.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† üá∏üá¶ ÿπŸÑÿßŸÇÿ™Ÿáÿß ÿ®ŸÄ Bag of Words\n",
    "\n",
    "ÿßŸÑŸÄ **N-Gram** ŸáŸä ŸÅŸä ÿßŸÑÿ£ÿ≥ÿßÿ≥ **ÿ™ÿ∑ŸàŸäÿ± ŸÑŸÅŸÉÿ±ÿ© ÿßŸÑŸÄ Bag of Words**.\n",
    "ŸÅŸä BoWÿå ŸÜÿ≠ÿ≥ÿ® ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÖŸÜŸÅÿµŸÑÿ©ÿå\n",
    "ŸÑŸÉŸÜ ŸÖÿπ N-Gram ŸÜÿ®ÿØÿ£ ŸÜÿ≠ÿ≥ÿ® ÿ™ŸÉÿ±ÿßÿ± **ÿßŸÑÿ™ÿ≥ŸÑÿ≥ŸÑÿßÿ™** ÿ£Ÿäÿ∂Ÿãÿß.\n",
    "ŸäÿπŸÜŸä ÿ®ÿØŸÑ ŸÖÿß ŸÜÿ≠ÿ≥ÿ® Ÿàÿ¨ŸàÿØ ŸÉŸÑŸÖÿ© ‚Äúartificial‚Äù ŸÅŸÇÿ∑ÿå\n",
    "ŸÜÿ≠ÿ≥ÿ® Ÿàÿ¨ŸàÿØ **‚Äúartificial intelligence‚Äù** ŸÉŸÉŸäÿßŸÜ Ÿàÿßÿ≠ÿØ ŸÑŸá ŸÖÿπŸÜŸâ ÿ£ŸÇŸàŸâ.\n",
    "\n",
    "‚úÖ **ÿßŸÑŸÅÿßÿ¶ÿØÿ©:**\n",
    "\n",
    "* ÿ™ÿ≥ÿßÿπÿØ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿπŸÑŸâ ŸÅŸáŸÖ **ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÇÿ±Ÿäÿ® ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™**.\n",
    "* ŸÖÿ´ŸÑŸãÿß ‚Äúnot good‚Äù = ÿ≥ŸÑÿ®Ÿäÿå ÿ®ŸäŸÜŸÖÿß ‚Äúgood‚Äù = ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿå ŸàÿßŸÑŸÅÿ±ŸÇ Ÿäÿ∏Ÿáÿ± ŸÅŸÇÿ∑ ÿπŸÜÿØ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Bigram.\n",
    "\n",
    "‚ö†Ô∏è **ÿßŸÑÿπŸäÿ®:**\n",
    "\n",
    "* ŸÉŸÑŸÖÿß ÿ≤ÿßÿØ Nÿå Ÿäÿ≤ŸäÿØ ÿπÿØÿØ ÿßŸÑÿ™ÿ±ŸÉŸäÿ®ÿßÿ™ ÿßŸÑŸÖŸÖŸÉŸÜÿ© ÿ®ÿ¥ŸÉŸÑ ÿ∂ÿÆŸÖ ÿ¨ÿØŸãÿß\n",
    "  (Ÿäÿ≥ŸÖŸâ Ÿáÿ∞ÿß ÿ®ŸÄ **Dimensionality Explosion**)\n",
    "  ŸÖŸÖÿß Ÿäÿ¨ÿπŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ£ÿ®ÿ∑ÿ£ Ÿàÿ£ŸÉÿ®ÿ± ŸÅŸä ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "#### üá¨üáß Relation to Bag of Words\n",
    "\n",
    "**N-Gram** is an **extension of the Bag of Words model**.\n",
    "In BoW, we count single words.\n",
    "In N-Gram, we count **word sequences** ‚Äî capturing short context.\n",
    "\n",
    "‚úÖ **Benefits:**\n",
    "\n",
    "* Helps the model understand local context (e.g., ‚Äúnot good‚Äù ‚â† ‚Äúgood‚Äù).\n",
    "* Improves accuracy in text classification and sentiment analysis.\n",
    "\n",
    "‚ö†Ô∏è **Drawbacks:**\n",
    "\n",
    "* As N increases, the number of combinations grows dramatically\n",
    "  (**Dimensionality Explosion**).\n",
    "* This increases both **memory usage** and **processing time**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## üá∏üá¶ ŸáŸÑ ÿßŸÑŸÄ N-Gram ŸÖÿß ÿ≤ÿßŸÑ ŸäŸèÿ≥ÿ™ÿÆÿØŸÖÿü ŸàŸáŸÑ ŸÅÿπŸÑÿßŸã ÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑŸÄ BoWÿü\n",
    "\n",
    "### üí¨ ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑŸÖÿÆÿ™ÿµÿ±:\n",
    "\n",
    "> ŸÜÿπŸÖÿå ÿßŸÑŸÄ **N-Gram** ŸÖÿß ÿ≤ÿßŸÑ ŸäŸèÿ≥ÿ™ÿÆÿØŸÖÿå\n",
    "> ŸÑŸÉŸÜŸá ÿßŸÑŸäŸàŸÖ ŸäŸèÿπÿ™ÿ®ÿ± **ÿ∑ÿ±ŸäŸÇÿ© ŸÉŸÑÿßÿ≥ŸäŸÉŸäÿ©** ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑŸÖŸáÿßŸÖ ŸÅŸÇÿ∑ÿå\n",
    "> ÿ®ŸäŸÜŸÖÿß **ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÖÿ´ŸÑ Word2Vec ŸàBERT** ÿ™ÿ¨ÿßŸàÿ≤ÿ™Ÿá ÿ®ŸÖÿ±ÿßÿ≠ŸÑ ŸÉÿ®Ÿäÿ±ÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† ÿßŸÑŸÅŸáŸÖ ÿßŸÑÿ™ŸÅÿµŸäŸÑŸä:\n",
    "\n",
    "#### üîπ ÿ£ŸàŸÑÿßŸã: ŸáŸÑ ÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑŸÄ BoWÿü\n",
    "\n",
    "ÿ•ŸÑŸâ ÿ≠ÿØŸçŸë ŸÖÿß ‚úÖ\n",
    "\n",
    "ÿßŸÑŸÄ **N-Gram** ÿ¨ÿßÿ° ŸÉÿ≠ŸÑŸë ŸÖÿ§ŸÇÿ™ ŸÑÿ£ŸÉÿ®ÿ± ÿπŸäÿ® ŸÅŸä BoW:\n",
    "\n",
    "> ÿ£ŸÜ BoW ŸÑÿß ŸäŸÅŸáŸÖ ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ÿ£Ÿà ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "ŸÅÿ®ÿØŸÑ ŸÖÿß ŸäÿπÿØŸë ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ®ÿ¥ŸÉŸÑ ŸÖŸÜŸÅÿµŸÑÿå\n",
    "ÿµÿßÿ± ŸäŸÇÿ±ÿ£ **ÿ™ÿ≥ŸÑÿ≥ŸÑÿßÿ™ ŸÇÿµŸäÿ±ÿ© ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ (n ŸÉŸÑŸÖÿßÿ™)**\n",
    "Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ÿµÿßÿ± ŸäŸÇÿØÿ± ŸäŸÑÿ™ŸÇÿ∑ ÿ¥ŸàŸäÿ© ŸÖÿπŸÜŸâ ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÇÿ±Ÿäÿ®.\n",
    "\n",
    "ŸÖÿ´ŸÑÿßŸã:\n",
    "\n",
    "* BoW Ÿäÿ¥ŸàŸÅ: ‚Äúnot‚Äù Ÿà‚Äúgood‚Äù ŸÉŸÉŸÑŸÖÿ™ŸäŸÜ ŸÖŸÜŸÅÿµŸÑÿ™ŸäŸÜ ‚Üí ŸàŸäÿ∏ŸÜ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ© üòÖ\n",
    "* N-Gram (Bigram) Ÿäÿ¥ŸàŸÅ ‚Äúnot good‚Äù ŸÉŸàÿ≠ÿØÿ© Ÿàÿßÿ≠ÿØÿ© ‚Üí ŸÅŸäŸÅŸáŸÖ ÿ•ŸÜŸáÿß ÿ≥ŸÑÿ®Ÿäÿ© üëé\n",
    "\n",
    "ŸÑŸÉŸÜ ÿ±ÿ∫ŸÖ Ÿáÿ∞ÿß ÿßŸÑÿ™ÿ∑ŸàŸäÿ±ÿå **ŸÖÿß ÿ≤ÿßŸÑ N-Gram ŸÖÿ≠ÿØŸàÿØ**:\n",
    "\n",
    "* ŸÖÿß ÿ≤ÿßŸÑ ŸÖÿß ŸäŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿ≠ŸÇŸäŸÇŸä ŸÑŸÑŸÉŸÑŸÖÿßÿ™.\n",
    "* ŸàŸÖÿß ŸäŸÇÿØÿ± ŸäŸÖŸäÿ≤ ÿ®ŸäŸÜ ‚Äúgood job‚Äù Ÿà‚Äúgood idea‚Äù ŸÖŸÜ ÿ≠Ÿäÿ´ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿπÿßŸÖ.\n",
    "* ŸàŸÉŸÑ ŸÖÿß ÿ≤ÿßÿØ Nÿå ÿµÿßÿ± ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ´ŸÇŸäŸÑ ÿ¨ÿØŸãÿß (ÿπÿ¥ÿ±ÿßÿ™ ÿ¢ŸÑÿßŸÅ ÿßŸÑÿ£ÿπŸÖÿØÿ© ŸÅŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ©).\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ ÿ´ÿßŸÜŸäŸãÿß: ŸáŸÑ ŸÖÿß ÿ≤ÿßŸÑ ŸäŸèÿ≥ÿ™ÿÆÿØŸÖÿü\n",
    "\n",
    "ŸÜÿπŸÖÿå ŸÑŸÉŸÜ ŸÅŸä ÿ≠ÿßŸÑÿßÿ™ ŸÖÿ≠ÿØÿØÿ© üéØ\n",
    "\n",
    "ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸäŸàŸÖ ŸÅŸä:\n",
    "\n",
    "* ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© (ŸÖÿ´ŸÑ Markov Models Ÿà n-gram language models).\n",
    "* ŸÖÿ≠ÿ±ŸÉÿßÿ™ ÿßŸÑÿ•ŸÉŸÖÿßŸÑ ÿßŸÑÿ™ŸÑŸÇÿßÿ¶Ÿä ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ© (Autocomplete, Predictive Text).\n",
    "* ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿØŸäŸÖÿ© ÿ£Ÿà ÿßŸÑŸÖÿ¥ÿßÿ±Ÿäÿπ ÿßŸÑÿ™ÿπŸÑŸäŸÖŸäÿ©.\n",
    "* Ÿàÿ®ÿπÿ∂ ÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ **ÿßŸÑŸÄ NLP ŸÖŸÜÿÆŸÅÿ∂ÿ© ÿßŸÑŸÖŸàÿßÿ±ÿØ** (Low-Resource NLP) ÿ≠Ÿäÿ´ ŸÖÿß ŸÅŸä ÿ®ŸäÿßŸÜÿßÿ™ ÿ∂ÿÆŸÖÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ ÿ´ÿßŸÑÿ´Ÿãÿß: ŸÖÿßÿ∞ÿß ÿ≠ŸÑŸë ŸÖŸÉÿßŸÜŸáÿü\n",
    "\n",
    "ŸÖŸÜÿ∞ ÿπÿßŸÖ **2013** ŸÖÿπ ÿ∏ŸáŸàÿ± **Word2Vec** ÿ´ŸÖ **GloVe** Ÿàÿ®ÿπÿØŸáÿß **BERT**ÿå\n",
    "ÿßŸÑÿπÿßŸÑŸÖ ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß **ÿßŸÜÿ™ŸÇŸÑ ÿ•ŸÑŸâ ŸÖÿ±ÿ≠ŸÑÿ© ‚ÄúÿßŸÑŸÅŸáŸÖ ÿßŸÑÿØŸÑÿßŸÑŸä ÿßŸÑÿ≠ŸÇŸäŸÇŸä‚Äù** (Semantic Understanding).\n",
    "\n",
    "> ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ¨ÿØŸäÿØÿ© ŸÖÿß ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿπÿØŸë ÿßŸÑŸÉŸÑŸÖÿßÿ™ÿå\n",
    "> ÿ®ŸÑ **ÿ™ÿ™ÿπŸÑŸÖ ÿßŸÑŸÖÿπŸÜŸâ ŸàÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÉÿßŸÖŸÑ ŸÑŸÉŸÑ ŸÉŸÑŸÖÿ©** ŸÖŸÜ ÿÆŸÑÿßŸÑ ŸÖŸÑÿßŸäŸäŸÜ ÿßŸÑÿ¨ŸÖŸÑ.\n",
    "\n",
    "ÿ®ÿßŸÑÿ™ÿßŸÑŸäÿå ŸÜŸÇÿØÿ± ŸÜŸÇŸàŸÑ ÿ®ÿßÿÆÿ™ÿµÿßÿ±:\n",
    "\n",
    "> BoW ‚Üí ŸÖÿ±ÿ≠ŸÑÿ© ÿ£ŸàŸÑŸâ\n",
    "> N-Gram ‚Üí ÿ™ÿ±ŸÇŸäÿπ ŸÖÿ§ŸÇÿ™ ÿ±ÿßÿ¶ÿπ üß©\n",
    "> Word2Vec ‚Üí ÿßŸÑÿ®ÿØÿßŸäÿ© ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ© ŸÑŸÑŸÅŸáŸÖ\n",
    "> BERT ‚Üí ÿπÿµÿ± ÿßŸÑŸÅŸáŸÖ ÿßŸÑÿπŸÖŸäŸÇ ŸÑŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÉÿßŸÖŸÑ ŸÑŸÑÿ¨ŸÖŸÑ üåç\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß In Summary (English Version)\n",
    "\n",
    "**Is N-Gram still used?**\n",
    "\n",
    "> Yes, but mainly for simple or classical NLP tasks.\n",
    "\n",
    "**Did it solve BoW‚Äôs problems?**\n",
    "\n",
    "> Partially.\n",
    "> It helped capture short context (‚Äúnot good‚Äù) but still failed to understand meaning deeply.\n",
    "\n",
    "**Why it‚Äôs less common today?**\n",
    "\n",
    "> Because modern models (Word2Vec, GloVe, FastText, BERT, GPT‚Ä¶)\n",
    "> learn contextual relationships directly ‚Äî without needing fixed N-Grams.\n",
    "\n",
    "**In short:**\n",
    "\n",
    "> BoW ‚Üí counts words\n",
    "> N-Gram ‚Üí counts short word sequences\n",
    "> Word2Vec ‚Üí understands meaning\n",
    "> BERT ‚Üí understands full context\n",
    "\n",
    "---\n",
    "\n",
    "### üí° üá∏üá¶ ÿßŸÑÿπŸäŸàÿ® | üá¨üáß Limitations\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑÿπŸäÿ®                             | üá¨üáß Limitation                        |\n",
    "| -------------------------------------- | -------------------------------------- |\n",
    "| ŸÑÿß ŸäŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®              | Loses meaning and order of words       |\n",
    "| ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑŸÉÿ®Ÿäÿ± ŸÑŸÑŸÖÿµŸÅŸàŸÅÿ©                  | Creates very large, sparse matrices    |\n",
    "| ÿ≠ÿ≥ÿßÿ≥ ŸÑŸÑÿ™ŸÉÿ±ÿßÿ± ŸàÿßŸÑÿ±ŸÖŸàÿ≤                   | Sensitive to text noise and redundancy |\n",
    "| ŸÑÿß ŸäŸÖŸäŸëÿ≤ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÖÿπŸÜŸàŸäŸãÿß | Cannot understand synonyms or context  |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ü™Ñ üá∏üá¶ ÿÆŸÑÿßÿµÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© | üá¨üáß Quick Summary\n",
    "\n",
    "üá∏üá¶ **ÿßŸÑÿÆŸÑÿßÿµÿ© ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©:**\n",
    "\n",
    "* üß© BoW ŸáŸà **ÿ£ŸàŸÑ ÿÆÿ∑Ÿàÿ© ŸÑŸÅŸáŸÖ ÿßŸÑŸÜÿµŸàÿµ ÿ®ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ**.\n",
    "* üìä ŸÑŸÉŸÜŸá Ÿäÿπÿ™ŸÖÿØ ŸÅŸÇÿ∑ ÿπŸÑŸâ **ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ŸàÿßŸÑÿ•ÿ≠ÿµÿßÿ°** ŸàŸÑŸäÿ≥ ÿπŸÑŸâ **ÿßŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿßŸÑÿ≥ŸäÿßŸÇ**.\n",
    "* üöÄ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÖÿ´ŸÑ **Word2Vec** Ÿà **BERT** ÿ¨ÿßÿ°ÿ™ ŸÑÿ™ŸÉŸÖŸëŸÑ Ÿáÿ∞ÿß ÿßŸÑŸÜŸÇÿµ Ÿàÿ™ŸÅŸáŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿ®ÿπŸÖŸÇ ÿ£ŸÉÿ®ÿ±.\n",
    "\n",
    "---\n",
    "\n",
    "üá¨üáß **English Summary:**\n",
    "\n",
    "* üß© **BoW** was the **first step toward representing text as numbers.**\n",
    "* üìä However, it relies only on **frequency and statistics**, not **meaning or context.**\n",
    "* üöÄ Modern models like **Word2Vec** and **BERT** were developed to overcome these limits and understand language more deeply.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ üá∏üá¶ ÿÆÿßÿ™ŸÖÿ© ÿ™ÿ≠ŸÅŸäÿ≤Ÿäÿ© ŸÑŸÑŸÇÿ≥ŸÖ ÿßŸÑŸÇÿßÿØŸÖ\n",
    "\n",
    "> üá∏üá¶\n",
    "> ŸàÿßŸÑÿ¢ŸÜ ÿ®ÿπÿØŸÖÿß ÿπÿ±ŸÅŸÜÿß ŸÉŸäŸÅ ŸÜÿ≠ŸàŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ÿ£ÿ±ŸÇÿßŸÖ ÿ®ÿ≥Ÿäÿ∑ÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ BoWÿå\n",
    "> ÿ≠ÿßŸÜ ÿßŸÑŸàŸÇÿ™ ŸÜÿÆÿ∑Ÿà ÿÆÿ∑Ÿàÿ© ŸÑŸÑÿ£ŸÖÿßŸÖ ŸàŸÜÿ™ÿπŸÑŸÖ ŸÉŸäŸÅ ŸÜÿ¨ÿπŸÑ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ **ÿ™ŸÅŸáŸÖ ÿßŸÑŸÖÿπÿßŸÜŸä**ÿå\n",
    "> ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿ™ŸÇŸÜŸäÿßÿ™ ÿ£ŸÉÿ´ÿ± ÿ∞ŸÉÿßÿ°Ÿã ŸÖÿ´ŸÑ **TF-IDF** Ÿà **Word2Vec**.\n",
    "> ÿßÿ≥ÿ™ÿπÿØ ŸÑÿ£ŸÜ ÿßŸÑŸÇÿßÿØŸÖ ŸáŸà ÿ®ÿØÿßŸäÿ© ÿßŸÑÿ≥ÿ≠ÿ± ÿßŸÑÿ≠ŸÇŸäŸÇŸä ŸÅŸä ÿßŸÑŸÄ NLP! ‚ú®\n",
    "\n",
    "> üá¨üáß\n",
    "> Now that we‚Äôve learned how to represent text with simple numbers using BoW,\n",
    "> it‚Äôs time to move forward and make those numbers **understand meaning**.\n",
    "> Next, we‚Äôll explore smarter methods like **TF-IDF** and **Word2Vec** ‚Äî\n",
    "> where mathematics meets language magic. ‚ú®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79337471",
   "metadata": {},
   "source": [
    "### üá∏üá¶ ŸÖŸÇÿØŸÖÿ©\n",
    "\n",
    "ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÇÿ≥ŸÖ ÿ≥ŸÜŸÇŸàŸÖ ÿ®ÿ™ÿ∑ÿ®ŸäŸÇ **Bag of Words (BoW)** ÿπŸÖŸÑŸäŸãÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿ®ÿßŸäÿ´ŸàŸÜ.\n",
    "ÿ≥ŸÜÿ≠ŸàŸëŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿ±ŸÇŸÖŸäÿ© ÿ™Ÿèÿ∏Ÿáÿ± **Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ Ÿàÿ™ŸÉÿ±ÿßÿ±Ÿáÿß**ÿå ÿØŸàŸÜ ÿßŸÑŸÜÿ∏ÿ± ÿ•ŸÑŸâ ŸÖÿπŸÜÿßŸáÿß.\n",
    "ÿ±ÿ∫ŸÖ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ£ÿ≥ŸÑŸàÿ® ŸÇÿØŸäŸÖ ŸÜŸàÿπŸãÿß ŸÖÿß ŸÅŸä 2025ÿå ÿ•ŸÑÿß ÿ£ŸÜŸá ŸÑÿß Ÿäÿ≤ÿßŸÑ ŸÖŸÅŸäÿØŸãÿß ÿ¨ÿØŸãÿß ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑŸÖŸáÿßŸÖ .\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Introduction\n",
    "\n",
    "In this section, we‚Äôll implement **Bag of Words (BoW)** in Python.\n",
    "We‚Äôll convert text into numerical vectors showing **the presence and frequency of words**,\n",
    "without understanding their meaning.\n",
    "\n",
    "Although BoW is considered a **classic** method by 2025, it‚Äôs still useful.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef45d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function: clean and split sentence into words\n",
    "def word_extraction(sentence):\n",
    "    # Remove punctuation and split text\n",
    "    words = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
    "    # Convert to lowercase and remove English stopwords\n",
    "    cleaned_text = [w.lower() for w in words if w not in stopwords.words('english')]\n",
    "    return cleaned_text\n",
    "\n",
    "# Function: create sorted list of unique words (vocabulary)\n",
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "    # Remove duplicates and sort alphabetically\n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f74287",
   "metadata": {},
   "source": [
    "### üá∏üá¶ ÿßŸÑÿ¥ÿ±ÿ≠\n",
    "\n",
    "ÿßŸÑÿØÿßŸÑÿ© `word_extraction()` ÿ™ŸÜÿ∏ŸëŸÅ ÿßŸÑŸÜÿµ ŸÖŸÜ ÿßŸÑÿ±ŸÖŸàÿ≤ Ÿàÿ™ÿ≠ÿ∞ŸÅ ŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™ŸàŸÇŸÅ ŸÖÿ´ŸÑ ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù.\n",
    "ÿ£ŸÖÿß `tokenize()` ŸÅÿ™ŸÜÿ¥ÿ¶ **ŸÇÿßÿ¶ŸÖÿ© ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÅÿ±ŸäÿØÿ©** ÿßŸÑÿ™Ÿä ÿ≥ŸÜÿ®ŸÜŸä ÿπŸÑŸäŸáÿß ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÄ BoW.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Explanation\n",
    "\n",
    "`word_extraction()` cleans the text by removing punctuation and stopwords.\n",
    "`tokenize()` creates a **sorted list of unique words**, forming the vocabulary for the BoW model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0389b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arrived', 'bus', 'early', 'mary', 'noon', 'samantha', 'station', 'waited']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"\n",
    "\n",
    "# Test cleaning function\n",
    "word_extraction(text)\n",
    "\n",
    "# Test tokenization\n",
    "tokenize(word_extraction(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256ab2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Generate Bag of Words manually\n",
    "def generate_bow(allsentences):\n",
    "    vocab = tokenize(allsentences)\n",
    "    print(\"Word List for Document \\n{0} \\n\".format(vocab))\n",
    "    \n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        bag_vector = numpy.zeros(len(vocab))  # Create a vector of zeros\n",
    "        \n",
    "        # Count each word's presence in the sentence\n",
    "        for w in words:\n",
    "            for i, word in enumerate(vocab):\n",
    "                if word == w:\n",
    "                    bag_vector[i] += 1\n",
    "        \n",
    "        print(\"{0}\\n{1}\\n\".format(sentence, numpy.array(bag_vector)))\n",
    "        print('---------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29293b97",
   "metadata": {},
   "source": [
    "### üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ©\n",
    "\n",
    "ŸÉŸÑ ÿ¨ŸÖŸÑÿ© ÿ™ÿ™ÿ≠ŸàŸÑ ÿ•ŸÑŸâ **ŸÖÿ™ÿ¨Ÿá ŸÖŸÜ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ** Ÿäÿπÿ®Ÿëÿ± ÿπŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸäŸáÿßÿå\n",
    "ÿ®ÿØŸàŸÜ ÿ£Ÿä ŸÅŸáŸÖ ŸÑŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®.\n",
    "\n",
    "ŸÑŸÉŸÜ ŸÑÿßÿ≠ÿ∏ üëá\n",
    "BoW ŸÑÿß ŸäÿØÿ±ŸÉ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ:\n",
    "\n",
    "> ‚Äúÿ¨ŸÖŸäŸÑ‚Äù Ÿà ‚ÄúŸÖÿ¥ÿ±ŸÇ‚Äù\n",
    "> ŸàŸÑÿß Ÿäÿπÿ±ŸÅ ÿ£ŸÜ ‚ÄúÿßŸÑŸäŸàŸÖ ŸÖŸÖÿ∑ÿ±‚Äù ÿ™ÿπŸÜŸä ÿ¥Ÿäÿ¶Ÿãÿß ÿπŸÉÿ≥ ‚ÄúÿßŸÑŸäŸàŸÖ ŸÖÿ¥ŸÖÿ≥‚Äù.\n",
    "\n",
    "üîπ ŸÅŸä 2025ÿå ŸÜÿßÿØÿ±ÿßŸã ŸÖÿß ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ BoW ŸÑŸàÿ≠ÿØŸáÿå\n",
    "ÿ®ŸÑ ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπ ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ÿ£ÿ®ÿ≥ÿ∑ ÿ£Ÿà ŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿ≥ÿ±Ÿäÿπ ŸÇÿ®ŸÑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿ™ŸÇŸÜŸäÿßÿ™ ŸÖÿ´ŸÑ **Word2Vec** ÿ£Ÿà **Transformer Models**.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Concept\n",
    "\n",
    "Each sentence becomes a **vector of numbers**, representing which words appear in it ‚Äî\n",
    "without understanding meaning or word order.\n",
    "\n",
    "BoW doesn‚Äôt recognize relationships like:\n",
    "\n",
    "> ‚Äúnice‚Äù ‚âà ‚Äúbeautiful‚Äù\n",
    "> ‚Äúrainy‚Äù ‚â† ‚Äúsunny‚Äù\n",
    "\n",
    "üîπ In 2025, BoW is rarely used alone.\n",
    "It‚Äôs mainly used for **quick text analysis** or in **lightweight NLP models** before moving to more advanced embeddings like **Word2Vec** or **BERT**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3265b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List for Document \n",
      "['arrived', 'bus', 'early', 'i', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'the', 'took', 'train', 'waited'] \n",
      "\n",
      "Joe waited for the train\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "\n",
      "---------------------------------------------------\n",
      "The train was late\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "Mary and Samantha took the bus\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "I looked for Mary and Samantha at the bus station\n",
      "[0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
      "[1. 2. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "allsentences = [\n",
    "    \"Joe waited for the train\",\n",
    "    \"The train was late\",\n",
    "    \"Mary and Samantha took the bus\",\n",
    "    \"I looked for Mary and Samantha at the bus station\",\n",
    "    \"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"\n",
    "]\n",
    "\n",
    "# Generate BoW representation for each sentence\n",
    "generate_bow(allsentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0971bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe waited for the train\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]\n",
      "---------------------------------------------------\n",
      "The train was late\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "---------------------------------------------------\n",
      "Mary and Samantha took the bus\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n",
      "---------------------------------------------------\n",
      "I looked for Mary and Samantha at the bus station\n",
      "[1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "---------------------------------------------------\n",
      "Mary and Samantha arrived at the bus station early but waited until noon for the bus\n",
      "[1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0]\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform sentences into numerical vectors\n",
    "X = vectorizer.fit_transform(allsentences)\n",
    "\n",
    "# Display results\n",
    "for i in range(len(allsentences)):\n",
    "    print(allsentences[i])\n",
    "    print(list(X.toarray()[i]))\n",
    "    print('---------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a36352",
   "metadata": {},
   "source": [
    "### üá∏üá¶ ÿßŸÑÿ¥ÿ±ÿ≠\n",
    "\n",
    "ÿ®ÿØŸÑÿßŸã ŸÖŸÜ ÿ®ŸÜÿßÿ° BoW ŸäÿØŸàŸäŸãÿßÿå ÿ™ŸàŸÅÿ± ŸÖŸÉÿ™ÿ®ÿ© **scikit-learn** ÿ£ÿØÿßÿ© `CountVectorizer`\n",
    "ÿßŸÑÿ™Ÿä ÿ™ŸÇŸàŸÖ ÿ®ŸÉŸÑ ÿßŸÑÿπŸÖŸÑŸäÿßÿ™ ÿ™ŸÑŸÇÿßÿ¶ŸäŸãÿß ‚Äî ÿ™ŸÜÿ∏ŸäŸÅÿå ÿ™ÿ¨ÿ≤ÿ¶ÿ©ÿå ŸàÿπÿØŸë ÿßŸÑŸÉŸÑŸÖÿßÿ™.\n",
    "\n",
    "ŸÑŸÉŸÜ BoW Ÿäÿ∏ŸÑ **ÿ®ÿ≥Ÿäÿ∑Ÿãÿß ŸàŸÅÿπÿßŸÑŸãÿß** ŸÖÿπ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÇÿµŸäÿ±ÿ© ÿ£Ÿà ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ÿßŸÑÿ™ÿπŸÑŸäŸÖŸäÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Explanation\n",
    "\n",
    "Instead of manually building BoW, the **scikit-learn** library provides `CountVectorizer`,\n",
    "which automates cleaning, tokenizing, and counting.\n",
    "\n",
    "BoW remains **simple and effective** for small-scale or educational NLP tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ea3db",
   "metadata": {},
   "source": [
    "* BoW ŸäÿπŸÖŸÑ **ÿ®ÿ¥ŸÉŸÑ ŸÖŸÖÿ™ÿßÿ≤ ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©** ŸÑÿ£ŸÜ ÿ¢ŸÑŸäÿ™Ÿá ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ **ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ©** ŸàŸÑŸäÿ≥ ŸÖÿπŸÜÿßŸáÿß.\n",
    "* BoW performs **very well with Arabic** since it focuses on **word occurrence**, not meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16525a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List for Document \n",
      "['ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©', 'ÿßŸÑŸä', 'ÿßŸÑŸäŸàŸÖ', 'ÿ∞Ÿáÿ®', 'ÿ¥ÿ±ŸäŸÅ', 'ŸÖÿ≠ŸÖÿØ', 'Ÿáÿ∞ÿß', 'Ÿà'] \n",
      "\n",
      "ÿ∞Ÿáÿ®\n",
      "[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ŸÖÿ≠ŸÖÿØ\n",
      "[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "Ÿà\n",
      "[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿ¥ÿ±ŸäŸÅ\n",
      "[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿßŸÑŸä\n",
      "[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "Ÿáÿ∞ÿß\n",
      "[0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿßŸÑŸäŸàŸÖ\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"ÿ∞Ÿáÿ® ŸÖÿ≠ŸÖÿØ Ÿà ÿ¥ÿ±ŸäŸÅ ÿßŸÑŸä ÿßŸÑŸÖÿØÿ±ÿ≥ÿ© Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ\"\n",
    "word_extraction(text)\n",
    "tokenize(word_extraction(text))\n",
    "generate_bow(word_extraction(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af44325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List for Document \n",
      "['ÿßÿ∏ŸÜ', 'ÿßÿπÿ™ŸÇÿØ', 'ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±', 'ÿßŸÑÿ¨ÿßŸÖÿπÿ©', 'ÿßŸÑÿ≥ŸÅÿ±', 'ÿßŸÑŸä', 'ÿßŸÑŸäŸàŸÖ', 'ÿßŸÜ', 'ÿ®ŸÜÿß', 'ÿ™ŸÖŸÉŸÜ', 'ÿ¨Ÿàÿ±ÿ¨', 'ÿ∞Ÿáÿ®', 'ÿ≥ŸáŸÑÿß', 'ÿ≥Ÿäÿ™ÿµŸÑ', 'ÿ≥ŸäŸÉŸàŸÜ', 'ŸÅŸä', 'ŸÑÿß', 'ŸÖÿ≠ŸÖÿØ', 'ŸÖŸÜ', 'ŸÖŸÜŸä', 'ŸÜÿ¨ÿ≠ÿ™', 'Ÿáÿ∞ÿß'] \n",
      "\n",
      "ÿ∞Ÿáÿ® ŸÖÿ≠ŸÖÿØ ÿßŸÑŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ŸÜÿ¨ÿ≠ÿ™ ŸÖŸÜŸä ŸÅŸä ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿ™ŸÖŸÉŸÜ ŸÖÿ≠ŸÖÿØ ŸÖŸÜ ÿßŸÑÿ≥ŸÅÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ÿßÿπÿ™ŸÇÿØ ÿßŸÜ ÿ¨Ÿàÿ±ÿ¨ ÿ≥Ÿäÿ™ÿµŸÑ ÿ®ŸÜÿß ÿßŸÑŸäŸàŸÖ\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n",
      "ŸÑÿß ÿßÿ∏ŸÜ ÿßŸÜ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ± ÿ≥ŸäŸÉŸàŸÜ ÿ≥ŸáŸÑÿß\n",
      "[1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "allsentences = [\n",
    "    \"ÿ∞Ÿáÿ® ŸÖÿ≠ŸÖÿØ ÿßŸÑŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©\",\n",
    "    \"ŸÜÿ¨ÿ≠ÿ™ ŸÖŸÜŸä ŸÅŸä ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±\",\n",
    "    \"ÿ™ŸÖŸÉŸÜ ŸÖÿ≠ŸÖÿØ ŸÖŸÜ ÿßŸÑÿ≥ŸÅÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ\",\n",
    "    \"ÿßÿπÿ™ŸÇÿØ ÿßŸÜ ÿ¨Ÿàÿ±ÿ¨ ÿ≥Ÿäÿ™ÿµŸÑ ÿ®ŸÜÿß ÿßŸÑŸäŸàŸÖ\",\n",
    "    \"ŸÑÿß ÿßÿ∏ŸÜ ÿßŸÜ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ± ÿ≥ŸäŸÉŸàŸÜ ÿ≥ŸáŸÑÿß\"\n",
    "]\n",
    "\n",
    "generate_bow(allsentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add6617",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üßÆ ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ±ÿßÿ®ÿπ: üá∏üá¶ **TF-IDF** | üá¨üáß **Term Frequency ‚Äì Inverse Document Frequency**\n",
    "\n",
    "---\n",
    "\n",
    "## üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "ÿ∑ÿ±ŸäŸÇÿ© **TF-IDF** ÿ™ŸèÿπÿØ ŸÖŸÜ ÿ£ŸáŸÖ ÿßŸÑÿ£ÿØŸàÿßÿ™ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ© ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ© **(NLP)**ÿå\n",
    "Ÿàÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ÿ®ŸÉÿ´ÿ±ÿ© ŸÅŸä ŸÖŸáÿßŸÖ ŸÖÿ´ŸÑ **ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿµŸàÿµ (Text Classification)** Ÿà**ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ¥ÿßÿπÿ± (Sentiment Analysis)**.\n",
    "\n",
    "ŸáŸä ÿ™ÿ∑ŸàŸäÿ± ÿ∞ŸÉŸä ŸÑŸÅŸÉÿ±ÿ© **Bag of Words (BoW)**:\n",
    "\n",
    "* BoW ŸÉÿßŸÜ ŸäŸáÿ™ŸÖ ŸÅŸÇÿ∑ **ÿ®Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿ© Ÿàÿ™ŸÉÿ±ÿßÿ±Ÿáÿß**.\n",
    "* TF-IDF Ÿäÿ∂ŸäŸÅ **ÿπÿßŸÖŸÑŸãÿß ÿ∞ŸÉŸäŸãÿß** ŸäŸàÿßÿ≤ŸÜ ÿ®ŸäŸÜ **ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµ** Ÿà**ŸÜÿØÿ±ÿ™Ÿáÿß ÿπÿ®ÿ± ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑŸÜÿµŸàÿµ**.\n",
    "\n",
    "ÿßŸÑŸáÿØŸÅ ŸáŸà ÿßŸÑÿ™ŸÖŸäŸäÿ≤ ÿ®ŸäŸÜ **ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÖŸäÿ≤ÿ© ŸÅÿπŸÑŸãÿß** Ÿà**ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿπÿßŸÖÿ© ÿπÿØŸäŸÖÿ© ÿßŸÑŸÅÿßÿ¶ÿØÿ©** ŸÖÿ´ŸÑ \"Ÿáÿ∞ÿß\" ÿ£Ÿà \"ÿßŸÑÿ∞Ÿä\".\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Core Concept\n",
    "\n",
    "**TF-IDF** is a powerful statistical method in **Natural Language Processing (NLP)**.\n",
    "It‚Äôs widely used in **text classification** and **sentiment analysis**.\n",
    "\n",
    "It builds upon the **Bag of Words** model:\n",
    "\n",
    "* BoW only counts word occurrences.\n",
    "* TF-IDF adds intelligence by balancing **word frequency within a document** and **word rarity across documents**.\n",
    "\n",
    "The goal is to highlight **unique, meaningful words** and down-weight **common filler words** like ‚Äúthe‚Äù or ‚Äúis‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ TF-IDF Ÿà BoW | Difference from BoW\n",
    "\n",
    "| BoW                                   | TF-IDF                           |\n",
    "| ------------------------------------- | -------------------------------- |\n",
    "| Ÿäÿ≠ÿ≥ÿ® Ÿàÿ¨ŸàÿØ ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸÇÿ∑                  | Ÿäÿ≠ÿ≥ÿ® Ÿàÿ¨ŸàÿØŸáÿß √ó ÿ£ŸáŸÖŸäÿ™Ÿáÿß            |\n",
    "| ŸÑÿß ŸäŸÅÿ±ŸëŸÇ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ŸàÿßŸÑŸÜÿßÿØÿ±ÿ© | Ÿäÿπÿ∑Ÿä Ÿàÿ≤ŸÜŸãÿß ÿ£ŸÉÿ®ÿ± ŸÑŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ©  |\n",
    "| Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ÿßŸÑÿÆÿßŸÖ               | Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÜÿ≥ÿ®Ÿä ŸàÿßŸÑŸÜÿØÿ±ÿ© |\n",
    "| ŸÖŸÜÿßÿ≥ÿ® ŸÑŸÑÿ•ÿ≠ÿµÿßÿ° ÿßŸÑÿ®ÿ≥Ÿäÿ∑ ŸÅŸÇÿ∑              | ŸÖŸÜÿßÿ≥ÿ® ŸÑŸÑŸÅŸáŸÖ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿä ÿßŸÑÿ£ÿπŸÖŸÇ      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è üá∏üá¶ ŸÉŸäŸÅ ÿ®ÿØÿ£ ÿßŸÑÿ™ÿ∑Ÿàÿ±ÿü\n",
    "\n",
    "ÿ™ÿÆŸäŸÑ ÿ£ŸÜŸÉ ÿ™ÿ≠ŸÑŸÑ **ÿ±ŸàÿßŸäÿßÿ™ ÿ¥ŸÉÿ≥ÿ®Ÿäÿ±** Ÿàÿ™ÿ≠ÿµŸä ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ£ÿ®ÿ∑ÿßŸÑ.\n",
    "ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ BoWÿå ÿ™ÿπÿ±ŸÅ ŸÅŸÇÿ∑ ÿ•ŸÜ ŸÉÿßŸÜÿ™ ŸÉŸÑŸÖÿ© ‚ÄúŸÇŸäÿµÿ±‚Äù ŸÖŸàÿ¨ŸàÿØÿ© ÿ£ŸÖ ŸÑÿß.\n",
    "ŸÑŸÉŸÜ TF-IDF Ÿäÿ∞Ÿáÿ® ÿ£ÿ®ÿπÿØ ŸÖŸÜ ÿ∞ŸÑŸÉ ‚Äî ŸÅŸáŸà Ÿäÿπÿ±ŸÅ ÿ£ŸÜŸáÿß **ÿ™ŸÉÿ±ÿ±ÿ™ ÿ£ŸÉÿ´ÿ± ÿ®ŸÄ150 ŸÖÿ±ÿ© ŸÖŸÜ \"ŸÖŸäÿ±ÿ≥Ÿä\"**ÿå\n",
    "ŸàŸäŸÅŸáŸÖ ÿ£ŸÜ ‚ÄúŸÇŸäÿµÿ±‚Äù **ŸÉŸÑŸÖÿ© ŸÖÿ≠Ÿàÿ±Ÿäÿ© ŸÅŸä ÿßŸÑÿ±ŸàÿßŸäÿ©**.\n",
    "\n",
    "ŸÑŸÉŸÜ ŸÑÿßÿ≠ÿ∏ üëá\n",
    "BoW ŸàTF ŸäÿπÿßŸÜŸäÿßŸÜ ŸÖŸÜ ŸÖÿ¥ŸÉŸÑÿ©: **ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ŸÑÿß ŸäŸèÿ§ÿÆÿ∞ ÿ®ÿßŸÑÿ≠ÿ≥ÿ®ÿßŸÜ.**\n",
    "ÿßŸÑÿ¨ŸÖŸÑÿ™ÿßŸÜ:\n",
    "\n",
    "> ‚ÄúŸÅÿ±ŸÜÿ≥ÿß ÿØŸàŸÑÿ© ÿ∫ŸÜŸäÿ© ÿ®ŸäŸÜŸÖÿß ÿ£ŸÑŸÖÿßŸÜŸäÿß Ÿäÿ™ÿ±ÿßÿ¨ÿπ ÿßŸÇÿ™ÿµÿßÿØŸáÿß.‚Äù\n",
    "> ‚Äúÿ£ŸÑŸÖÿßŸÜŸäÿß ÿØŸàŸÑÿ© ÿ∫ŸÜŸäÿ© ÿ®ŸäŸÜŸÖÿß ŸÅÿ±ŸÜÿ≥ÿß Ÿäÿ™ÿ±ÿßÿ¨ÿπ ÿßŸÇÿ™ÿµÿßÿØŸáÿß.‚Äù\n",
    "> ŸÑŸáŸÖÿß ŸÜŸÅÿ≥ ŸÇŸäŸÖÿ© BoWÿå ÿ±ÿ∫ŸÖ ÿßÿÆÿ™ŸÑÿßŸÅ ÿßŸÑŸÖÿπŸÜŸâ.\n",
    "> ŸàŸÑÿ∞ŸÑŸÉ ÿ¨ÿßÿ° ÿ™ÿ∑ŸàŸäÿ± ‚ÄúÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ÿßŸÑŸÖŸàÿ∂ÿπŸä (Positional Encoding)‚Äù ŸÑÿßÿ≠ŸÇŸãÿß ŸÑÿπŸÑÿßÿ¨ Ÿáÿ∞ÿß ÿßŸÑŸÜŸÇÿµÿå\n",
    "> ŸÑŸÉŸÜŸëŸá ŸÖŸàÿ∂Ÿàÿπ ŸÑÿßÿ≠ŸÇ ÿÆÿßÿ±ÿ¨ ŸÜÿ∑ÿßŸÇ TF-IDF.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Evolution Example\n",
    "\n",
    "Imagine analyzing **Shakespeare‚Äôs novels**:\n",
    "BoW tells you if ‚ÄúCaesar‚Äù appears or not,\n",
    "but TF-IDF shows that ‚ÄúCaesar‚Äù appeared **150√ó more often** than ‚ÄúMercy‚Äù,\n",
    "so it must be **more central to the story**.\n",
    "\n",
    "However, word order is ignored ‚Äî\n",
    "so ‚ÄúFrance is rich but Germany‚Äôs economy is falling‚Äù\n",
    "is treated the same as the reverse version.\n",
    "That‚Äôs one limitation of TF-only methods, later improved by **positional encoding** techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ ÿßŸÑŸÖŸÉŸàŸÜÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© | Components\n",
    "\n",
    "| ÿßŸÑÿ±ŸÖÿ≤        | ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠                      | ÿßŸÑÿ¥ÿ±ÿ≠                                                           |\n",
    "| ------------ | ---------------------------- | --------------------------------------------------------------- |\n",
    "| **TF**       | *Term Frequency*             | ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ÿØÿßÿÆŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ √∑ ÿßŸÑÿπÿØÿØ ÿßŸÑŸÉŸÑŸä ŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ |\n",
    "| **IDF**      | *Inverse Document Frequency* | ŸÖÿØŸâ ŸÜÿØÿ±ÿ© ÿßŸÑŸÉŸÑŸÖÿ© ÿπÿ®ÿ± ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™                              |\n",
    "| **TF √ó IDF** | ÿßŸÑŸàÿ≤ŸÜ ÿßŸÑŸÜŸáÿßÿ¶Ÿä                | ŸÇŸäŸÖÿ© ÿ™ŸèŸÖÿ´ŸÑ ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÉŸÑŸÖÿ© ÿØÿßÿÆŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ŸÖŸÇÿßÿ±ŸÜÿ©Ÿã ÿ®ÿ®ŸÇŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™    |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ŸÑŸÖÿßÿ∞ÿß ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÑŸàÿ∫ÿßÿ±Ÿäÿ™ŸÖÿü\n",
    "\n",
    "ŸÑŸà ÿ∏Ÿáÿ±ÿ™ ŸÉŸÑŸÖÿ© 20 ŸÖÿ±ÿ© ŸÅŸä ŸÖÿ≥ÿ™ŸÜÿØÿå Ÿáÿ∞ÿß ŸÑÿß ŸäÿπŸÜŸä ÿ£ŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ÿ£ŸáŸÖ 20 ŸÖÿ±ÿ© ŸÖŸÜ ÿ¢ÿÆÿ± ÿ∏Ÿáÿ±ÿ™ ŸÅŸäŸá ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ©.\n",
    "ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÑŸàÿ∫ÿßÿ±Ÿäÿ™ŸÖ ŸÑÿ™ŸÇŸÑŸäŸÑ ÿßŸÑŸÅÿ±ŸàŸÇ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ÿ®ŸäŸÜ ÿßŸÑÿ™ŸÉÿ±ÿßÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÑŸäÿ©.\n",
    "ŸÅÿ™ÿµÿ®ÿ≠ ÿßŸÑÿπŸÑÿßŸÇÿ© ÿ∫Ÿäÿ± ÿÆÿ∑Ÿäÿ© ÿ£ŸÉÿ´ÿ± ŸàÿßŸÇÿπŸäÿ©:\n",
    "\n",
    "* ŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ© ‚Üí 1\n",
    "* ŸÖÿ±ÿ™ŸäŸÜ ‚Üí 1.3\n",
    "* 10 ŸÖÿ±ÿßÿ™ ‚Üí 2\n",
    "* 1000 ŸÖÿ±ÿ© ‚Üí 4\n",
    "\n",
    "ÿ®ÿßŸÑÿ™ÿßŸÑŸäÿå TF ŸäÿπŸÉÿ≥ **ŸÖÿØŸâ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµ**ÿå\n",
    "ŸÑŸÉŸÜ **IDF ŸäÿπŸÉÿ≥ ŸÖÿØŸâ ŸÜÿØÿ±ÿ™Ÿáÿß ÿπÿ®ÿ± ÿßŸÑŸÜÿµŸàÿµ**.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Why a Logarithmic Scale?\n",
    "\n",
    "Because frequency grows too fast linearly.\n",
    "A word appearing 20 times doesn‚Äôt mean it‚Äôs 20√ó more important.\n",
    "Using logarithms compresses large frequencies:\n",
    "1 ‚Üí 1, 2 ‚Üí 1.3, 10 ‚Üí 2, 1000 ‚Üí 4.\n",
    "\n",
    "Thus, TF captures **within-document frequency**,\n",
    "while IDF captures **across-document rarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê üá∏üá¶ ÿ≠ÿ≥ÿßÿ® IDF ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©\n",
    "\n",
    "ŸÜÿ≠ÿ≥ÿ® ÿ±ŸÇŸÖŸäŸÜ:\n",
    "\n",
    "* **N:** ÿπÿØÿØ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™.\n",
    "* **df:** ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸä ÿßŸÑŸÉŸÑŸÖÿ©.\n",
    "\n",
    "[\n",
    "IDF = \\log\\left(\\frac{N}{df}\\right)\n",
    "]\n",
    "\n",
    "ŸÉŸÑŸÖÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÉŸÑŸÖÿ© ŸÜÿßÿØÿ±ÿ© ‚Üí ÿ≤ÿßÿØÿ™ ŸÇŸäŸÖÿ© IDF\n",
    "ŸÉŸÑŸÖÿß ŸÉÿßŸÜÿ™ ÿ¥ÿßÿ¶ÿπÿ© ‚Üí ŸÇŸÑÿ™ ŸÇŸäŸÖÿ™Ÿáÿß.\n",
    "\n",
    "ŸÖÿ´ŸÑŸãÿß:\n",
    "\n",
    "* ÿ∏Ÿáÿ±ÿ™ ŸÅŸä 10 ŸÖŸÇÿßŸÑÿßÿ™ ŸÅŸÇÿ∑ ŸÖŸÜ ŸÖŸÑŸäŸàŸÜ ‚Üí log(1,000,000/10) = 5\n",
    "* ÿ∏Ÿáÿ±ÿ™ ŸÅŸä ŸÜÿµŸÅŸáÿß ‚Üí log(2) = 0.3\n",
    "* ÿ∏Ÿáÿ±ÿ™ ŸÅŸä ÿ¨ŸÖŸäÿπŸáÿß ‚Üí log(1) = 0 ‚Üí ÿ£Ÿä ÿ∫Ÿäÿ± ŸÖŸÅŸäÿØÿ© ÿ•ÿ≠ÿµÿßÿ¶ŸäŸãÿß.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Step-by-Step IDF Calculation\n",
    "\n",
    "Two values:\n",
    "\n",
    "* **N:** Total number of documents\n",
    "* **df:** Number of documents containing the word\n",
    "\n",
    "[\n",
    "IDF = \\log\\left(\\frac{N}{df}\\right)\n",
    "]\n",
    "\n",
    "A word appearing in all documents ‚Üí IDF = 0\n",
    "Rare word ‚Üí higher IDF, hence more informative.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© üá∏üá¶ ŸÖÿ´ÿßŸÑ ÿπŸÖŸÑŸä ŸÖÿ®ÿ≥Ÿëÿ∑\n",
    "\n",
    "ÿßŸÅÿ™ÿ±ÿ∂ ŸÉŸÑŸÖÿ© **\"Trump\"** ÿ∏Ÿáÿ±ÿ™ 50 ŸÖÿ±ÿ© ŸÅŸä ŸÖŸÇÿßŸÑ Ÿäÿ≠ÿ™ŸàŸä 1000 ŸÉŸÑŸÖÿ©\n",
    "Ÿàÿ∞ŸÉÿ±ÿ™ ŸÅŸä 10,000 ŸÖŸÜ ÿ£ÿµŸÑ ŸÖŸÑŸäŸàŸÜ ŸÖŸÇÿßŸÑ.\n",
    "\n",
    "ÿ•ÿ∞ŸÜ:\n",
    "\n",
    "* TF = 50 √∑ 1000 = 0.05\n",
    "* IDF = log(1,000,000 √∑ 10,000) = log(100) = 2\n",
    "  ‚Üí TF-IDF = 0.05 √ó 2 = **0.1**\n",
    "\n",
    "ÿ™Ÿèÿ∏Ÿáÿ± Ÿáÿ∞Ÿá ÿßŸÑŸÇŸäŸÖÿ© ÿ£ŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ŸÖŸÖŸäÿ≤ÿ© ŸÜÿ≥ÿ®ŸäŸãÿß ÿØÿßÿÆŸÑ ÿßŸÑŸÖŸÇÿßŸÑ.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Example\n",
    "\n",
    "If the word ‚ÄúTrump‚Äù appears 50 times in a 1000-word article\n",
    "and in 10,000 out of 1,000,000 articles overall:\n",
    "\n",
    "* TF = 0.05\n",
    "* IDF = log(100) = 2\n",
    "* TF-IDF = 0.1\n",
    "\n",
    "The higher the score, the more representative the word is for that document.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è üá∏üá¶ ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖÿßÿ™ ÿßŸÑÿπŸÖŸÑŸäÿ©\n",
    "\n",
    "* ÿ™ŸÇŸäŸäŸÖ ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµŸàÿµ.\n",
    "* ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©.\n",
    "* ÿ™ÿ±ÿ™Ÿäÿ® ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ®ÿ≠ÿ´ (Search Ranking).\n",
    "* ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ¥ÿßÿπÿ± Ÿàÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿµŸàÿµ.\n",
    "* ÿ™ÿ¨ŸáŸäÿ≤ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑÿ©.\n",
    "\n",
    "ŸàŸÅŸä 2025ÿå ŸÑÿß ÿ™ÿ≤ÿßŸÑ **TF-IDF** ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ÿπŸÑŸâ ŸÜÿ∑ÿßŸÇ Ÿàÿßÿ≥ÿπ ŸÑÿ£ŸÜŸáÿß **ÿ®ÿ≥Ÿäÿ∑ÿ© Ÿàÿ≥ÿ±Ÿäÿπÿ© ŸàŸÅÿπÿßŸÑÿ©**ÿå\n",
    "ÿÆÿµŸàÿµŸãÿß ŸÖÿπ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ≠Ÿäÿ´ **ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ£ŸÉÿ´ÿ± ÿ£ŸáŸÖŸäÿ© ŸÖŸÜ ŸÖÿπŸÜÿßŸáÿß** ÿ£ÿ≠ŸäÿßŸÜŸãÿß.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Real-World Uses (as of 2025)\n",
    "\n",
    "* Keyword extraction\n",
    "* Search engine ranking\n",
    "* Sentiment analysis\n",
    "* Document classification\n",
    "* Feature extraction for ML models\n",
    "\n",
    "In 2025, TF-IDF remains popular for **Arabic NLP**,\n",
    "since it values **word occurrence statistics** over deep semantics ‚Äî\n",
    "a great match for tasks like **NER** and **POS tagging** foundations.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© üá∏üá¶ ÿßŸÑÿπŸÑÿßŸÇÿ© ŸÖÿπ ŸÜŸÖÿßÿ∞ÿ¨ 2025 ÿßŸÑÿ≠ÿØŸäÿ´ÿ©\n",
    "\n",
    "ŸÅŸä ÿßŸÑÿ≥ŸÜŸàÿßÿ™ ÿßŸÑÿ£ÿÆŸäÿ±ÿ©ÿå ŸÑŸÖ ÿ™ÿÆÿ™ŸÅŸê TF-IDF ÿ®ŸÑ ÿ™ŸÖ **ÿØŸÖÿ¨Ÿáÿß** ŸÖÿπ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿ£ÿπŸÖŸÇ ŸÖÿ´ŸÑ **AraBERT** Ÿà**FastText**\n",
    "ŸÑÿ•ŸÜÿ¥ÿßÿ° **ŸÜŸÖŸàÿ∞ÿ¨ Ÿáÿ¨ŸäŸÜ Hybrid Model** Ÿäÿ¨ŸÖÿπ ÿ®ŸäŸÜ:\n",
    "\n",
    "* ŸÇŸàÿ© TF-IDF ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ©\n",
    "* ŸàŸÅŸáŸÖ ÿßŸÑŸÄ Embeddings ÿßŸÑÿØŸÑÿßŸÑŸä.\n",
    "\n",
    "ŸàŸáÿ∞ÿß ŸÖŸÅŸäÿØ ÿ¨ÿØŸãÿß ŸÅŸä ÿßŸÑŸÖÿ¨ÿßŸÑÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÜÿµŸàÿµ ŸÖÿÆÿ™ŸÑÿ∑ÿ© ÿ®ŸäŸÜ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ŸàÿßŸÑÿπÿßŸÖŸäÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß TF-IDF in Modern (2025) Systems\n",
    "\n",
    "In recent years, TF-IDF hasn‚Äôt disappeared ‚Äî it evolved.\n",
    "It‚Äôs now combined with **semantic embeddings** (like AraBERT or FastText)\n",
    "to create **hybrid systems** that blend statistical precision with contextual understanding.\n",
    "\n",
    "This is especially effective for **Arabic NLP**,\n",
    "where meaning often depends on context and morphology.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö üá∏üá¶ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©\n",
    "\n",
    "[\n",
    "TF\\text{-}IDF(w, d) = TF(w, d) \\times \\log\\left(\\frac{N}{df(w)}\\right)\n",
    "]\n",
    "\n",
    "ŸÉŸÑŸÖÿß:\n",
    "\n",
    "* ÿ≤ÿßÿØ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ\n",
    "* ŸàŸÇŸÑŸë ÿßŸÜÿ™ÿ¥ÿßÿ±Ÿáÿß ŸÅŸä ÿ®ŸÇŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™\n",
    "  ‚¨ÖÔ∏è ÿ≤ÿßÿØ Ÿàÿ≤ŸÜŸáÿß Ÿàÿ£ŸáŸÖŸäÿ™Ÿáÿß.\n",
    "\n",
    "---\n",
    "\n",
    "## üá¨üáß Final Formula\n",
    "\n",
    "[\n",
    "TF\\text{-}IDF(w, d) = TF(w, d) \\times \\log\\left(\\frac{N}{df(w)}\\right)\n",
    "]\n",
    "\n",
    "A word is more important when:\n",
    "\n",
    "* It appears frequently in one document\n",
    "* But rarely across others.\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ üá∏üá¶ ÿÆŸÑÿßÿµÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© | üá¨üáß Quick Summary\n",
    "\n",
    "| üá∏üá¶                                       | üá¨üáß                                                   |\n",
    "| ------------------------------------------ | ------------------------------------------------------ |\n",
    "| TF-IDF ÿ™ÿ∑ŸàŸäÿ± ÿ∞ŸÉŸä ŸÑŸÅŸÉÿ±ÿ© BoW                 | TF-IDF is an advanced form of BoW                      |\n",
    "| Ÿäÿπÿ∑Ÿä Ÿàÿ≤ŸÜŸãÿß ŸÑŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ© ŸàÿßŸÑŸÖŸÖŸäÿ≤ÿ©        | Gives higher weight to rare, informative words         |\n",
    "| ŸäŸÇŸÑŸÑ ŸÖŸÜ ÿ£ÿ´ÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¥ÿßÿ¶ÿπÿ©                | Reduces influence of frequent, generic words           |\n",
    "| ŸÖÿß ÿ≤ÿßŸÑ ŸÅÿπÿßŸÑŸãÿß ÿ¨ÿØŸãÿß ÿÆÿµŸàÿµŸãÿß ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© | Still effective ‚Äî especially for Arabic text           |\n",
    "| ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÉÿ´Ÿäÿ±Ÿãÿß ŸÇÿ®ŸÑ ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ŸÅŸä ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿßÿ™ ML | Often used for ML preprocessing and feature extraction |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e751a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ŸÖŸÇÿØŸÖÿ© ÿπŸÖŸÑŸäÿ© | üá¨üáß Practical Introduction\n",
    "\n",
    "ÿßŸÑÿ¢ŸÜ ÿ≥ŸÜŸÇŸàŸÖ ÿ®ÿ™ÿ∑ÿ®ŸäŸÇ ÿ∑ÿ±ŸäŸÇÿ© **TF-IDF** ŸäÿØŸàŸäŸãÿß ÿ£ŸàŸÑŸãÿß ŸÑŸÅŸáŸÖ ÿßŸÑŸÖÿ®ÿØÿ£ ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿå\n",
    "ÿ´ŸÖ ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸÖŸÉÿ™ÿ®ÿ© **Scikit-learn** ŸÑÿ™ÿ∑ÿ®ŸäŸÇŸáÿß ŸÖÿ®ÿßÿ¥ÿ±ÿ©.\n",
    "ÿßŸÑŸáÿØŸÅ ŸáŸà ŸÅŸáŸÖ ŸÉŸäŸÅ Ÿäÿ™ŸÖ ÿ≠ÿ≥ÿßÿ®:\n",
    "\n",
    "* **TF** (ÿπÿØÿØ ÿ™ŸÉÿ±ÿßÿ±ÿßÿ™ ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÜÿµ)\n",
    "* **IDF** (ŸÖÿØŸâ ŸÜÿØÿ±ÿ© ÿßŸÑŸÉŸÑŸÖÿ© ÿπÿ®ÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™)\n",
    "* ÿ´ŸÖ **TF √ó IDF** (ÿßŸÑÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© ŸÑŸÑŸÉŸÑŸÖÿ©)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "934f82f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'man', 'went', 'out', 'for', 'a', 'walk']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üèÅ Example Documents\n",
    "# üá¨üáß Two short documents for comparison\n",
    "# üá∏üá¶ ŸÖÿ´ÿßŸÑ ÿ®ÿ≥Ÿäÿ∑ ÿπŸÑŸâ ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÑÿ™ÿ≠ŸÑŸäŸÑŸáÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ TF-IDF\n",
    "documentA = 'the man went out for a walk'\n",
    "documentB = 'the children sat around the fire'\n",
    "\n",
    "# Split each document into words (tokenization)\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "bagOfWordsA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69f9f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ£ŸàŸÑŸâ: ÿ•ŸÜÿ¥ÿßÿ° ŸÇÿßÿ¶ŸÖÿ© ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÅÿ±ŸäÿØÿ©\n",
    "\n",
    "### üá¨üáß Step 1: Create the unique vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebfd7d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'around',\n",
       " 'children',\n",
       " 'fire',\n",
       " 'for',\n",
       " 'man',\n",
       " 'out',\n",
       " 'sat',\n",
       " 'the',\n",
       " 'walk',\n",
       " 'went'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "uniqueWords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30d5e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ©: ÿ≠ÿ≥ÿßÿ® ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿ∏ŸáŸàÿ± ŸÉŸÑ ŸÉŸÑŸÖÿ©\n",
    "\n",
    "### üá¨üáß Step 2: Count occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2de43176",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bac01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ´ÿßŸÑÿ´ÿ©: ÿ≠ÿ≥ÿßÿ® TF\n",
    "\n",
    "**TF = ÿπÿØÿØ ŸÖÿ±ÿßÿ™ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© √∑ ÿπÿØÿØ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÉŸÑŸä ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ**\n",
    "\n",
    "### üá¨üáß Step 3: Compute Term Frequency (TF)\n",
    "\n",
    "**TF = (count of a word in the document) / (total number of words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98c04fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e526f2f",
   "metadata": {},
   "source": [
    "üìò **ŸÖŸÑÿßÿ≠ÿ∏ÿ©:** TF ŸäŸèÿ∏Ÿáÿ± ŸÖÿØŸâ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÜÿµÿå\n",
    "ŸÑŸÉŸÜ ŸÑÿß Ÿäÿ£ÿÆÿ∞ ŸÅŸä ÿßŸÑÿßÿπÿ™ÿ®ÿßÿ± ŸÖÿØŸâ ÿ¥ŸäŸàÿπŸáÿß ŸÅŸä ÿ®ŸÇŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™.\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿ±ÿßÿ®ÿπÿ©: ÿ≠ÿ≥ÿßÿ® IDF\n",
    "\n",
    "**IDF = log(ÿπÿØÿØ ŸÉŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ √∑ ÿπÿØÿØ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸä ÿßŸÑŸÉŸÑŸÖÿ©)**\n",
    "\n",
    "### üá¨üáß Step 4: Compute Inverse Document Frequency (IDF)\n",
    "\n",
    "**IDF = log(N / df)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f14e58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.0,\n",
       " 'fire': 0.6931471805599453,\n",
       " 'around': 0.6931471805599453,\n",
       " 'sat': 0.6931471805599453,\n",
       " 'children': 0.6931471805599453,\n",
       " 'man': 0.6931471805599453,\n",
       " 'went': 0.6931471805599453,\n",
       " 'out': 0.6931471805599453,\n",
       " 'walk': 0.6931471805599453,\n",
       " 'a': 0.6931471805599453,\n",
       " 'for': 0.6931471805599453}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeIDF(documents):\n",
    "    N = len(documents)  # total number of documents\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "idfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30ff24",
   "metadata": {},
   "source": [
    "üìò **ŸÖŸÑÿßÿ≠ÿ∏ÿ© ŸÖŸáŸÖÿ©:**\n",
    "\n",
    "ŸÉŸÑŸÖÿß ÿ∏Ÿáÿ±ÿ™ ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸä ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÜÿµŸàÿµÿå ŸÇŸÑŸëÿ™ ŸÇŸäŸÖÿ™Ÿáÿß.\n",
    "ŸàŸáÿ∞ÿß ŸÖÿß Ÿäÿ¨ÿπŸÑ TF-IDF ÿ£ŸÉÿ´ÿ± ÿ∞ŸÉÿßÿ°Ÿã ŸÖŸÜ BoW.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5a9bf",
   "metadata": {},
   "source": [
    "### üá∏üá¶ ÿßŸÑÿÆÿ∑Ÿàÿ© ÿßŸÑÿÆÿßŸÖÿ≥ÿ©: ÿ≠ÿ≥ÿßÿ® TF √ó IDF\n",
    "\n",
    "### üá¨üáß Step 5: Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "775df473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>fire</th>\n",
       "      <th>around</th>\n",
       "      <th>sat</th>\n",
       "      <th>children</th>\n",
       "      <th>man</th>\n",
       "      <th>went</th>\n",
       "      <th>out</th>\n",
       "      <th>walk</th>\n",
       "      <th>a</th>\n",
       "      <th>for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the      fire    around       sat  children       man      went       out  \\\n",
       "0  0.0  0.000000  0.000000  0.000000  0.000000  0.099021  0.099021  0.099021   \n",
       "1  0.0  0.115525  0.115525  0.115525  0.115525  0.000000  0.000000  0.000000   \n",
       "\n",
       "       walk         a       for  \n",
       "0  0.099021  0.099021  0.099021  \n",
       "1  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0d78c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è üá∏üá¶ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖŸÉÿ™ÿ®ÿ© sklearn\n",
    "\n",
    "### üá¨üáß Using Scikit-learn‚Äôs Built-in TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c342c633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>around</th>\n",
       "      <th>children</th>\n",
       "      <th>fire</th>\n",
       "      <th>for</th>\n",
       "      <th>man</th>\n",
       "      <th>out</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "      <th>walk</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303216</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.407401</td>\n",
       "      <td>0.407401</td>\n",
       "      <td>0.407401</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.407401</td>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     around  children      fire      for      man      out       sat  \\\n",
       "0  0.000000  0.000000  0.000000  0.42616  0.42616  0.42616  0.000000   \n",
       "1  0.407401  0.407401  0.407401  0.00000  0.00000  0.00000  0.407401   \n",
       "\n",
       "        the     walk     went  \n",
       "0  0.303216  0.42616  0.42616  \n",
       "1  0.579739  0.00000  0.00000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "df = pd.DataFrame(dense, columns=feature_names)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88174f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò üá∏üá¶ ŸÖÿ´ÿßŸÑ ÿ¢ÿÆÿ± ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© | üá¨üáß Arabic Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc16e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ÿßŸÑŸä</th>\n",
       "      <th>ÿßŸÑÿ¨ÿßŸÖÿπÿ©</th>\n",
       "      <th>ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™</th>\n",
       "      <th>ŸÖÿ≠ŸÖÿØ</th>\n",
       "      <th>ŸÖŸÜŸä</th>\n",
       "      <th>ÿ∞Ÿáÿ®</th>\n",
       "      <th>ŸÅŸä</th>\n",
       "      <th>ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°</th>\n",
       "      <th>Ÿà</th>\n",
       "      <th>ÿßŸÑŸÉŸäŸÖŸäÿßÿ°</th>\n",
       "      <th>ŸÑŸäÿØÿ±ÿ≥</th>\n",
       "      <th>ÿ∞ÿßŸÉÿ±ÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ÿßŸÑŸä  ÿßŸÑÿ¨ÿßŸÖÿπÿ©  ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™      ŸÖÿ≠ŸÖÿØ       ŸÖŸÜŸä       ÿ∞Ÿáÿ®        ŸÅŸä  \\\n",
       "0  0.086643      0.0   0.000000  0.086643  0.000000  0.086643  0.000000   \n",
       "1  0.000000      0.0   0.099021  0.000000  0.099021  0.000000  0.099021   \n",
       "\n",
       "   ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°    Ÿà  ÿßŸÑŸÉŸäŸÖŸäÿßÿ°     ŸÑŸäÿØÿ±ÿ≥     ÿ∞ÿßŸÉÿ±ÿ™  \n",
       "0       0.0  0.0  0.086643  0.086643  0.000000  \n",
       "1       0.0  0.0  0.000000  0.000000  0.099021  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentA = 'ÿ∞Ÿáÿ® ŸÖÿ≠ŸÖÿØ ÿßŸÑŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸÑŸäÿØÿ±ÿ≥ ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ° Ÿà ÿßŸÑŸÉŸäŸÖŸäÿßÿ°'\n",
    "documentB = 'ÿ∞ÿßŸÉÿ±ÿ™ ŸÖŸÜŸä ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™ Ÿà ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ° ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©'\n",
    "\n",
    "# Tokenize (split words)\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "\n",
    "# Create vocabulary\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "\n",
    "# Count word frequencies\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA: numOfWordsA[word] += 1\n",
    "for word in bagOfWordsB: numOfWordsB[word] += 1\n",
    "\n",
    "# Compute TF\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "\n",
    "# Compute IDF\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "pd.DataFrame([tfidfA, tfidfB])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1bd2b",
   "metadata": {},
   "source": [
    "üìô **ŸÖŸÑÿßÿ≠ÿ∏ÿ© ŸÖŸáŸÖÿ©:**\n",
    "TF-IDF ÿ™ÿπŸÖŸÑ ÿ®ŸÉŸÅÿßÿ°ÿ© ÿπÿßŸÑŸäÿ© ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå\n",
    "ŸÑÿ£ŸÜŸáÿß ÿ™Ÿáÿ™ŸÖ **ÿ®ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ŸàŸÑŸäÿ≥ ŸÖÿπŸÜÿßŸáÿß**ÿå\n",
    "ŸàŸáÿ∞ÿß Ÿäÿ¨ÿπŸÑŸáÿß ŸÖŸÅŸäÿØÿ© ÿ¨ÿØŸãÿß ŸÅŸä ŸÖŸáÿßŸÖ ŸÖÿ´ŸÑ:\n",
    "\n",
    "* **ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Text Mining)**\n",
    "* **ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÖŸÇÿßŸÑÿßÿ™**\n",
    "* **ÿßŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸáŸÖÿ© (Keyword Extraction)**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b02ab",
   "metadata": {},
   "source": [
    "## üöÄ üá∏üá¶ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Scikit-learn ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\n",
    "\n",
    "### üá¨üáß Arabic Text TF-IDF using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6fb6faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ÿßŸÑÿ¨ÿßŸÖÿπÿ©</th>\n",
       "      <th>ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™</th>\n",
       "      <th>ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°</th>\n",
       "      <th>ÿßŸÑŸÉŸäŸÖŸäÿßÿ°</th>\n",
       "      <th>ÿßŸÑŸä</th>\n",
       "      <th>ÿ∞ÿßŸÉÿ±ÿ™</th>\n",
       "      <th>ÿ∞Ÿáÿ®</th>\n",
       "      <th>ŸÅŸä</th>\n",
       "      <th>ŸÑŸäÿØÿ±ÿ≥</th>\n",
       "      <th>ŸÖÿ≠ŸÖÿØ</th>\n",
       "      <th>ŸÖŸÜŸä</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.29017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.29017</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.407824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.31780</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.31780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ÿßŸÑÿ¨ÿßŸÖÿπÿ©  ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™  ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°  ÿßŸÑŸÉŸäŸÖŸäÿßÿ°       ÿßŸÑŸä     ÿ∞ÿßŸÉÿ±ÿ™       ÿ∞Ÿáÿ®  \\\n",
       "0  0.29017   0.000000   0.29017  0.407824  0.407824  0.000000  0.407824   \n",
       "1  0.31780   0.446656   0.31780  0.000000  0.000000  0.446656  0.000000   \n",
       "\n",
       "         ŸÅŸä     ŸÑŸäÿØÿ±ÿ≥      ŸÖÿ≠ŸÖÿØ       ŸÖŸÜŸä  \n",
       "0  0.000000  0.407824  0.407824  0.000000  \n",
       "1  0.446656  0.000000  0.000000  0.446656  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "pd.DataFrame(dense, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56eee4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ŸÖŸáŸÖÿ© (ÿ™ÿ≠ÿØŸäÿ´ 2025) | üá¨üáß Key Notes (as of 2025)\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑŸÖŸÑÿßÿ≠ÿ∏ÿ©                                    | üá¨üáß Note                                                  |\n",
    "| ------------------------------------------------ | ---------------------------------------------------------- |\n",
    "| TF-IDF ŸÑÿß ŸäŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ÿ£Ÿà ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®                 | TF-IDF doesn‚Äôt capture semantic meaning or order           |\n",
    "| ŸÑŸÉŸÜŸá ŸÖŸÖÿ™ÿßÿ≤ ŸÅŸä ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸáŸÖÿ©             | Great for identifying significant terms                    |\n",
    "| ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÉÿ£ÿ≥ÿßÿ≥ ŸÇÿ®ŸÑ ŸÜŸÖÿßÿ∞ÿ¨ ÿ£ÿπŸÖŸÇ ŸÖÿ´ŸÑ BERT            | Often used as preprocessing before deep models             |\n",
    "| ŸäÿØÿπŸÖ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ®ÿØŸÇÿ© ÿπÿßŸÑŸäÿ© ÿÆÿµŸàÿµŸãÿß ŸÅŸä ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© | Works very well with Modern Standard Arabic                |\n",
    "| ŸÅŸä 2025 ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿØŸÖŸàÿ¨Ÿãÿß ŸÖÿπ Word Embeddings       | In 2025, often combined with embeddings for hybrid systems |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e30fa3",
   "metadata": {},
   "source": [
    "---\n",
    "**ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ®ÿ≠ÿ´ÿ™ ÿπŸÜŸáÿß ŸÑÿ≤ŸäÿßÿØÿ© ÿßŸÑŸÅŸáŸÖ**\n",
    "\n",
    "\n",
    "### üß± ÿ£ŸàŸÑŸãÿß: ŸÑŸäÿ¥ TF-IDF ŸÖÿß ŸäŸáÿ™ŸÖ ÿ®ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®ÿü\n",
    "\n",
    "ŸÑÿ£ŸÜ ŸÅŸÉÿ±ÿ™Ÿá ÿßŸÑÿ£ÿµŸÑŸäÿ© **ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ© ÿ®ÿ≠ÿ™ÿ©**ÿå\n",
    "ŸäÿπŸÜŸä ŸÖÿß ŸäŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâÿå ŸáŸà ÿ®ÿ≥ **ŸäÿπÿØŸë ÿßŸÑŸÉŸÑŸÖÿßÿ™** ŸÉŸÖ ŸÖÿ±ÿ© ÿ∏Ÿáÿ±ÿ™.\n",
    "\n",
    "> ŸáŸà Ÿäÿ¥ŸàŸÅ ÿßŸÑŸÜÿµ ŸÖÿ´ŸÑ ‚ÄúŸÉŸäÿ≥ ŸÉŸÑŸÖÿßÿ™‚Äù (Bag of Words)ÿå\n",
    "> ŸÖÿß ŸäŸáŸÖŸá ŸàŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ÿ¨ÿßÿ™ÿå ÿßŸÑŸÖŸáŸÖ ŸÉŸÖ ŸÖÿ±ÿ© ÿ¨ÿßÿ™.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ‚ÄúÿßŸÑŸàŸÑÿØ ÿ£ŸÉŸÑ ÿßŸÑÿ™ŸÅÿßÿ≠ÿ©‚Äù\n",
    "> ‚ÄúÿßŸÑÿ™ŸÅÿßÿ≠ÿ© ÿ£ŸÉŸÑ ÿßŸÑŸàŸÑÿØ‚Äù\n",
    "\n",
    "ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÄ TF-IDF:\n",
    "ŸÜŸÅÿ≥ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ‚Üí ŸÜŸÅÿ≥ ÿßŸÑŸÇŸäŸÖ ‚Üí ŸÜŸÅÿ≥ ÿßŸÑÿ™ŸÖÿ´ŸäŸÑÿå\n",
    "ŸÖÿπ ÿ•ŸÜŸá ÿßŸÑŸÖÿπŸÜŸâ ŸÖÿÆÿ™ŸÑŸÅ ÿ™ŸÖÿßŸÖŸãÿß.\n",
    "\n",
    "ÿßŸÑÿ≥ÿ®ÿ®ÿü ŸÑÿ£ŸÜŸá ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© **ŸÖÿß ÿµŸèŸÖŸÖ ŸÑŸÅŸáŸÖ ÿßŸÑŸÑÿ∫ÿ©**ÿå\n",
    "ÿ®ŸÑ ŸÅŸÇÿ∑ ŸÑÿ≠ÿ≥ÿßÿ® **ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÉŸÑŸÖÿßÿ™** ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµŸàÿµ.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† ÿ´ÿßŸÜŸäŸãÿß: ŸÑŸäŸá ÿßŸÑÿπŸÑŸÖÿßÿ° ŸÖÿß ÿÆŸÑŸàŸá ŸäŸáÿ™ŸÖ ÿ®ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®ÿü\n",
    "\n",
    "ŸÑÿ£ŸÜ ŸáÿØŸÅŸáŸÖ ŸàŸÇÿ™Ÿáÿß (ŸÇÿ®ŸÑ ÿ∏ŸáŸàÿ± ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿπŸÖŸäŸÇ)\n",
    "ŸÉÿßŸÜ ÿ®ÿ≥ **ÿ™ŸÖŸäŸäÿ≤ ÿßŸÑŸÜÿµŸàÿµ Ÿàÿ™ÿµŸÜŸäŸÅŸáÿß ÿ®ÿ≥ÿ±ÿπÿ©**ÿå\n",
    "ŸÖŸà ŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑŸÉÿßŸÖŸÑ.\n",
    "ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® Ÿäÿ≠ÿ™ÿßÿ¨ ŸÜŸÖÿßÿ∞ÿ¨ ÿ£ÿπŸÇÿØ ÿ®ŸÉÿ´ŸäŸäÿ± (ŸÖÿ´ŸÑ ÿßŸÑÿ¥ÿ®ŸÉÿßÿ™ ÿßŸÑÿπÿµÿ®Ÿäÿ©)ÿå\n",
    "ŸàŸáÿ∞ÿß ŸÉÿßŸÜ ÿµÿπÿ® ÿ£Ÿà ŸÖŸÉŸÑŸÅ ŸàŸÇÿ™Ÿáÿß.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ ÿ´ÿßŸÑÿ´Ÿãÿß: ŸáŸÑ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ¨ÿØŸäÿØÿ© ÿ™Ÿáÿ™ŸÖ ÿ®ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®ÿü\n",
    "\n",
    "ŸÜÿπŸÖ ‚úÖ\n",
    "ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÖÿ´ŸÑ:\n",
    "\n",
    "> **Word2Vec, BERT, AraBERT, GPT‚Ä¶**\n",
    "\n",
    "ÿ™Ÿáÿ™ŸÖ **ÿ®ÿßŸÑÿ≥ŸäÿßŸÇ ŸàÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ŸàÿßŸÑŸÖÿπŸÜŸâ**.\n",
    "ŸäÿπŸÜŸä ÿ™ÿπÿ±ŸÅ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ:\n",
    "\n",
    "> ‚Äúÿ£ŸÉŸÑ ÿßŸÑŸàŸÑÿØ ÿßŸÑÿ™ŸÅÿßÿ≠ÿ©‚Äù üçé\n",
    "> ‚ÄúÿßŸÑÿ™ŸÅÿßÿ≠ÿ© ÿ£ŸÉŸÑÿ™ ÿßŸÑŸàŸÑÿØ‚Äù üò®\n",
    "\n",
    "ŸÑÿ£ŸÜŸáÿß ÿ™ŸÅŸáŸÖ ÿ£ŸÜ ÿßŸÑŸÖÿπŸÜŸâ ÿ™ÿ∫ŸäŸëÿ± ÿ≠ÿ≥ÿ® ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÉŸÑŸÖÿßÿ™.\n",
    "\n",
    "---\n",
    "\n",
    "üìç **ÿßŸÑÿÆŸÑÿßÿµÿ©:**\n",
    "\n",
    "* **TF-IDF:** ŸäÿπÿØŸë ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸÇÿ∑ ‚Üí ŸÖÿß ŸäŸáŸÖŸá ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®.\n",
    "* **ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ©:** ÿ™ŸÅŸáŸÖ ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® ŸàÿßŸÑŸÖÿπŸÜŸâ ŸàÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### üí° **TF-IDF ŸÇŸàŸä ŸÑŸÉŸÜŸá ŸÖÿ≠ÿØŸàÿØ**\n",
    "\n",
    "ŸÜÿπŸÖÿå ÿ™ŸÇÿØÿ± ÿ™ÿ®ŸÜŸä ÿπŸÑŸäŸá ÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ Ÿàÿ™ÿØÿ±ÿ® ŸÜŸÖÿßÿ∞ÿ¨ÿå\n",
    "ŸÑŸÉŸÜ **ÿØŸÇÿ™Ÿá ŸÖÿ≠ÿØŸàÿØÿ©** ŸÅŸä ŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ.\n",
    "\n",
    "ÿÆŸÑŸÜŸä ÿ£Ÿàÿ∂ÿ≠Ÿáÿß ÿ®ÿ®ÿ≥ÿßÿ∑ÿ©:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ ŸÖÿ™Ÿâ TF-IDF ŸÖŸÖÿ™ÿßÿ≤\n",
    "\n",
    "* ŸÑŸÖÿß ÿßŸÑŸáÿØŸÅ **ÿ™ŸÖŸäŸäÿ≤ ÿ£Ÿà ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿµŸàÿµ** (ŸÖÿ´ŸÑ ŸÖÿπÿ±ŸÅÿ© ŸáŸÑ ÿßŸÑÿ™ÿ∫ÿ±ŸäÿØÿ© ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ© ÿ£Ÿà ÿ≥ŸÑÿ®Ÿäÿ©).\n",
    "* ŸÑŸÖÿß ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÉÿ´Ÿäÿ±ÿ© ŸàÿßŸÑŸÉŸÑŸÖÿßÿ™ Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÖŸÖŸäÿ≤ÿ©.\n",
    "* ŸÑŸÖÿß ŸÖÿß ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ™ŸÅŸáŸÖ **ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿØŸÇŸäŸÇ ÿ£Ÿà ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑÿ¨ŸÖŸÑ**.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ÿ™ÿ®Ÿä ÿ™ÿµŸÜŸÅ ŸÖÿ±ÿßÿ¨ÿπÿßÿ™ ŸÖŸÜÿ™ÿ¨ÿßÿ™ (ÿ¨ŸäÿØÿ© / ÿ≥Ÿäÿ¶ÿ©)\n",
    "> TF-IDF Ÿäÿπÿ∑ŸäŸÉ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ≠ŸÑŸàÿ© Ÿàÿ≥ÿ±Ÿäÿπÿ© ŸàŸÅÿπŸëÿßŸÑÿ© üí™\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è ŸÖÿ™Ÿâ ŸäÿµŸäÿ± ÿ∂ÿπŸäŸÅ\n",
    "\n",
    "* ŸÑŸÖÿß ÿßŸÑŸÜÿµŸàÿµ ŸÖÿπŸÇÿØÿ© ÿ£Ÿà ŸÅŸäŸáÿß **ŸÖÿπÿßŸÜŸä ÿ∂ŸÖŸÜŸäÿ© Ÿàÿ≥ŸäÿßŸÇ**.\n",
    "* ŸÑŸÖÿß ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ® Ÿäÿ∫ŸäŸëÿ± ÿßŸÑŸÖÿπŸÜŸâ (ŸÖÿ´ŸÑ ‚ÄúŸÖÿß ÿ£ÿ≠ÿ® ÿßŸÑŸÉÿ∞ÿ®‚Äù ‚â† ‚Äúÿ£ÿ≠ÿ® ÿßŸÑŸÉÿ∞ÿ®‚Äù).\n",
    "* ŸÑŸÖÿß ÿ™ÿ™ÿπÿßŸÖŸÑ ŸÖÿπ **ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ© ÿßŸÑŸÖÿ™ŸÜŸàÿπÿ©** (ŸÖÿ´ŸÑ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑÿπÿßŸÖŸäÿ© ÿ£Ÿà ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿ∑ŸàŸäŸÑÿ©).\n",
    "\n",
    "ŸÅŸä ŸáÿßŸÑÿ≠ÿßŸÑÿßÿ™ÿå TF-IDF ŸÖÿß ŸäŸÅŸáŸÖ ‚ÄúÿßŸÑŸÜŸäÿ©‚Äù ÿ£Ÿà ‚ÄúÿßŸÑÿ≥ŸäÿßŸÇ‚Äùÿå\n",
    "ŸÅŸÜÿ™ÿßÿ¶ÿ¨Ÿá ÿ™ŸÉŸàŸÜ ÿ£ŸÇŸÑ ÿØŸÇÿ© ŸÖŸÇÿßÿ±ŸÜÿ© ÿ®ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üß† ŸÖÿ´ÿßŸÑ ÿ®ÿ≥Ÿäÿ∑\n",
    "\n",
    "ÿπŸÜÿØŸÉ ÿ®ŸäÿßŸÜÿßÿ™ ŸÅŸäŸáÿß:\n",
    "\n",
    "| ÿßŸÑŸÜÿµ                      | ÿßŸÑÿ™ÿµŸÜŸäŸÅ |\n",
    "| ------------------------- | ------- |\n",
    "| Ÿáÿ∞ÿß ÿßŸÑŸÖÿ∑ÿπŸÖ ÿ±ÿßÿ¶ÿπ ÿ¨ÿØŸãÿß      | ÿ•Ÿäÿ¨ÿßÿ®Ÿä  |\n",
    "| ÿßŸÑÿ£ŸÉŸÑ ŸÑÿ∞Ÿäÿ∞ ŸàÿßŸÑÿÆÿØŸÖÿ© ŸÖŸÖÿ™ÿßÿ≤ÿ© | ÿ•Ÿäÿ¨ÿßÿ®Ÿä  |\n",
    "| ÿßŸÑÿ£ŸÉŸÑ ÿ≥Ÿäÿ¶ ÿ¨ÿØŸãÿß            | ÿ≥ŸÑÿ®Ÿä    |\n",
    "| ÿßŸÑÿÆÿØŸÖÿ© ÿ®ÿ∑Ÿäÿ¶ÿ©              | ÿ≥ŸÑÿ®Ÿä    |\n",
    "\n",
    "ŸÑŸÖÿß ŸÜÿ≠ŸàŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ **TF-IDF**ÿå\n",
    "ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸäŸÑÿßÿ≠ÿ∏ ÿ•ÿ≠ÿµÿßÿ¶ŸäŸãÿß ÿ£ŸÜ:\n",
    "\n",
    "* ‚Äúÿ±ÿßÿ¶ÿπ‚Äù Ÿà‚ÄúŸÖŸÖÿ™ÿßÿ≤‚Äù ÿ∫ÿßŸÑÿ®Ÿãÿß ÿ™ÿ∏Ÿáÿ± ŸÅŸä ŸÜÿµŸàÿµ ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ© ‚úÖ\n",
    "* ‚Äúÿ≥Ÿäÿ¶‚Äù Ÿà‚Äúÿ®ÿ∑Ÿäÿ¶ÿ©‚Äù ÿ™ÿ∏Ÿáÿ± ŸÅŸä ŸÜÿµŸàÿµ ÿ≥ŸÑÿ®Ÿäÿ© ‚ùå\n",
    "\n",
    "ŸÅÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ (ŸÖÿ´ŸÑ Logistic Regression ÿ£Ÿà SVM)\n",
    "Ÿäÿ™ÿπŸÑŸÖ ÿßŸÑÿπŸÑÿßŸÇÿ© ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ© ÿ®ŸäŸÜ **ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸàÿßŸÑÿ™ÿµŸÜŸäŸÅ**ÿå\n",
    "ÿ®ÿØŸàŸÜ ŸÖÿß ŸäŸÅŸáŸÖ ŸÖÿπŸÜŸâ ÿßŸÑŸÉŸÑŸÖÿ© ŸÅÿπŸÑŸäŸãÿß.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è ÿ•ÿ∞Ÿãÿß Ÿàÿ¥ ÿßŸÑÿπŸäÿ® ŸáŸÜÿßÿü\n",
    "\n",
    "ŸÑŸà ÿ¨ÿ™Ÿá ÿ¨ŸÖŸÑÿ© ŸÖÿ´ŸÑ:\n",
    "\n",
    "> ‚ÄúÿßŸÑŸÖÿ∑ÿπŸÖ ŸÉÿßŸÜ ÿ±ÿßÿ¶ÿπ ŸÇÿ®ŸÑ ÿ≥ŸÜÿ©ÿå ÿ®ÿ≥ ÿµÿßÿ± ÿ≥Ÿäÿ¶ ÿßŸÑÿ¢ŸÜ‚Äù\n",
    "> TF-IDF ŸÖŸÖŸÉŸÜ Ÿäÿ≠ÿ™ÿßÿ± üòÖ\n",
    "> ŸÑÿ£ŸÜ ŸÅŸäŸáÿß ‚Äúÿ±ÿßÿ¶ÿπ‚Äù (ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©) Ÿà‚Äúÿ≥Ÿäÿ¶‚Äù (ÿ≥ŸÑÿ®Ÿäÿ©)ÿå\n",
    "> ŸàŸáŸà ŸÖÿß Ÿäÿπÿ±ŸÅ ÿßŸÑÿ≥ŸäÿßŸÇ ŸàŸÑÿß ÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®ÿå\n",
    "> ŸÅŸÖÿß ŸäŸÇÿØÿ± ŸäŸÅŸáŸÖ ÿ£ŸÜ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿπÿßŸÖ ÿ≥ŸÑÿ®Ÿä.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© ÿ®ŸäŸÜŸÖÿß ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© (BERTÿå GPT...)\n",
    "\n",
    "Ÿáÿ∞Ÿä **ÿ™ŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ŸàÿßŸÑÿ≥ŸäÿßŸÇ**ÿå\n",
    "ŸÅÿ™ÿπÿ±ŸÅ ÿ£ŸÜ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ™ŸÇŸàŸÑ ÿ•ŸÜ ÿßŸÑÿ™ÿ¨ÿ±ÿ®ÿ© ÿ≥ÿßÿ°ÿ™ ŸÖÿπ ÿßŸÑŸàŸÇÿ™ÿå\n",
    "Ÿàÿ™ÿµŸÜŸÅŸáÿß ‚Äúÿ≥ŸÑÿ®Ÿäÿ©‚Äù ÿ®ÿ≥ŸáŸàŸÑÿ© ‚úÖ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b928cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† üá∏üá¶ ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ±ÿßÿ®ÿπ: Word2Vec (ÿ£Ÿà Vec2Word)\n",
    "\n",
    "## üá¨üáß Section 7: Word2Vec (Vec2Word)\n",
    "\n",
    "---\n",
    "\n",
    "### ü™Ñ üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿπÿßŸÖÿ©\n",
    "\n",
    "**Word2Vec** (ÿ£Ÿà **Vec2Word**) ŸáŸä ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© ŸÖÿ®ŸÜŸäÿ© ÿπŸÑŸâ **ÿ¥ÿ®ŸÉÿ© ÿπÿµÿ®Ÿäÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© ŸÖŸÉŸàŸÜÿ© ŸÖŸÜ ÿ∑ÿ®ŸÇÿ™ŸäŸÜ**.\n",
    "ŸáÿØŸÅŸáÿß ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ŸáŸà ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ•ŸÑŸâ **ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿ±ŸÇŸÖŸäÿ© (Vectors)** ÿ®ÿ≠Ÿäÿ´:\n",
    "\n",
    "* ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ ÿ™ŸÉŸàŸÜ ŸÇÿ±Ÿäÿ®ÿ© ÿ±Ÿäÿßÿ∂ŸäŸãÿß.\n",
    "* ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ÿ™ŸÉŸàŸÜ ÿ®ÿπŸäÿØÿ©.\n",
    "\n",
    "üë©‚Äçüíª ŸÖÿ´ŸÑŸãÿß ÿ®ÿØŸÑ ŸÖÿß ÿ™ŸÉŸàŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ‚Äúÿ≥ŸÑÿßŸÖ‚Äù ŸÖÿ¨ÿ±ÿØ ŸÜÿµÿå\n",
    "ÿ™ÿµÿ®ÿ≠ ŸÖÿ™ÿ¨ŸáŸãÿß ÿ±ŸÇŸÖŸäŸãÿß ŸÖÿ´ŸÑ:\n",
    "`ÿ≥ŸÑÿßŸÖ ‚Üí [0.52, 0.31, -0.22, ...]`\n",
    "ŸàŸáÿ∞ÿß Ÿäÿ≥ŸÖÿ≠ ŸÑŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ£ŸÜ ‚ÄúŸäŸÅŸáŸÖ‚Äù ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ŸÖÿ´ŸÑ:\n",
    "\n",
    "> ÿßŸÑÿ±ÿ¨ŸÑ - ÿµÿ®Ÿä ‚âà ÿßŸÑŸÖÿ±ÿ£ÿ© - ŸÅÿ™ÿßÿ©\n",
    "> king - man + woman ‚âà queen üëë\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß The General Idea\n",
    "\n",
    "**Word2Vec** (also called **Vec2Word**) is a **two-layer neural network** designed to transform words into **numerical representations (vectors)**.\n",
    "Its goal is to capture **semantic meaning** mathematically so that:\n",
    "\n",
    "* Words with similar meanings are **closer together** in vector space.\n",
    "* Words with different meanings are **farther apart**.\n",
    "\n",
    "üë©‚Äçüíª For example, instead of treating ‚Äúpeace‚Äù as just text, it becomes:\n",
    "`peace ‚Üí [0.52, 0.31, -0.22, ...]`\n",
    "This allows the model to understand analogies like:\n",
    "\n",
    "> man - boy ‚âà woman - girl\n",
    "> king - man + woman ‚âà queen üëë\n",
    "\n",
    "This process is called **semantic embedding** ‚Äî representing meaning as numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è üá∏üá¶ ŸÉŸäŸÅ ÿ™ÿ¥ÿ™ÿ∫ŸÑ ÿßŸÑŸÅŸÉÿ±ÿ©ÿü\n",
    "\n",
    "## üá¨üáß How It Works\n",
    "\n",
    "ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Ÿäÿ™ÿπŸÑŸÖ ŸÖŸÜ ÿÆŸÑÿßŸÑ ŸÅŸÉÿ±ÿ© ÿ®ÿ≥Ÿäÿ∑ÿ©:\n",
    "\n",
    "> ‚ÄúÿßŸÑŸÉŸÑŸÖÿ© ÿ™Ÿèÿπÿ±ŸÅ ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ≠ŸàŸÑŸáÿß.‚Äù\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© ‚ÄúÿßŸÑŸàŸÑÿØ ÿ∞Ÿáÿ® ÿ•ŸÑŸâ ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©‚Äùÿå\n",
    "ŸÉŸÑŸÖÿ© **ÿ∞Ÿáÿ®** ÿ™ÿ∏Ÿáÿ± ÿØÿßÿ¶ŸÖŸãÿß ÿ®ÿ¨ÿßŸÜÿ® ‚ÄúÿßŸÑŸàŸÑÿØ‚Äù Ÿà‚ÄúÿßŸÑŸÖÿØÿ±ÿ≥ÿ©‚Äù Ÿà‚Äúÿ•ŸÑŸâ‚Äùÿå\n",
    "ŸÅŸäÿ™ÿπŸÑŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ£ŸÜ ‚Äúÿ∞Ÿáÿ®‚Äù ÿ™ÿ±ÿ™ÿ®ÿ∑ ÿ®ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ≠ÿ±ŸÉŸä ÿ£Ÿà ÿßŸÑŸÅÿπŸÑ.\n",
    "\n",
    "---\n",
    "\n",
    "üìò Example:\n",
    "In the sentence ‚ÄúThe boy went to school,‚Äù\n",
    "the word **went** often appears near ‚Äúboy,‚Äù ‚Äúto,‚Äù and ‚Äúschool.‚Äù\n",
    "So the model learns that **went** is semantically related to motion or actions.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ üá∏üá¶ ÿ£ŸÜŸàÿßÿπ Word2Vec\n",
    "\n",
    "## üá¨üáß The Two Main Architectures\n",
    "\n",
    "Word2Vec Ÿäÿ™ŸÉŸàŸÜ ŸÖŸÜ ŸÜŸÖŸàÿ∞ÿ¨ŸäŸÜ ÿ£ÿ≥ÿßÿ≥ŸäŸäŸÜ:\n",
    "\n",
    "1. **CBOW ‚Äì Continuous Bag of Words**\n",
    "2. **Skip-Gram**\n",
    "\n",
    "---\n",
    "\n",
    "### üß© üá∏üá¶ ÿ£ŸàŸÑŸãÿß: ŸÜŸÖŸàÿ∞ÿ¨ CBOW\n",
    "\n",
    "**(Continuous Bag of Words)**\n",
    "\n",
    "#### üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ©\n",
    "\n",
    "ŸÜŸÖŸàÿ∞ÿ¨ **CBOW** Ÿäÿ™ÿπŸÑŸÖ ŸÖŸÜ ÿÆŸÑÿßŸÑ **ÿ™ŸàŸÇÿπ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖŸÅŸÇŸàÿØÿ©** ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßÿπÿ™ŸÖÿßÿØŸãÿß ÿπŸÑŸâ **ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≠Ÿäÿ∑ÿ© ÿ®Ÿáÿß**.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ‚ÄúÿßŸÑÿ∑ÿßŸÑÿ® ‚Ä¶ ÿ•ŸÑŸâ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸÉŸÑ ŸäŸàŸÖ.‚Äù\n",
    "> ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Ÿäÿ≠ÿßŸàŸÑ ÿ£ŸÜ Ÿäÿ™ŸÜÿ®ÿ£ ÿ®ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÜÿßŸÇÿµÿ© ‚ÄúŸäÿ∞Ÿáÿ®‚Äù ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ŸÇÿ®ŸÑŸáÿß Ÿàÿ®ÿπÿØŸáÿß.\n",
    "\n",
    "---\n",
    "\n",
    "#### üá¨üáß The Idea\n",
    "\n",
    "The **CBOW** model learns by **predicting a missing word** based on the **surrounding words** in a sentence.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "> ‚ÄúThe student ‚Ä¶ to the university every day.‚Äù\n",
    "> The model predicts the missing word ‚Äúgoes‚Äù from its context words.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ üá∏üá¶ ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿπŸÖŸÑ ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ\n",
    "\n",
    "### üá¨üáß Detailed Working Steps\n",
    "\n",
    "1. ŸÜÿÆÿ™ÿßÿ± **ŸÜÿßŸÅÿ∞ÿ© (window)** ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ≠ŸàŸÑ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅÿ© (ŸÖÿ´ŸÑ ŸÉŸÑŸÖÿ™ŸäŸÜ ŸÇÿ®ŸÑ Ÿàÿ®ÿπÿØ).\n",
    "2. ŸÜÿ≠ŸàŸÑ ŸÉŸÑ ŸÉŸÑŸÖÿ© ÿ•ŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑ ÿ±ŸÇŸÖŸä (embedding vector).\n",
    "3. ŸÜÿ≠ÿ≥ÿ® ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ŸÑŸáÿ∞Ÿá ÿßŸÑÿ™ŸÖÿ´ŸäŸÑÿßÿ™.\n",
    "4. ŸÜÿØÿÆŸÑŸá ŸÅŸä **ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿÆŸÅŸäÿ© (hidden layer)**.\n",
    "5. ŸÜÿ≥ÿ™ÿÆÿØŸÖ **Softmax** ŸÅŸä ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿ£ÿÆŸäÿ±ÿ© ŸÑÿ™ŸàŸÇÿπ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖŸÅŸÇŸàÿØÿ©.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ‚Äúthe ___ sat on the mat‚Äù\n",
    "> ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≠Ÿäÿ∑ÿ© ‚Üí {the, sat, on, the, mat}\n",
    "> ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Ÿäÿ™ÿπŸÑŸÖ ÿ£ŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖŸÅŸÇŸàÿØÿ© ÿ∫ÿßŸÑÿ®Ÿãÿß ‚Äúcat‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "üìò Example (English):\n",
    "\n",
    "1. Choose a **window** of words around the target word (e.g., two before and two after).\n",
    "2. Convert each into a numerical embedding.\n",
    "3. Compute the **average** of those embeddings.\n",
    "4. Pass it through a **hidden layer**.\n",
    "5. Use **Softmax** to predict the missing word.\n",
    "\n",
    "> ‚Äúthe ___ sat on the mat‚Äù\n",
    "> Context words ‚Üí {the, sat, on, the, mat}\n",
    "> The model learns that the missing word is probably ‚Äúcat.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üí° üá∏üá¶ ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖÿßÿ™ ÿßŸÑŸàÿßŸÇÿπŸäÿ©\n",
    "\n",
    "### üá¨üáß Real-World Applications\n",
    "\n",
    "ÿ£ÿ≠ÿØ ÿ£ÿ¥Ÿáÿ± ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ŸáŸà:\n",
    "\n",
    "> **Spinning Articles** üåÄ\n",
    "> ŸàŸáŸà ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿßŸÑŸÜÿµŸàÿµ ÿ™ŸÑŸÇÿßÿ¶ŸäŸãÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÉŸÑŸÖÿßÿ™ ŸÖÿ±ÿßÿØŸÅÿ© ŸÑŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ŸÜŸÅÿ≥ ÿßŸÑŸÖÿπŸÜŸâ.\n",
    "\n",
    "üîπ ÿßŸÑÿ∑ÿ±ŸäŸÇÿ©:\n",
    "\n",
    "1. ŸÜÿ≠ÿ∞ŸÅ ÿ®ÿπÿ∂ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÖŸÜ ÿßŸÑŸÜÿµ.\n",
    "2. CBOW Ÿäÿ™ŸÜÿ®ÿ£ ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "3. ÿßŸÑŸÜÿßÿ™ÿ¨: ŸÜÿµ ÿ¨ÿØŸäÿØ ÿ®ŸÜŸÅÿ≥ ÿßŸÑŸÖÿπŸÜŸâ ŸàŸÑŸÉŸÜ ÿ®ÿµŸäÿßÿ∫ÿ© ŸÖÿÆÿ™ŸÑŸÅÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "One popular application is:\n",
    "\n",
    "> **Article Spinning** üåÄ\n",
    "> It rewrites existing text automatically by replacing words with synonyms while preserving meaning.\n",
    "\n",
    "üîπ Process:\n",
    "\n",
    "1. Some words are removed from the text.\n",
    "2. CBOW predicts suitable replacements from context.\n",
    "3. The result is a rephrased text with the same meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† üá∏üá¶ ÿ´ÿßŸÜŸäŸãÿß: ŸÜŸÖŸàÿ∞ÿ¨ Skip-Gram\n",
    "\n",
    "### üá¨üáß Second: Skip-Gram Model\n",
    "\n",
    "#### üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ©\n",
    "\n",
    "ŸáŸà **ÿπŸÉÿ≥ CBOW** ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß.\n",
    "ÿ®ÿØŸÑ ŸÖÿß Ÿäÿ™ŸÜÿ®ÿ£ ÿ®ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖŸÅŸÇŸàÿØÿ© ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇÿå\n",
    "Ÿäÿ™ŸÜÿ®ÿ£ **ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≠Ÿäÿ∑ÿ©** ÿßÿπÿ™ŸÖÿßÿØŸãÿß ÿπŸÑŸâ ŸÉŸÑŸÖÿ© Ÿàÿßÿ≠ÿØÿ©.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ÿßŸÑŸÉŸÑŸÖÿ© ‚Äúÿ∑ÿ®Ÿäÿ®‚Äù ‚Üí Ÿäÿ™ŸàŸÇÿπ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÇÿ±Ÿäÿ®ÿ© ŸÖŸÜŸáÿß ŸÖÿ´ŸÑ ‚ÄúŸÖÿ±Ÿäÿ∂‚Äùÿå ‚ÄúŸÖÿ≥ÿ™ÿ¥ŸÅŸâ‚Äùÿå ‚ÄúÿØŸàÿßÿ°‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "#### üá¨üáß The Idea\n",
    "\n",
    "It works almost **opposite to CBOW**.\n",
    "Instead of predicting a missing word from context,\n",
    "it predicts the **context words** given a single word.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "> Input word: ‚Äúdoctor‚Äù ‚Üí predicts words like ‚Äúpatient,‚Äù ‚Äúhospital,‚Äù ‚Äúmedicine.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è üá∏üá¶ ŸÉŸäŸÅ ŸäÿπŸÖŸÑ Skip-Gramÿü\n",
    "\n",
    "### üá¨üáß How Skip-Gram Works\n",
    "\n",
    "1. ŸÜÿ£ÿÆÿ∞ ŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÖŸÜÿ™ÿµŸÅ (ŸÖÿ´ŸÑÿßŸã ‚Äúis‚Äù).\n",
    "2. ŸÜÿ≠ÿßŸàŸÑ ÿ™ŸàŸÇÿπ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ≠ŸàŸÑŸáÿß ÿ∂ŸÖŸÜ ŸÜÿßŸÅÿ∞ÿ© ŸÖÿ≠ÿØÿØÿ©.\n",
    "3. ŸÜŸÉÿ±ÿ± ÿßŸÑÿπŸÖŸÑŸäÿ© ŸÑŸÉŸÑ ŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÜÿµ.\n",
    "4. Ÿäÿ™ÿπŸÑŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÇÿ±Ÿäÿ®ÿ© ÿ®ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "üìò ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ‚Äúthis is a simple example‚Äù\n",
    "> ÿπŸÜÿØ ‚Äúthis‚Äù: Ÿäÿ™ÿπŸÑŸÖ ÿßŸÑÿπŸÑÿßŸÇÿ© ŸÖÿπ ‚Äúis‚Äù, ‚Äúa‚Äù.\n",
    "> ÿπŸÜÿØ ‚Äúis‚Äù: Ÿäÿ™ÿπŸÑŸÖ ÿßŸÑÿπŸÑÿßŸÇÿ© ŸÖÿπ ‚Äúthis‚Äù, ‚Äúa‚Äù, ‚Äúsimple‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "1. Take a target word (e.g., ‚Äúis‚Äù).\n",
    "2. Predict the words surrounding it within a set window.\n",
    "3. Repeat for each word in the text.\n",
    "4. The model learns strong associations between nearby words.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "> ‚Äúthis is a simple example‚Äù\n",
    "> For ‚Äúthis‚Äù ‚Üí predicts ‚Äúis‚Äù, ‚Äúa‚Äù.\n",
    "> For ‚Äúis‚Äù ‚Üí predicts ‚Äúthis‚Äù, ‚Äúa‚Äù, ‚Äúsimple‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### üí≠ üá∏üá¶ ÿßŸÑŸÅÿ±ŸÇ ÿ®ŸäŸÜ CBOW Ÿà Skip-Gram\n",
    "\n",
    "### üá¨üáß Difference Between CBOW and Skip-Gram\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨  | üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ©             | üá∏üá¶ ÿßŸÑÿ≥ÿ±ÿπÿ©    | üá∏üá¶ ÿßŸÑÿØŸÇÿ© ŸÅŸä ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ© | üá¨üáß Model                 | üá¨üáß Concept | üá¨üáß Speed                      | üá¨üáß Rare Word Accuracy |\n",
    "| ------------- | ----------------------- | -------------- | ----------------------------- | -------------------------- | ------------ | ------------------------------- | ----------------------- |\n",
    "| **CBOW**      | Ÿäÿ™ŸÜÿ®ÿ£ ÿ®ÿßŸÑŸÉŸÑŸÖÿ© ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ | ÿ£ÿ≥ÿ±ÿπ ‚ö°         | ÿ£ŸÇŸÑ ÿØŸÇÿ© ŸÑŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ©       | Predicts word from context | Faster ‚ö°     | Less accurate for rare words    |                         |\n",
    "| **Skip-Gram** | Ÿäÿ™ŸÜÿ®ÿ£ ÿ®ÿßŸÑÿ≥ŸäÿßŸÇ ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿ© | ÿ£ÿ®ÿ∑ÿ£ ŸÇŸÑŸäŸÑŸãÿß üê¢ | ÿ£ÿØŸÇ ŸÑŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ© üîç        | Predicts context from word | Slower üê¢    | More accurate for rare words üîç |                         |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ üá∏üá¶ ŸÖÿ¥ŸÉŸÑÿ© Softmax ŸÅŸä Word2Vec\n",
    "\n",
    "## üá¨üáß The Softmax Problem\n",
    "\n",
    "ÿπŸÜÿØ Ÿàÿ¨ŸàÿØ ŸÖÿ¶ÿßÿ™ ÿßŸÑÿ¢ŸÑÿßŸÅ ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸä ÿßŸÑŸÇÿßŸÖŸàÿ≥ÿå\n",
    "ÿ™ŸÉŸàŸÜ ÿØÿßŸÑÿ© **Softmax** ÿ®ÿ∑Ÿäÿ¶ÿ© ÿ¨ÿØŸãÿß ŸÑÿ£ŸÜŸáÿß ÿ™ÿ≠ÿ≥ÿ® ÿßŸÑÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÑŸÉŸÑ ŸÉŸÑŸÖÿ© ŸÖŸÖŸÉŸÜÿ©.\n",
    "\n",
    "With vocabularies of hundreds of thousands of words,\n",
    "**Softmax** becomes **very slow and computationally expensive**,\n",
    "since it must compute probabilities for *every possible word*.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è üá∏üá¶ ÿ≠ŸÑŸàŸÑ ŸÑÿ™ÿ≥ÿ±Ÿäÿπ ÿßŸÑÿ™ÿØÿ±Ÿäÿ®\n",
    "\n",
    "## üá¨üáß Techniques to Speed Up Training\n",
    "\n",
    "### 1Ô∏è‚É£ üá∏üá¶ ÿßŸÑÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸáÿ±ŸÖŸä ‚Äì Hierarchical Softmax\n",
    "\n",
    "### 1Ô∏è‚É£ üá¨üáß Hierarchical Softmax\n",
    "\n",
    "ÿ®ÿØŸÑ ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿßÿ≠ÿ™ŸÖÿßŸÑ ŸÑŸÉŸÑ ŸÉŸÑŸÖÿ©ÿå\n",
    "ŸÜÿ±ÿ™ÿ® ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸä **ÿ¥ÿ¨ÿ±ÿ© Ÿáÿ±ŸÖŸäÿ©** (ŸÖÿ´ŸÑ ÿ¥ÿ¨ÿ±ÿ© ŸáŸàŸÅŸÖÿßŸÜ).\n",
    "ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ÿ™ŸÉŸàŸÜ ŸÅŸä ÿßŸÑÿ£ÿπŸÑŸâÿå ŸàÿßŸÑŸÜÿßÿØÿ±ÿ© ŸÅŸä ÿßŸÑÿ£ÿ≥ŸÅŸÑ.\n",
    "ÿßŸÑÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑŸÜŸáÿßÿ¶Ÿä = ÿ≠ÿßÿµŸÑ ÿ∂ÿ±ÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™ ÿßŸÑŸÖÿ≥ÿßÿ± ŸÑŸÑŸàÿµŸàŸÑ ŸÑŸÑŸÉŸÑŸÖÿ©.\n",
    "\n",
    "üìò ÿ®Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ŸÜÿ≠ÿ≥ÿ® ÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™ ÿ£ŸÇŸÑ ‚Üí ÿ£ÿ≥ÿ±ÿπ ÿ®ŸÉÿ´Ÿäÿ±.\n",
    "\n",
    "Instead of calculating probabilities for all words,\n",
    "the vocabulary is arranged in a **hierarchical tree** (like a Huffman tree).\n",
    "Common words are at the top, rare words at the bottom.\n",
    "The final probability is the product of probabilities along the path.\n",
    "\n",
    "üìò This reduces the number of computations, making training much faster.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ üá∏üá¶ ÿßŸÑÿπŸäŸÜÿ© ÿßŸÑÿ≥ŸÑÿ®Ÿäÿ© ‚Äì Negative Sampling\n",
    "\n",
    "### 2Ô∏è‚É£ üá¨üáß Negative Sampling\n",
    "\n",
    "ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ£ÿ¥Ÿáÿ± ŸàÿßŸÑÿ£ÿ≥ÿ±ÿπ ‚ù§Ô∏è\n",
    "\n",
    "ŸÜÿÆÿ™ÿßÿ± ŸÅŸÇÿ∑ **ÿπÿØÿØ ÿµÿ∫Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿÆÿßÿ∑ÿ¶ÿ©** (ŸÖÿ´ŸÑÿßŸã 5‚Äì10)\n",
    "ŸàŸÜÿØÿ±ÿ® ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿπŸÑŸâ ÿ≤ŸäÿßÿØÿ© ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© Ÿàÿ™ŸÇŸÑŸäŸÑ ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßŸÑÿÆÿßÿ∑ÿ¶ÿ©.\n",
    "\n",
    "The most common and efficient technique ‚ù§Ô∏è\n",
    "\n",
    "Only a **small set of negative (wrong) words** (e.g., 5‚Äì10) are sampled,\n",
    "and the model is trained to increase probabilities for correct words and decrease them for wrong ones.\n",
    "\n",
    "üìò Example / ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> Input word: ‚Äújumps‚Äù\n",
    "> Correct words: {the, fox, brown, over}\n",
    "> Wrong words: {apple, Tokyo, boat, orange}\n",
    "\n",
    "ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Ÿäÿ™ÿπŸÑŸÖ ÿ£ŸÜ ‚Äújumps‚Äù ÿ™ÿ±ÿ™ÿ®ÿ∑ ÿ®ŸÄ ‚Äúfox‚Äù Ÿà ‚Äúover‚Äù ŸàŸÑŸäÿ≥ ÿ®ŸÄ ‚Äúapple‚Äù ÿ£Ÿà ‚ÄúTokyo‚Äù.\n",
    "The model learns that ‚Äújumps‚Äù relates to ‚Äúfox‚Äù and ‚Äúover‚Äù, but not to ‚Äúapple‚Äù or ‚ÄúTokyo‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© üá∏üá¶ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©\n",
    "\n",
    "## üá¨üáß Final Outcome\n",
    "\n",
    "ÿ®ÿπÿØ ÿ™ÿØÿ±Ÿäÿ® Word2Vec ÿπŸÑŸâ ŸÉŸÖŸäÿ© ÿ∂ÿÆŸÖÿ© ŸÖŸÜ ÿßŸÑŸÜÿµŸàÿµ:\n",
    "\n",
    "* ŸÉŸÑ ŸÉŸÑŸÖÿ© ÿ™ÿ≠ÿµŸÑ ÿπŸÑŸâ **ÿ™ŸÖÿ´ŸäŸÑ ÿ±ŸÇŸÖŸä (Vector)** ŸÅŸä ŸÅÿ∂ÿßÿ° ÿßŸÑŸÖÿπÿßŸÜŸä.\n",
    "* ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ŸÇÿßÿ±ÿ®ÿ© ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ ÿ™ŸÉŸàŸÜ ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ© ÿ±Ÿäÿßÿ∂ŸäŸãÿß.\n",
    "* ŸäŸÖŸÉŸÜ ŸÑŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÅŸáŸÖ ÿπŸÑÿßŸÇÿßÿ™ ŸÖŸÜÿ∑ŸÇŸäÿ© ŸÖÿ´ŸÑ:\n",
    "\n",
    "  > ‚ÄúŸÖŸÑŸÉ - ÿ±ÿ¨ŸÑ + ÿßŸÖÿ±ÿ£ÿ© = ŸÖŸÑŸÉÿ©‚Äù\n",
    "  > ‚ÄúŸÖÿØÿ±ÿ≥ÿ© - ÿ∑ŸÑÿßÿ® + ÿ£ÿ∑ÿ®ÿßÿ° ‚âà ŸÖÿ≥ÿ™ÿ¥ŸÅŸâ‚Äù\n",
    "\n",
    "After training Word2Vec on large text corpora:\n",
    "\n",
    "* Each word has a **numerical vector** in semantic space.\n",
    "* Words with similar meanings are close together.\n",
    "* The model can understand analogies like:\n",
    "\n",
    "  > ‚Äúking - man + woman = queen‚Äù\n",
    "  > ‚Äúschool - students + doctors ‚âà hospital‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üí° üá∏üá¶ ÿÆŸÑÿßÿµÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© | üá¨üáß Quick Summary\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑŸÖŸÅŸáŸàŸÖ                                     | üá¨üáß Concept                                                    |\n",
    "| ------------------------------------------------ | --------------------------------------------------------------- |\n",
    "| Ÿäÿ≠ŸàŸëŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ•ŸÑŸâ ŸÖÿ™ÿ¨Ÿáÿßÿ™ ÿ±ŸÇŸÖŸäÿ©                   | Converts words into numerical vectors                           |\n",
    "| Ÿäÿ™ÿπŸÑŸÖ ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ŸàŸÑŸäÿ≥ ŸÖŸÜ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ŸÅŸÇÿ∑              | Learns from context, not just frequency                         |\n",
    "| ŸÑŸá ŸÜŸàÿπÿßŸÜ: CBOW Ÿà Skip-Gram                       | Two main architectures: CBOW and Skip-Gram                      |\n",
    "| Ÿäÿ≥ÿ™ÿÆÿØŸÖ Softmax ÿ£Ÿà Negative Sampling              | Uses Softmax or Negative Sampling                               |\n",
    "| ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÖÿ´ŸÑ Word2Vec Ÿà GloVe Ÿà BERT | Foundation for modern NLP models like Word2Vec, GloVe, and BERT |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® üá∏üá¶ ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ŸÖŸáŸÖÿ© | üá¨üáß Key Notes\n",
    "\n",
    "* **CBOW** ÿ£ÿ≥ÿ±ÿπ ŸÅŸä ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ŸàŸÖŸÜÿßÿ≥ÿ® ŸÑŸÑŸÜÿµŸàÿµ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©.\n",
    "  **CBOW** trains faster and works well for large datasets.\n",
    "* **Skip-Gram** ÿ£ÿØŸÇ ŸÅŸä ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÜÿßÿØÿ±ÿ©.\n",
    "  **Skip-Gram** handles rare words with higher accuracy.\n",
    "* ŸÖŸÉÿ™ÿ®ÿ© **Gensim** ŸÅŸä ÿ®ÿßŸäÿ´ŸàŸÜ ŸÖŸÜ ÿ£ÿ¥Ÿáÿ± ÿßŸÑÿ£ÿØŸàÿßÿ™ ŸÑÿ™ÿ∑ÿ®ŸäŸÇ Word2Vec.\n",
    "  **Gensim** is a popular Python library for implementing Word2Vec.\n",
    "* ŸäŸÖŸÉŸÜ ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖÿßÿ∞ÿ¨ ÿπÿ±ÿ®Ÿäÿ© ÿ¨ÿßŸáÿ≤ÿ© ŸÖÿ´ŸÑ **AraVec**.\n",
    "  Pre-trained Arabic models like **AraVec** are also available.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4bfa0",
   "metadata": {},
   "source": [
    "CBOW:\n",
    "\n",
    "https://www.kaggle.com/alincijov/continuous-bag-of-words-cbow-numpy-for-beginners\n",
    "\n",
    "Skip - Gram:\n",
    "\n",
    "https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0154f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß≠ üá∏üá¶ ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿÆÿßŸÖÿ≥: ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÜÿµŸàÿµ | üá¨üáß Text Similarity \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ üá∏üá¶ ŸÖÿß ŸáŸà ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÜÿµŸàÿµÿü | üá¨üáß What is Text Similarity?\n",
    "\n",
    "**ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÜÿµŸàÿµ (Text Similarity)** ŸäÿπŸÜŸä:\n",
    "ŸÉŸäŸÅ ŸÜŸÇŸäÿ≥ *ŸÖÿØŸâ ŸÇÿ±ÿ®* ŸÉŸÑŸÖÿ™ŸäŸÜ ÿ£Ÿà ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÖŸÜ ÿ®ÿπÿ∂ ‚Äî ÿ•ŸÖŸëÿß ŸÅŸä:\n",
    "\n",
    "* **ÿßŸÑÿ¥ŸÉŸÑ (ÿßŸÑÿ≠ÿ±ŸàŸÅ)** ‚Üí ŸÖÿ´ŸÑ: `graffe` Ÿà `giraffe`\n",
    "* **ÿßŸÑŸÖÿπŸÜŸâ (ÿßŸÑÿ≥ŸäŸÖÿßŸÜÿ™ŸÉÿ≥)** ‚Üí ŸÖÿ´ŸÑ:\n",
    "\n",
    "  * ‚ÄúÿßŸÑÿ¨Ÿà ÿ¨ŸÖŸäŸÑ ÿßŸÑŸäŸàŸÖ‚Äù\n",
    "  * ‚ÄúÿßŸÑŸäŸàŸÖ ÿßŸÑÿ∑ŸÇÿ≥ ÿ±ÿßÿ¶ÿπ‚Äù\n",
    "\n",
    "üõ†Ô∏è ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÅŸÉÿßÿ± ŸÅŸä ÿ£ÿ¥Ÿäÿßÿ° ŸÉÿ´Ÿäÿ±ÿ©ÿå ŸÖÿ´ŸÑ:\n",
    "\n",
    "* **ÿ™ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ•ŸÖŸÑÿßÿ° (Auto-Correct)**\n",
    "* **ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∫ÿßŸÖÿ∂ (Fuzzy Search)**\n",
    "* **ÿ™ŸÇŸäŸäŸÖ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ¢ŸÑŸäÿ©**\n",
    "* **ŸÉÿ¥ŸÅ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©/ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÅŸä ÿßŸÑÿØÿßÿ™ÿß**\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ üá∏üá¶ ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ≠ÿ±ŸàŸÅ: ŸÖÿ≥ÿßŸÅÿ© ÿßŸÑÿ™ÿπÿØŸäŸÑ (Edit Distance) | üá¨üáß Character-Level Similarity\n",
    "\n",
    "### üß± ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "ŸÑŸà ÿπŸÜÿØŸÉ ŸÉŸÑŸÖÿ© ŸÖŸÉÿ™Ÿàÿ®ÿ© ÿÆÿ∑ÿ£ÿå ŸÖÿ´ŸÑ:\n",
    "\n",
    "> `graffe`\n",
    "\n",
    "ŸàÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ŸáŸä:\n",
    "\n",
    "> `giraffe`\n",
    "\n",
    "ŸÜÿ±ŸäÿØ ÿ£ŸÜ ŸÜÿπÿ±ŸÅ:\n",
    "**ŸÉŸÖ ÿÆÿ∑Ÿàÿ© ÿ™ÿπÿØŸäŸÑ ŸÜÿ≠ÿ™ÿßÿ¨ ŸÑŸÜÿ≠ŸàŸëŸÑ ÿßŸÑÿ£ŸàŸÑŸâ ÿ•ŸÑŸâ ÿßŸÑÿ´ÿßŸÜŸäÿ©ÿü**\n",
    "\n",
    "ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑŸÖÿ≥ŸÖŸàÿ≠ ÿ®Ÿáÿß ŸáŸä ÿ´ŸÑÿßÿ´ ÿπŸÖŸÑŸäÿßÿ™:\n",
    "\n",
    "1. **ÿ•ÿ∂ÿßŸÅÿ© ÿ≠ÿ±ŸÅ (Insertion)**\n",
    "2. **ÿ≠ÿ∞ŸÅ ÿ≠ÿ±ŸÅ (Deletion)**\n",
    "3. **ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ ÿ≠ÿ±ŸÅ ÿ®ÿ≠ÿ±ŸÅ ÿ¢ÿÆÿ± (Substitution)**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è ŸÖÿ´ÿßŸÑ: jraaffe ‚Üí giraffe\n",
    "\n",
    "ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿÆÿßÿ∑ÿ¶ÿ©: `jraaffe`\n",
    "ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©: `giraffe`\n",
    "\n",
    "ŸÖŸÖŸÉŸÜ ŸÜÿπÿØŸëŸÑŸáÿß ŸáŸÉÿ∞ÿß:\n",
    "\n",
    "1. ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ `j` ÿ®ŸÄ `g` ‚Üí `graaffe`\n",
    "2. ÿ•ÿ∂ÿßŸÅÿ© `i` ÿ®ÿπÿØ `g` ‚Üí `giraaffe`\n",
    "3. ÿ≠ÿ∞ŸÅ ÿ≠ÿ±ŸÅ `a` ÿßŸÑÿ≤ÿßÿ¶ÿØ ‚Üí `giraffe`\n",
    "\n",
    "ÿ•ÿ∞ŸÜ **ŸÖÿ≥ÿßŸÅÿ© ÿßŸÑÿ™ÿπÿØŸäŸÑ (Edit Distance)** = 3\n",
    "ÿ£Ÿä ŸÜÿ≠ÿ™ÿßÿ¨ **3 ÿÆÿ∑Ÿàÿßÿ™** ŸÑÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÉŸÑŸÖÿ© ÿ•ŸÑŸâ ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©.\n",
    "\n",
    "Ÿáÿ∞ÿß ŸÖÿß Ÿäÿ≥ŸÖŸëŸâ:\n",
    "\n",
    "> **Minimum Edit Distance**\n",
    "> ÿ£ŸÇŸÑ ÿπÿØÿØ ŸÖŸÜ ÿπŸÖŸÑŸäÿßÿ™: ÿ•ÿ∂ÿßŸÅÿ© / ÿ≠ÿ∞ŸÅ / ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑÿå ŸÑÿ™ÿ≠ŸàŸäŸÑ ŸÉŸÑŸÖÿ© ÿ•ŸÑŸâ ÿ£ÿÆÿ±Ÿâ.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ üá∏üá¶ ŸÉŸäŸÅ ÿ™Ÿèÿ≠ÿ≥ÿ® ÿ±Ÿäÿßÿ∂ŸäŸãÿßÿü (ÿ®ÿ®ÿ≥ÿßÿ∑ÿ©)\n",
    "\n",
    "### üá¨üáß How Is It Computed?\n",
    "\n",
    "ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© ÿ™ÿ¥ÿ®Ÿá ÿßŸÑÿ¨ÿØÿßŸàŸÑ (Dynamic Programming):\n",
    "\n",
    "* ŸÜÿ∂ÿπ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿπŸÑŸâ ÿßŸÑÿµŸÅ ÿßŸÑÿπŸÑŸàŸäÿå\n",
    "* ŸàÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ© ÿπŸÑŸâ ÿßŸÑÿπŸÖŸàÿØ ÿßŸÑÿ¨ÿßŸÜÿ®Ÿä.\n",
    "* ŸÉŸÑ ÿÆÿßŸÜÿ© ÿ™ŸÖÿ´ŸÑ:\n",
    "  **ÿ£ŸÇŸÑ ÿπÿØÿØ ÿ™ÿπÿØŸäŸÑÿßÿ™ ŸÑÿ™ÿ≠ŸàŸäŸÑ ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿ•ŸÑŸâ ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑÿ´ÿßŸÜŸäÿ©.**\n",
    "\n",
    "ŸÜŸÇÿßÿ±ŸÜ:\n",
    "\n",
    "* ÿ£ŸàŸÑ ÿ≠ÿ±ŸÅ ŸÖÿπ ÿ£ŸàŸÑ ÿ≠ÿ±ŸÅ\n",
    "* ÿ£ŸàŸÑ ÿ≠ÿ±ŸÅŸäŸÜ ŸÖÿπ ÿ£ŸàŸÑ ÿ≠ÿ±ŸÅŸäŸÜ\n",
    "* ÿ£ŸàŸÑ 3 ÿ≠ÿ±ŸàŸÅ ŸÖÿπ ÿ£ŸàŸÑ 3 ÿ≠ÿ±ŸàŸÅ\n",
    "  ‚Ä¶ ŸàŸáŸÉÿ∞ÿßÿå ÿ•ŸÑŸâ ÿ£ŸÜ ŸÜŸÜÿ™ŸáŸä ŸÖŸÜ ÿ£ŸÇÿµÿ± ÿßŸÑŸÉŸÑŸÖÿ™ŸäŸÜ.\n",
    "\n",
    "ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©:\n",
    "**ÿ¢ÿÆÿ± ÿÆÿßŸÜÿ© ŸÅŸä ÿßŸÑÿ¨ÿØŸàŸÑ = ŸÖÿ≥ÿßŸÅÿ© ÿßŸÑÿ™ÿπÿØŸäŸÑ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿ™ŸäŸÜ.**\n",
    "\n",
    "ŸÖÿß ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ™ÿ≠ŸÅÿ∏ ÿßŸÑÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ©ÿå ÿßŸÑŸÖŸáŸÖ ÿ™ŸÅŸáŸÖ ÿßŸÑŸÅŸÉÿ±ÿ© üëå\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ üá∏üá¶ Ÿàÿ∏ŸäŸÅÿ© ÿßŸÑŸÄ Edit Distance ŸÅŸä ÿßŸÑŸÄ Auto-Correct\n",
    "\n",
    "## üá¨üáß How It Powers Auto-Correct?\n",
    "\n",
    "ŸÑŸà ŸÉÿ™ÿ® ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ:\n",
    "\n",
    "> `graffe`\n",
    "\n",
    "ŸäŸÇŸàŸÖ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿ®ÿßŸÑÿ™ÿßŸÑŸä:\n",
    "\n",
    "1. Ÿäÿ®ÿ≠ÿ´ ŸÅŸä ÿßŸÑŸÇÿßŸÖŸàÿ≥ ÿπŸÜ ŸÉŸÑŸÖÿßÿ™ ŸÇÿ±Ÿäÿ®ÿ© ŸÅŸä ÿßŸÑÿ≠ÿ±ŸàŸÅ (ŸÖÿ≥ÿßŸÅÿ© ÿ™ÿπÿØŸäŸÑ ÿµÿ∫Ÿäÿ±ÿ©):\n",
    "\n",
    "   * `giraffe`\n",
    "   * `grail`\n",
    "   * `graft`\n",
    "   * `graf`\n",
    "2. Ÿäÿ≠ÿ≥ÿ® ŸÖÿ≥ÿßŸÅÿ© ÿßŸÑÿ™ÿπÿØŸäŸÑ ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ÿßŸÑŸÖŸÉÿ™Ÿàÿ®ÿ© ŸàŸÉŸÑ ŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÇÿßŸÖŸàÿ≥.\n",
    "3. ŸäÿÆÿ™ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™ **ÿßŸÑÿ£ŸÇÿ±ÿ®** (ÿ£ŸÇŸÑ Edit Distance).\n",
    "4. ÿ£ÿ≠ŸäÿßŸÜŸãÿß Ÿäÿ£ÿÆÿ∞ ŸÅŸä ÿßŸÑÿ≠ÿ≥ÿ®ÿßŸÜ:\n",
    "\n",
    "   * ŸÖÿØŸâ ÿßŸÜÿ™ÿ¥ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© ŸÅŸä ÿßŸÑŸÑÿ∫ÿ©\n",
    "   * ÿ≥ŸäÿßŸÇ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ≠ŸàŸÑŸáÿß\n",
    "\n",
    "ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\n",
    "ŸäŸÇÿ™ÿ±ÿ≠ ÿπŸÑŸäŸÉ ŸÖÿ´ŸÑŸãÿß:\n",
    "\n",
    "> Did you mean **‚Äúgiraffe‚Äù**?\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ üá∏üá¶ ÿ™ŸÉŸÑŸÅÿ© ÿßŸÑÿ™ÿπÿØŸäŸÑ (Cost Function) | üá¨üáß Edit Cost Function\n",
    "\n",
    "ŸÖÿ¥ ŸÉŸÑ ÿ™ÿπÿØŸäŸÑ ŸÑÿßÿ≤ŸÖ ŸäŸÉŸàŸÜ ŸÑŸá ŸÜŸÅÿ≥ ÿßŸÑÿ™ŸÉŸÑŸÅÿ©.\n",
    "ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™:\n",
    "\n",
    "* ÿ≠ÿ∞ŸÅ ÿ≠ÿ±ŸÅ = 1\n",
    "* ÿ•ÿ∂ÿßŸÅÿ© ÿ≠ÿ±ŸÅ = 1\n",
    "* ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ ÿ≠ÿ±ŸÅ = **2** (ÿ£ÿµÿπÿ® ÿ¥ŸàŸä)\n",
    "\n",
    "ÿ£Ÿà ŸÖŸÖŸÉŸÜ ÿßŸÑÿπŸÉÿ≥ ÿ≠ÿ≥ÿ® ÿ∑ÿ®Ÿäÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™.\n",
    "\n",
    "ŸÅŸÉÿ±ÿ© **Cost Function**:\n",
    "\n",
    "> ŸÜÿ™ÿ≠ŸÉŸÖ ŸÅŸä Ÿàÿ≤ŸÜ ŸÉŸÑ ŸÜŸàÿπ ÿ™ÿπÿØŸäŸÑÿå\n",
    "> ÿπÿ¥ÿßŸÜ ŸÜÿ≠ÿµŸÑ ÿπŸÑŸâ ŸÖÿ≥ÿßŸÅÿ© ÿ£ŸÉÿ´ÿ± ŸàÿßŸÇÿπŸäÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ üá∏üá¶ ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿßŸÑÿ™ÿ®ÿßÿ≥ ÿ®ŸäŸÜ ÿßŸÑÿ≠ÿ±ŸàŸÅ | üá¨üáß Letter Confusion Matrix\n",
    "\n",
    "ÿ®ÿπÿ∂ ÿßŸÑÿ≠ÿ±ŸàŸÅ **ÿ™ÿ™ÿ¥ÿßÿ®Ÿá ÿ£Ÿà ÿ™ŸèÿÆŸÑÿ∑ ŸÉÿ´Ÿäÿ±Ÿãÿß**:\n",
    "\n",
    "* ŸÅŸä ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©:\n",
    "\n",
    "  * `a` ‚Üî `e`\n",
    "  * `o` ‚Üî `u`\n",
    "* ŸÅŸä ÿßŸÑÿπÿ±ÿ®Ÿäÿ©:\n",
    "\n",
    "  * `ÿ©` ‚Üî `Ÿá`\n",
    "  * `Ÿä` ‚Üî `Ÿâ`\n",
    "\n",
    "Ÿàÿ£ÿ≠ŸäÿßŸÜŸãÿß ŸäŸÉŸàŸÜ ÿßŸÑÿ≥ÿ®ÿ® **ŸÇÿ±ÿ® ÿßŸÑÿ≠ÿ±ŸàŸÅ ŸÅŸä ÿßŸÑŸÉŸäÿ®Ÿàÿ±ÿØ**:\n",
    "\n",
    "* ŸÖÿ´ŸÑŸãÿß: `ÿ•ÿ≥ŸÑÿßŸÖ` ‚Üî `ÿ∫ÿ≥ŸÑÿßŸÖ`\n",
    "\n",
    "ŸÑÿ∞ŸÑŸÉ ÿ®ÿπÿ∂ ÿßŸÑÿ£ŸÜÿ∏ŸÖÿ© ÿ™ÿ®ŸÜŸä **ŸÖÿµŸÅŸàŸÅÿ© ÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™**:\n",
    "\n",
    "> ÿ£Ÿä ÿ≠ÿ±ŸÅ ÿ∫ÿßŸÑÿ®Ÿãÿß ŸäŸèÿÆÿ∑ÿ£ ŸÅŸäŸá ÿ®ÿ£Ÿä ÿ≠ÿ±ŸÅÿü\n",
    "\n",
    "Ÿáÿ∞Ÿá ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ™ÿπÿØŸäŸÑ ÿßŸÑŸÄ **cost**:\n",
    "ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑ ÿ≠ÿ±ŸÅ ÿ®ÿ≠ÿ±ŸÅ \"ŸÖÿ¥ÿßÿ®Ÿá\" ŸäŸÉŸàŸÜ **ÿ£ÿ±ÿÆÿµ** ŸÖŸÜ ÿßÿ≥ÿ™ÿ®ÿØÿßŸÑŸá ÿ®ÿ≠ÿ±ŸÅ ÿ®ÿπŸäÿØ ÿ™ŸÖÿßŸÖŸãÿß.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/LCM.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ üá∏üá¶ ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ¨ŸÖŸÑ ÿ®ÿßŸÑÿ≠ÿ±ŸàŸÅ: ŸÖÿ≠ÿØŸàÿØ ÿ¨ÿØŸãÿß\n",
    "\n",
    "## üá¨üáß Limitations of Character-Based Similarity\n",
    "\n",
    "ŸÖÿ¥ŸÉŸÑÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸÅŸä ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ≠ÿ±ŸàŸÅ:\n",
    "\n",
    "* ŸÖŸÖŸÉŸÜ ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸäŸÉŸàŸÜ ÿ¥ŸÉŸÑŸáŸÖ ŸÖÿ™ŸÇÿßÿ±ÿ® ÿ¨ÿØŸãÿß\n",
    "  ŸÑŸÉŸÜ ÿßŸÑŸÖÿπŸÜŸâ **ŸÖÿÆÿ™ŸÑŸÅ ÿ™ŸÖÿßŸÖŸãÿß**.\n",
    "\n",
    "ŸÖÿ´ÿßŸÑ:\n",
    "\n",
    "> ‚ÄúŸÅÿ±ŸÜÿ≥ÿß ÿØŸàŸÑÿ© ÿ∫ŸÜŸäÿ© ÿ®ŸäŸÜŸÖÿß ÿ£ŸÑŸÖÿßŸÜŸäÿß Ÿäÿ™ÿ±ÿßÿ¨ÿπ ÿßŸÇÿ™ÿµÿßÿØŸáÿß‚Äù\n",
    "> ‚Äúÿ£ŸÑŸÖÿßŸÜŸäÿß ÿØŸàŸÑÿ© ÿ∫ŸÜŸäÿ© ÿ®ŸäŸÜŸÖÿß ŸÅÿ±ŸÜÿ≥ÿß Ÿäÿ™ÿ±ÿßÿ¨ÿπ ÿßŸÇÿ™ÿµÿßÿØŸáÿß‚Äù\n",
    "\n",
    "ŸÜŸÅÿ≥ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿßÿå\n",
    "ŸÑŸÉŸÜ ÿßŸÑŸÖÿπŸÜŸâ ŸÖŸÇŸÑŸàÿ® ÿ™ŸÖÿßŸÖŸãÿß.\n",
    "\n",
    "ŸÑÿ∞ŸÑŸÉ:\n",
    "ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ≠ÿ±ŸàŸÅ **ŸÖŸÖÿ™ÿßÿ≤ ŸÑŸÑÿ•ŸÖŸÑÿßÿ°**ÿå\n",
    "ŸÑŸÉŸÜŸá **ÿ∫Ÿäÿ± ŸÉÿßŸÅŸç ŸÑŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ**.\n",
    "\n",
    "ŸàŸáŸÜÿß ŸÜÿ±Ÿàÿ≠ ŸÑŸÑŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿ£ŸÇŸàŸâ üëá\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ üá∏üá¶ ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÖÿπŸÜŸâ ÿ®ŸäŸÜ ÿßŸÑÿ¨ŸÖŸÑ: ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ¨ŸÖŸÑ (Sentence Embeddings)\n",
    "\n",
    "## üá¨üáß Semantic Textual Similarity\n",
    "\n",
    "ÿ®ÿØŸÑ ŸÖÿß ŸÜŸÇŸäÿ≥ ÿ™ÿ¥ÿßÿ®Ÿá **ÿ≠ÿ±ŸàŸÅ**ÿå\n",
    "ŸÜŸÇŸäÿ≥ ÿ™ÿ¥ÿßÿ®Ÿá **ŸÖÿπŸÜŸâ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ®ÿßŸÑŸÉÿßŸÖŸÑ**.\n",
    "\n",
    "ÿßŸÑŸÅŸÉÿ±ÿ©:\n",
    "\n",
    "1. ŸÜÿ≥ÿ™ÿÆÿØŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿ´ŸÑ:\n",
    "\n",
    "   * **Universal Sentence Encoder**\n",
    "   * ÿ£Ÿà ŸÜŸÖÿßÿ∞ÿ¨ ÿ£ÿ≠ÿØÿ´ ŸÖÿ´ŸÑ **Sentence-BERT**ÿå ÿ£Ÿà ŸÜŸÖÿßÿ∞ÿ¨ ÿ™ÿπŸÑŸëŸÖ ÿπŸÖŸäŸÇ ÿ≠ÿØŸäÿ´ÿ©.\n",
    "2. ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ™ÿØÿÆŸÑ ŸÉŸÜÿµ:\n",
    "\n",
    "   > \"I love machine learning\"\n",
    "3. ÿ™ÿÆÿ±ÿ¨ ŸÉŸÄ **ŸÖÿ™ÿ¨Ÿá ÿ±ŸÇŸÖŸä ÿ∑ŸàŸäŸÑ** ŸÖÿ´ŸÑÿßŸã:\n",
    "\n",
    "   * ÿ∑ŸàŸÑ 512 ŸÇŸäŸÖÿ© (512-dim vector)\n",
    "4. ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ™ŸäŸÜ ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ ‚Üí\n",
    "   ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™ ÿ≠ŸÇŸëŸáŸÖ ŸÇÿ±Ÿäÿ®ÿ© (Cosine similarity ÿπÿßŸÑŸäÿ©).\n",
    "   ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÖÿÆÿ™ŸÑŸÅÿ™ŸäŸÜ ‚Üí ÿßŸÑŸÖÿ™ÿ¨Ÿáÿßÿ™ ÿ®ÿπŸäÿØÿ©.\n",
    "\n",
    "---\n",
    "\n",
    "### üéõÔ∏è üá∏üá¶ ŸÖÿµŸÅŸàŸÅÿ© ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ¨ŸÖŸÑ | üá¨üáß Sentence Similarity Matrix\n",
    "\n",
    "ŸÑŸà ÿπŸÜÿØŸÉ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ¨ŸÖŸÑÿå ŸÖŸÖŸÉŸÜ:\n",
    "\n",
    "* ŸÜÿ≠ÿ≥ÿ® ÿ™ÿ¥ÿßÿ®Ÿá ŸÉŸÑ ÿ¨ŸÖŸÑÿ© ŸÖÿπ ŸÉŸÑ ÿ¨ŸÖŸÑÿ©.\n",
    "* ŸÜÿ∂ÿπ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ŸÅŸä **ŸÖÿµŸÅŸàŸÅÿ©**.\n",
    "* ŸÜÿ±ÿ≥ŸÖŸáÿß ŸÉŸÄ **Heatmap** (ÿ£ŸÑŸàÿßŸÜ):\n",
    "\n",
    "  * ŸÑŸàŸÜ ŸÇŸàŸä = ÿ™ÿ¥ÿßÿ®Ÿá ÿπÿßŸÑŸä\n",
    "  * ŸÑŸàŸÜ ÿ∂ÿπŸäŸÅ = ÿ™ÿ¥ÿßÿ®Ÿá ŸÇŸÑŸäŸÑ\n",
    "\n",
    "Ÿáÿ∞ÿß ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÅŸä:\n",
    "\n",
    "* ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ© ÿ£Ÿà ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ©.\n",
    "* ŸÅŸáŸÖ ÿ£Ÿä ÿßŸÑÿ¨ŸÖŸÑ ÿ™ŸÜÿ™ŸÖŸä ŸÑŸÜŸÅÿ≥ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ.\n",
    "* ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ¨ŸÖŸÑ ÿ£Ÿà ÿßŸÑÿ±ÿØŸàÿØ ÿßŸÑŸÖÿ™ŸÇÿßÿ±ÿ®ÿ© ŸÅŸä ÿßŸÑŸÄ Chatbots.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/Semantic Textual Similarity.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/USE.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ üá∏üá¶ 2025: ŸÖÿßÿ∞ÿß ŸÜÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸäŸàŸÖÿü\n",
    "\n",
    "## üá¨üáß What‚Äôs Commonly Used Today?\n",
    "\n",
    "* ÿπŸÑŸâ ŸÖÿ≥ÿ™ŸàŸâ **ÿßŸÑŸÉŸÑŸÖÿßÿ™**:\n",
    "\n",
    "  * ŸÖÿß ÿ≤ÿßŸÑÿ™ **Edit Distance** ÿ£ÿ≥ÿßÿ≥Ÿäÿ© ŸÅŸä:\n",
    "\n",
    "    * Auto-correct\n",
    "    * Fuzzy search\n",
    "* ÿπŸÑŸâ ŸÖÿ≥ÿ™ŸàŸâ **ÿßŸÑÿ¨ŸÖŸÑ ŸàÿßŸÑŸÖÿπŸÜŸâ**:\n",
    "\n",
    "  * ŸÜÿ≥ÿ™ÿÆÿØŸÖ **Sentence Embeddings** ŸÖŸÜ ŸÜŸÖÿßÿ∞ÿ¨:\n",
    "\n",
    "    * Sentence-BERT\n",
    "    * Universal Sentence Encoder\n",
    "    * ŸÜŸÖÿßÿ∞ÿ¨ Transformer ÿ≠ÿØŸäÿ´ÿ© (ŸÖÿ´ŸÑ ÿπÿßÿ¶ŸÑÿ© BERT ŸàGPT Ÿàÿ∫Ÿäÿ±Ÿáÿß)\n",
    "\n",
    "ŸÉŸÑŸáÿß ÿ™ÿπÿ™ŸÖÿØ ÿπŸÑŸâ ŸÜŸÅÿ≥ ÿßŸÑŸÅŸÉÿ±ÿ©:\n",
    "\n",
    "> ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ•ŸÑŸâ **ŸÖÿ™ÿ¨Ÿá** ÿ´ŸÖ ŸÇŸäÿßÿ≥ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜŸáÿß Ÿàÿ®ŸäŸÜ ÿ∫Ÿäÿ±Ÿáÿß ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖŸÇÿßŸäŸäÿ≥ ŸÖÿ´ŸÑ **Cosine Similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß≠ üá¨üáß Section 5: Text Similarity\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ üá¨üáß What is Text Similarity?\n",
    "\n",
    "**Text Similarity** means measuring *how close* two words or sentences are ‚Äî either in:\n",
    "\n",
    "* **Form (characters)** ‚Üí e.g., `graffe` and `giraffe`\n",
    "* **Meaning (semantics)** ‚Üí e.g.:\n",
    "\n",
    "  * ‚ÄúThe weather is beautiful today‚Äù\n",
    "  * ‚ÄúToday the weather is lovely‚Äù\n",
    "\n",
    "üõ†Ô∏è These ideas are widely used in:\n",
    "\n",
    "* **Spell correction (Auto-Correct)**\n",
    "* **Fuzzy Search**\n",
    "* **Machine Translation Evaluation**\n",
    "* **Detecting similar questions or sentences in data**\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ üá¨üáß Character-Level Similarity: Edit Distance\n",
    "\n",
    "### üß± The Basic Idea\n",
    "\n",
    "If you have a misspelled word, like:\n",
    "\n",
    "> `graffe`\n",
    "\n",
    "and the correct word is:\n",
    "\n",
    "> `giraffe`\n",
    "\n",
    "We want to know:\n",
    "**How many edit steps do we need to transform the first into the second?**\n",
    "\n",
    "The allowed steps are three operations:\n",
    "\n",
    "1. **Insertion** ‚Äî adding a letter\n",
    "2. **Deletion** ‚Äî removing a letter\n",
    "3. **Substitution** ‚Äî replacing one letter with another\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Example: jraaffe ‚Üí giraffe\n",
    "\n",
    "Misspelled word: `jraaffe`\n",
    "Correct word: `giraffe`\n",
    "\n",
    "Possible steps:\n",
    "\n",
    "1. Replace `j` with `g` ‚Üí `graaffe`\n",
    "2. Insert `i` after `g` ‚Üí `giraaffe`\n",
    "3. Remove the extra `a` ‚Üí `giraffe`\n",
    "\n",
    "So the **Edit Distance = 3**\n",
    "That means we need **3 steps** to fix the word.\n",
    "\n",
    "This is called:\n",
    "\n",
    "> **Minimum Edit Distance**\n",
    "> The minimum number of Insertions / Deletions / Substitutions needed to transform one word into another.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ üá¨üáß How Is It Computed? (Simply)\n",
    "\n",
    "We use an algorithm similar to filling a table ‚Äî **Dynamic Programming**:\n",
    "\n",
    "* Place the first word along the top row,\n",
    "* The second word along the side column.\n",
    "* Each cell represents:\n",
    "  **The smallest number of edits needed to turn part of the first word into part of the second.**\n",
    "\n",
    "We compare:\n",
    "\n",
    "* First letter vs first letter\n",
    "* First two letters vs first two letters\n",
    "* First three vs first three\n",
    "  ‚Ä¶ and so on until we reach the shorter word.\n",
    "\n",
    "Finally:\n",
    "**The last cell = the total edit distance between the two words.**\n",
    "\n",
    "You don‚Äôt need to memorize the math ‚Äî just understand the idea üëå\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ üá¨üáß The Role of Edit Distance in Auto-Correct\n",
    "\n",
    "If a user types:\n",
    "\n",
    "> `graffe`\n",
    "\n",
    "The system works like this:\n",
    "\n",
    "1. Searches in the dictionary for nearby words (small edit distance):\n",
    "\n",
    "   * `giraffe`\n",
    "   * `grail`\n",
    "   * `graft`\n",
    "   * `graf`\n",
    "2. Calculates the edit distance between the typed word and each candidate.\n",
    "3. Picks the **closest** words (with smallest distance).\n",
    "4. Sometimes considers:\n",
    "\n",
    "   * How common each word is\n",
    "   * The sentence context\n",
    "\n",
    "Then it suggests:\n",
    "\n",
    "> Did you mean **‚Äúgiraffe‚Äù**?\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ üá¨üáß Edit Cost Function\n",
    "\n",
    "Not every edit has to cost the same.\n",
    "In some systems:\n",
    "\n",
    "* Deleting a character = 1\n",
    "* Inserting a character = 1\n",
    "* Substituting = **2** (a bit harder)\n",
    "\n",
    "Or vice versa ‚Äî depending on the use case.\n",
    "\n",
    "The **Cost Function** idea:\n",
    "\n",
    "> Controls the ‚Äúweight‚Äù of each edit operation\n",
    "> to get a more realistic distance measurement.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ üá¨üáß Letter Confusion Matrix\n",
    "\n",
    "Some letters are **often mixed up**:\n",
    "\n",
    "* In English:\n",
    "\n",
    "  * `a` ‚Üî `e`\n",
    "  * `o` ‚Üî `u`\n",
    "* In Arabic:\n",
    "\n",
    "  * `ÿ©` ‚Üî `Ÿá`\n",
    "  * `Ÿä` ‚Üî `Ÿâ`\n",
    "\n",
    "Sometimes the cause is **keyboard proximity**:\n",
    "\n",
    "* For example: `ÿ•ÿ≥ŸÑÿßŸÖ` ‚Üî `ÿ∫ÿ≥ŸÑÿßŸÖ`\n",
    "\n",
    "That‚Äôs why some systems build a **probability matrix**:\n",
    "\n",
    "> Which letters are most often confused with each other?\n",
    "\n",
    "This matrix is used to adjust the **cost** ‚Äî\n",
    "Replacing one letter with a *similar* one is **cheaper** than replacing it with a totally different one.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/LCM.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ üá¨üáß Character-Level Sentence Similarity: Very Limited\n",
    "\n",
    "A major problem with letter-based similarity:\n",
    "\n",
    "* Two sentences might look *very* similar\n",
    "  but have **completely different meanings**.\n",
    "\n",
    "Example:\n",
    "\n",
    "> ‚ÄúFrance is a rich country while Germany‚Äôs economy is declining.‚Äù\n",
    "> ‚ÄúGermany is a rich country while France‚Äôs economy is declining.‚Äù\n",
    "\n",
    "They share almost the same words,\n",
    "but the meaning is reversed.\n",
    "\n",
    "So:\n",
    "Character-level similarity is **great for spelling**,\n",
    "but **not enough for meaning**.\n",
    "\n",
    "That‚Äôs why we move to a stronger approach üëá\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ üá¨üáß Semantic Sentence Similarity (Sentence Embeddings)\n",
    "\n",
    "Instead of comparing **letters**,\n",
    "we compare **sentence meanings**.\n",
    "\n",
    "The idea:\n",
    "\n",
    "1. Use a model like:\n",
    "\n",
    "   * **Universal Sentence Encoder (USE)**\n",
    "   * or newer ones like **Sentence-BERT**, or other deep-learning Transformers.\n",
    "2. Input the sentence as text:\n",
    "\n",
    "   > \"I love machine learning\"\n",
    "3. Output: a **numeric vector**, for example:\n",
    "\n",
    "   * 512-dimensional vector (512 values)\n",
    "4. If two sentences are semantically close ‚Üí\n",
    "   their vectors are close (high cosine similarity).\n",
    "   If they differ ‚Üí their vectors are far apart.\n",
    "\n",
    "---\n",
    "\n",
    "### üéõÔ∏è üá¨üáß Sentence Similarity Matrix\n",
    "\n",
    "If you have multiple sentences, you can:\n",
    "\n",
    "* Compute similarity for every pair of sentences.\n",
    "* Store results in a **matrix**.\n",
    "* Visualize it as a **heatmap** (color-coded):\n",
    "\n",
    "  * Strong color = high similarity\n",
    "  * Faint color = low similarity\n",
    "\n",
    "This is useful for:\n",
    "\n",
    "* Detecting duplicate or similar sentences.\n",
    "* Understanding which sentences belong to the same topic.\n",
    "* Grouping or classifying chatbot responses.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/Semantic Textual Similarity.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/USE.png\" alt=\"NLP Tools\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ üá¨üáß What‚Äôs Commonly Used Today (2025)\n",
    "\n",
    "* On the **word level**:\n",
    "\n",
    "  * **Edit Distance** remains essential for:\n",
    "\n",
    "    * Auto-correct\n",
    "    * Fuzzy Search\n",
    "* On the **sentence/meaning level**:\n",
    "\n",
    "  * We use **Sentence Embeddings** from models like:\n",
    "\n",
    "    * **Sentence-BERT**\n",
    "    * **Universal Sentence Encoder (USE)**\n",
    "    * Modern Transformer models (e.g., **BERT**, **GPT**, etc.)\n",
    "\n",
    "They all follow the same idea:\n",
    "\n",
    "> Convert a sentence into a **vector**,\n",
    "> then measure similarity using metrics such as **Cosine Similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## ü™Ñ üá¨üáß Quick Summary\n",
    "\n",
    "| üá¨üáß Concept                       | Description                                |\n",
    "| ---------------------------------- | ------------------------------------------ |\n",
    "| **Edit Distance**                  | Measures minimal edits between words       |\n",
    "| **Cost Function**                  | Assigns weights to edit operations         |\n",
    "| **Letter Confusion Matrix**        | Models how letters are often confused      |\n",
    "| **Character Similarity**           | Good for spelling, not for meaning         |\n",
    "| **Sentence Embeddings (512 dims)** | Capture **semantic** similarity            |\n",
    "| **Heatmap / Similarity Matrix**    | Visualizes sentence-to-sentence similarity |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd6463",
   "metadata": {},
   "source": [
    "\n",
    "# üß≠ üá∏üá¶ ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ≥ÿßÿØÿ≥ ŸàÿßŸÑÿ£ÿÆŸäÿ±: ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™Ÿàÿ≤ŸäÿπŸä | üá¨üáß Distributional Similarity\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ üá∏üá¶ ŸÖÿß ŸáŸà ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™Ÿàÿ≤ŸäÿπŸäÿü | üá¨üáß What Is Distributional Similarity?\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™Ÿàÿ≤ŸäÿπŸä ŸÅŸÉÿ±ÿ© ÿ∞ŸÉŸäÿ© ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ŸÑŸÅŸáŸÖ ŸÖÿπŸÜŸâ ÿßŸÑŸÉŸÑŸÖÿßÿ™ **ÿ®ÿØŸàŸÜ ŸÇÿßŸÖŸàÿ≥**.\n",
    "ŸÑÿ£ŸÜ ÿßŸÑŸÇŸàÿßŸÖŸäÿ≥ ŸÖÿß ÿ™ÿ∫ÿ∑Ÿä ŸÉŸÑ ÿßŸÑŸÑÿ∫ÿßÿ™ ÿ£Ÿà ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¨ÿØŸäÿØÿ©.\n",
    "ŸÅÿßŸÑŸÅŸÉÿ±ÿ© ÿ™ŸÇŸàŸÑ:\n",
    "\n",
    "> \"ÿßŸÑŸÉŸÑŸÖÿ© ÿ™Ÿèÿπÿ±ŸÅ ŸÖŸÜ ÿ≥ŸäÿßŸÇŸáÿß ‚Äî ÿ£Ÿä ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ∏Ÿáÿ± ÿ®ÿ¨ÿßŸÜÿ®Ÿáÿß.\"\n",
    "\n",
    "ŸäÿπŸÜŸä ŸÑŸà ÿ∏Ÿáÿ±ÿ™ ŸÉŸÑŸÖÿ© ÿ∫ÿ±Ÿäÿ®ÿ© ŸÉÿ´Ÿäÿ±Ÿãÿß ÿ®ÿ¨ÿßŸÜÿ® ŸÉŸÑŸÖÿßÿ™ ŸÜÿπÿ±ŸÅŸáÿßÿå ŸÜŸÇÿØÿ± ŸÜÿÆŸÖŸÜ ŸÖÿπŸÜÿßŸáÿß ŸÖŸÜ ÿßŸÑÿ®Ÿäÿ¶ÿ© ÿ≠ŸàŸÑŸáÿß.\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "Distributional Similarity helps us understand **word meanings without a dictionary**.\n",
    "Since dictionaries can‚Äôt include every new word, we rely on context:\n",
    "\n",
    "> ‚ÄúYou shall know a word by the company it keeps.‚Äù\n",
    "\n",
    "If an unknown word appears near known ones, we can infer its meaning from the surrounding context.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ üá∏üá¶ ŸÖÿ´ÿßŸÑ ÿ®ÿ≥Ÿäÿ∑ | üá¨üáß A Simple Example\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ŸÑŸà ÿπŸÜÿØŸÜÿß ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑÿ™ÿßŸÑŸäÿ©:\n",
    "\n",
    "1. ÿ∞ÿ®ÿ≠ ÿ£ÿ®Ÿä **ŸÇŸÜÿπÿ±ÿß**\n",
    "2. ŸÑŸÖ Ÿäÿ≠ÿ® ÿ£ÿÆŸä ŸÑÿ≠ŸÖ **ÿßŸÑŸÇŸÜÿπÿ±** ÿßŸÑÿ®ÿßÿ±ÿ≠ÿ©\n",
    "3. Ÿáÿ±ŸàŸÑ **ÿßŸÑŸÇŸÜÿπÿ±** Ÿáÿ±ÿ®Ÿãÿß ŸÖŸÜ ÿßŸÑŸÑŸäÿ´\n",
    "4. ÿ•ŸÜŸá ÿ≥ŸÖŸäŸÜ ŸÉŸÄ **ÿßŸÑŸÇŸÜÿπÿ±**\n",
    "\n",
    "ÿ≠ÿ™Ÿâ ŸÑŸà ŸÖÿß ŸÜÿπÿ±ŸÅ ŸÉŸÑŸÖÿ© \"ÿßŸÑŸÇŸÜÿπÿ±\"ÿå ŸÜŸÑÿßÿ≠ÿ∏ ÿ£ŸÜŸáÿß ÿ™ÿ∏Ÿáÿ± ŸÖÿπ ŸÉŸÑŸÖÿßÿ™ ŸÖÿ´ŸÑ:\n",
    "\"ÿ∞ÿ®ÿ≠\"ÿå \"ŸÑÿ≠ŸÖ\"ÿå \"ÿ≥ŸÖŸäŸÜ\"ÿå \"Ÿáÿ±ÿ®\"\n",
    "ŸÅŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ÿ£ŸÜŸáÿß ÿ™ÿ¥Ÿäÿ± ÿ•ŸÑŸâ **ÿ≠ŸäŸàÿßŸÜ ÿ≥ŸÖŸäŸÜ**.\n",
    "\n",
    "ŸàŸáÿ∞ÿß ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ŸáŸà ÿ¨ŸàŸáÿ± **ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™Ÿàÿ≤ŸäÿπŸä**.\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "Suppose we have these sentences:\n",
    "\n",
    "1. My father slaughtered a **qan øar**.\n",
    "2. My brother disliked **qan øar** meat yesterday.\n",
    "3. The **qan øar** ran away from the lion.\n",
    "4. It‚Äôs as fat as a **qan øar**.\n",
    "\n",
    "Even without knowing ‚Äúqan øar,‚Äù\n",
    "we can infer it‚Äôs likely **an animal**, probably a **fat one**,\n",
    "since it appears with words like *meat*, *lion*, and *fat*.\n",
    "\n",
    "That‚Äôs **distributional similarity** ‚Äî inferring meaning from context.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ üá∏üá¶ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿ© | üá¨üáß The Mathematical Intuition\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ŸÜÿÆÿ≤ŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸä **ŸÖÿµŸÅŸàŸÅÿ© ŸÉÿ®Ÿäÿ±ÿ©** ÿ™ŸÖÿ´ŸÑ ŸÉŸÖ ŸÖÿ±ÿ© ÿ™ÿ∏Ÿáÿ± ŸÉŸÑŸÖÿ© ÿ®ÿ¨ÿßŸÜÿ® ŸÉŸÑŸÖÿ© ÿ£ÿÆÿ±Ÿâ.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity.png\" alt=\"PPMI Example\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity-2.png\" alt=\"Distributional Similarity\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity-3.png\" alt=\"Distributional Similarity\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "We represent words in a **co-occurrence matrix** ‚Äî\n",
    "how often each word appears near others.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity.png\" alt=\"PPMI Example\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity-2.png\" alt=\"Distributional Similarity\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/dist-similarity-3.png\" alt=\"Distributional Similarity\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ üá∏üá¶ ÿßŸÑŸÜÿ∑ÿßŸÇ (Window) | üá¨üáß Context Window\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ŸÖÿß ŸÜÿ≠ÿ™ÿßÿ¨ ŸÜÿ≠ŸÑŸÑ ÿßŸÑŸÜÿµ ŸÉŸÑŸáÿå\n",
    "ÿ®ŸÑ ŸÜŸÉÿ™ŸÅŸä ÿ®ÿπÿØÿØ ŸÉŸÑŸÖÿßÿ™ ŸÇÿ®ŸÑ Ÿàÿ®ÿπÿØ ŸÉŸÑ ŸÉŸÑŸÖÿ© ‚Äî ŸÖÿ´ŸÑŸãÿß **20 ŸÉŸÑŸÖÿ©** ‚Äî\n",
    "ŸÑÿ£ŸÜ Ÿáÿ∞ÿß ŸäŸÉŸÅŸä ŸÑÿ™ÿ≠ÿØŸäÿØ ŸÖÿπŸÜÿßŸáÿß ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿä.\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "We don‚Äôt need to scan the whole text.\n",
    "We use a **context window** ‚Äî say, 20 words around each target word ‚Äî\n",
    "to capture the nearby meaning effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ üá∏üá¶ ŸÖŸÜ ÿßŸÑÿ¨ŸÖŸÑ ÿ•ŸÑŸâ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ | üá¨üáß From Sentences to Numbers\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ŸÜÿ≠ŸàŸëŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ŸÖÿµŸÅŸàŸÅÿßÿ™ ÿ±ŸÇŸÖŸäÿ© ÿ™ŸÖÿ´ŸÑ ŸÉŸÖ ŸÖÿ±ÿ© ÿ™ÿ∏Ÿáÿ± ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÖÿπŸãÿß.\n",
    "ŸÖÿ´ŸÑŸãÿß:\n",
    "`pineapple` Ÿà `apricot` ÿ™ÿ∏Ÿáÿ±ÿßŸÜ ŸÅŸä ÿ£ŸÖÿßŸÉŸÜ ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ©ÿå\n",
    "ÿ®ŸäŸÜŸÖÿß `digital` Ÿà `information` ŸÉÿ∞ŸÑŸÉ ŸÖÿ™ŸÇÿßÿ±ÿ®ÿ™ÿßŸÜ.\n",
    "\n",
    "ŸÅŸÜŸÇŸàŸÑ:\n",
    "\n",
    "> ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÅŸä ÿßŸÑÿ™Ÿàÿ≤Ÿäÿπ ‚Üí ŸÖÿ™ÿ¥ÿßÿ®Ÿáÿ© ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ.\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "We convert text into numeric matrices showing co-occurrence frequencies.\n",
    "If `pineapple` and `apricot` appear in similar contexts,\n",
    "and `digital` with `information`,\n",
    "then they share similar meanings.\n",
    "\n",
    "> Words that appear in similar contexts have similar meanings.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 6Ô∏è‚É£ üá∏üá¶ ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© PPMI ‚Äî ŸÖÿ™Ÿâ ŸÜÿ≥ÿ™ÿÆÿØŸÖŸáÿß ŸàŸÑŸäÿ¥\n",
    "\n",
    "## 6Ô∏è‚É£ üá¨üáß PPMI Algorithm ‚Äî When and Why We Use It\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿ£ŸàŸÑŸãÿß: ŸÑŸäÿ¥ ŸÜÿ≠ÿ™ÿßÿ¨Ÿáÿßÿü\n",
    "\n",
    "ŸÑŸÖÿß ŸÜŸÉŸàŸëŸÜ **ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿ™ÿ¥ÿßÿ±ŸÉ (co-occurrence matrix)** ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸàÿßŸÑÿ≥ŸäÿßŸÇÿßÿ™ÿå\n",
    "ÿ®ŸäŸÉŸàŸÜ ÿπŸÜÿØŸÜÿß ÿ£ÿ±ŸÇÿßŸÖ ÿ®ÿ≥Ÿäÿ∑ÿ© ŸÖÿ´ŸÑ:\n",
    "\n",
    "> ŸÉŸÑŸÖÿ© \"book\" ÿ∏Ÿáÿ±ÿ™ ŸÖÿπ \"read\" = 5 ŸÖÿ±ÿßÿ™ÿå ŸàŸÖÿπ \"pen\" = 2 ŸÖÿ±ÿßÿ™.\n",
    "\n",
    "ŸÑŸÉŸÜ ŸÖÿ¨ÿ±ÿØ **ÿπÿØÿØ ÿßŸÑÿ∏ŸáŸàÿ± ÿßŸÑÿÆÿßŸÖ (count)** ŸÖÿß ŸäŸÉŸÅŸä üòï\n",
    "ŸÑÿ£ŸÜŸá ŸÖŸÖŸÉŸÜ ŸÉŸÑŸÖÿ© \"the\" ÿ™ÿ∏Ÿáÿ± ŸÖÿπ ŸÉŸÑ ÿ¥Ÿäÿ° ÿ™ŸÇÿ±Ÿäÿ®Ÿãÿß ‚Äî ŸÅŸáŸÑ Ÿáÿ∞ÿß ŸäÿπŸÜŸä ÿ•ŸÜŸáÿß ŸÖŸáŸÖÿ©ÿü ŸÑÿß.\n",
    "\n",
    "ÿ•ÿ≠ŸÜÿß ŸÜÿ≠ÿ™ÿßÿ¨ ŸÜÿπÿ±ŸÅ:\n",
    "\n",
    "> ŸáŸÑ ÿ∏ŸáŸàÿ± ÿßŸÑŸÉŸÑŸÖÿ© ÿ®ÿ¨ÿßŸÜÿ® ŸÉŸÑŸÖÿ© ŸÖÿπŸäŸÜÿ© ‚ÄúŸÖŸÖŸäÿ≤‚Äù ŸÅÿπŸÑŸãÿßÿü ŸàŸÑÿß ŸÖÿ¨ÿ±ÿØ ÿµÿØŸÅÿ©ÿü\n",
    "\n",
    "ŸàŸáŸÜÿß Ÿäÿ¨Ÿä ÿØŸàÿ± **PPMI** üí°\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Why We Need PPMI\n",
    "\n",
    "Raw word counts (like how often two words co-occur) aren‚Äôt meaningful on their own.\n",
    "Common words like *the*, *and*, *a* appear everywhere ‚Äî but they don‚Äôt tell us real relationships.\n",
    "\n",
    "We need to know:\n",
    "\n",
    "> Is this co-occurrence **statistically significant** or just random?\n",
    "\n",
    "That‚Äôs what **PPMI (Positive Pointwise Mutual Information)** measures.\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿ´ÿßŸÜŸäŸãÿß: ŸÖÿ™Ÿâ ŸÜÿ≥ÿ™ÿÆÿØŸÖŸáÿß\n",
    "\n",
    "ŸÜÿ≥ÿ™ÿÆÿØŸÖ PPMI ŸÅŸä ÿ£Ÿä ŸàŸÇÿ™ ŸÜÿ®ÿ∫Ÿâ **ŸÜŸÇŸäÿ≥ ÿßŸÑÿπŸÑÿßŸÇÿ© ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ© ÿ®ŸäŸÜ ÿßŸÑŸÉŸÑŸÖÿ© ŸàÿßŸÑÿ≥ŸäÿßŸÇ**\n",
    "ÿ®ÿØŸàŸÜ ŸÖÿß ŸÜÿ™ÿ£ÿ´ÿ± ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ÿ¨ÿØŸãÿß.\n",
    "\n",
    "üß† ÿ£ŸÖÿ´ŸÑÿ© ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸáÿß:\n",
    "\n",
    "1. ÿ®ŸÜÿßÿ° **ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿßŸÑŸÉŸÑŸÖÿßÿ™ (word embeddings)** ŸÖŸÜ ŸÜÿµŸàÿµ ÿ∂ÿÆŸÖÿ©.\n",
    "2. ÿ™ÿ≠ŸÑŸäŸÑ **ÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÉŸÑŸÖÿßÿ™** (ŸÖÿ´ŸÑ Word2Vec ŸÑŸÉŸÜ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿ©).\n",
    "3. ÿ•ŸÜÿ¥ÿßÿ° **ÿÆÿ±ÿßÿ¶ÿ∑ ÿØŸÑÿßŸÑŸäÿ©** ÿ£Ÿà **ŸÖÿµŸÅŸàŸÅÿßÿ™ ÿ™Ÿàÿ≤Ÿäÿπ** ŸÑŸÅŸáŸÖ ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿ®ŸäŸÜ ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß When We Use It\n",
    "\n",
    "We use **PPMI** whenever we want to measure how strongly a word is associated with its context,\n",
    "without being biased by very frequent words.\n",
    "\n",
    "‚úÖ Use cases:\n",
    "\n",
    "1. Building **word embeddings** from co-occurrence matrices.\n",
    "2. Measuring **word similarity** statistically.\n",
    "3. Creating **semantic maps** or **distributional representations**.\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ÿ´ÿßŸÑÿ´Ÿãÿß: Ÿàÿ¥ ÿ™ÿ≥ŸàŸä ÿ®ÿßŸÑÿ∂ÿ®ÿ∑\n",
    "\n",
    "PPMI ÿ™ÿπŸäÿØ ‚ÄúŸàÿ≤ŸÜ‚Äù ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿØÿßÿÆŸÑ ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿ®ÿ≠Ÿäÿ´:\n",
    "\n",
    "* ÿßŸÑÿπŸÑÿßŸÇÿ© ÿßŸÑŸÇŸàŸäÿ© ÿ®ŸäŸÜ ŸÉŸÑŸÖÿ™ŸäŸÜ ŸÜÿßÿØÿ±ÿ© ŸÑŸÉŸÜŸáÿß ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© ÿ¨ÿØŸãÿß ‚Üí **ŸÇŸäŸÖÿ© ÿπÿßŸÑŸäÿ©**\n",
    "* ÿßŸÑÿπŸÑÿßŸÇÿ© ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ÿ®ŸäŸÜ ŸÉŸÑŸÖÿ™ŸäŸÜ ÿ∫Ÿäÿ± ŸÖŸÖŸäÿ≤ÿ™ŸäŸÜ ‚Üí **ŸÇŸäŸÖÿ© ŸÖŸÜÿÆŸÅÿ∂ÿ© ÿ£Ÿà ÿµŸÅÿ±**\n",
    "\n",
    "ŸàŸáÿ∞ÿß ŸäÿÆŸÑŸä ÿßŸÑŸÖÿµŸÅŸàŸÅÿ© ÿ™ÿπÿ®Ÿëÿ± ÿπŸÜ **ÿßŸÑŸÖÿπŸÜŸâ** ÿ®ÿØŸÑ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ŸÅŸÇÿ∑.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß What It Actually Does\n",
    "\n",
    "PPMI **re-weights** the raw co-occurrence matrix:\n",
    "\n",
    "* Rare but meaningful associations ‚Üí **High score**\n",
    "* Common but uninformative ones ‚Üí **Low or zero**\n",
    "\n",
    "This helps capture **true semantic relationships**, not just word frequency.\n",
    "\n",
    "---\n",
    "\n",
    "### üá∏üá¶ ŸÖÿ´ÿßŸÑ ÿ≥ÿ±Ÿäÿπ\n",
    "\n",
    "ŸÉŸÑŸÖÿ© \"king\" ÿ™ÿ∏Ÿáÿ± ŸÉÿ´Ÿäÿ± ŸÖÿπ \"queen\" ÿ£ŸÉÿ´ÿ± ŸÖŸÜ \"banana\"ÿå\n",
    "ÿ≠ÿ™Ÿâ ŸÑŸà \"banana\" ÿ∏Ÿáÿ±ÿ™ ŸÉÿ´Ÿäÿ± ÿ®ÿ¥ŸÉŸÑ ÿπÿßŸÖ.\n",
    "PPMI ŸäŸÅŸáŸÖ ÿ•ŸÜ ÿßŸÑÿπŸÑÿßŸÇÿ© \"king‚Äìqueen\" ÿ£ŸÉÿ´ÿ± **ÿØŸÑÿßŸÑŸäÿ©** ŸÖŸÜ ŸÖÿ¨ÿ±ÿØ ÿßŸÑÿ™ŸÉÿ±ÿßÿ±.\n",
    "\n",
    "---\n",
    "\n",
    "### üá¨üáß Quick Example\n",
    "\n",
    "‚Äúking‚Äù co-occurs with ‚Äúqueen‚Äù more meaningfully than with ‚Äúbanana,‚Äù\n",
    "even if ‚Äúbanana‚Äù appears more often overall.\n",
    "PPMI captures that ‚Äî it focuses on **semantic significance**, not raw counts.\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ üá∏üá¶ ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑŸÄ Bias Ÿàÿ≠ŸÑ ŸÑÿßÿ®ŸÑÿßÿ≥ | üá¨üáß Bias Problem and Laplace Smoothing\n",
    "\n",
    "### üá∏üá¶ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿä:\n",
    "\n",
    "ÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ™ÿ∏Ÿáÿ± ÿßŸÑŸÇŸäŸÖ ÿ∫Ÿäÿ± ŸÖŸÜÿ∑ŸÇŸäÿ© ‚Äî\n",
    "ŸÖÿ´ŸÑÿßŸã Ÿäÿπÿ∑Ÿä ÿ™ÿ¥ÿßÿ®Ÿá ÿπÿßŸÑŸä ÿ®ŸäŸÜ `pineapple` Ÿà `apricot`\n",
    "ŸÑŸÉŸÜ ŸÖŸÜÿÆŸÅÿ∂ ÿ®ŸäŸÜ `digital` Ÿà `information`.\n",
    "\n",
    "ÿßŸÑÿ≥ÿ®ÿ®ÿü\n",
    "Ÿàÿ¨ŸàÿØ **ÿ™ÿ≠ŸäŸëÿ≤ (bias)** ŸÅŸä ÿßŸÑÿ≠ÿ≥ÿßÿ®ÿßÿ™ ÿπŸÜÿØŸÖÿß ÿ™ŸÉŸàŸÜ ÿ®ÿπÿ∂ ÿßŸÑŸÇŸäŸÖ ÿµÿ∫Ÿäÿ±ÿ© ÿ¨ÿØŸãÿß.\n",
    "\n",
    "ÿßŸÑÿ≠ŸÑÿü\n",
    "ŸÜÿ∂ŸäŸÅ ÿ±ŸÇŸÖ ÿµÿ∫Ÿäÿ± (ŸÖÿ´ŸÑ **2**) ŸÑŸÉŸÑ ÿßŸÑŸÇŸäŸÖ ŸÇÿ®ŸÑ ÿßŸÑÿ≠ÿ≥ÿßÿ®.\n",
    "Ÿáÿ∞ÿß ŸäŸèÿ≥ŸÖŸëŸâ **Laplace Smoothing**.\n",
    "\n",
    "ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ©:\n",
    "ÿ™ÿµÿ®ÿ≠ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ÿ£ŸÉÿ´ÿ± ŸÖŸÜÿ∑ŸÇŸäÿ© ŸàŸÖÿ™Ÿàÿßÿ≤ŸÜÿ©.\n",
    "\n",
    "### üá¨üáß In English:\n",
    "\n",
    "Sometimes results look illogical ‚Äî\n",
    "e.g., `pineapple` and `apricot` show higher similarity than `digital` and `information`.\n",
    "\n",
    "The reason: statistical **bias** from low-frequency counts.\n",
    "\n",
    "The fix:\n",
    "Add a small constant (like **2**) to all counts before computing.\n",
    "This is called **Laplace Smoothing**.\n",
    "\n",
    "This adjustment produces more realistic similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ üá∏üá¶ ÿßŸÑÿÆŸÑÿßÿµÿ© | üá¨üáß Summary\n",
    "\n",
    "| üá∏üá¶ ÿßŸÑŸÖŸÅŸáŸàŸÖ                   | üá¨üáß Concept                             |\n",
    "| ------------------------------ | ---------------------------------------- |\n",
    "| ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ™Ÿàÿ≤ŸäÿπŸä               | Distributional Similarity                |\n",
    "| ÿßŸÑŸÉŸÑŸÖÿ© ÿ™ŸèŸÅŸáŸÖ ŸÖŸÜ ÿ≥ŸäÿßŸÇŸáÿß         | A word is understood by its context      |\n",
    "| ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿ™ÿ¥ÿßÿ±ŸÉ (Co-occurrence) | Measures how often words appear together |\n",
    "| PPMI                           | Measures strength of association         |\n",
    "| Laplace Smoothing              | Fixes bias in frequency counts           |\n",
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
